{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    모델 및 데이터셋 클래스 정의하는 코드\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# 시계열 데이터를 처리하는 클래스를 정의합니다.\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len  # 입력 시퀀스의 길이를 정의합니다.\n",
    "        self.pred_len = pred_len  # 예측할 시퀀스의 길이를 정의합니다.\n",
    "        self.scaler = MinMaxScaler()  # 데이터 정규화를 위한 MinMaxScaler 객체를 생성합니다.\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)  # 데이터 전처리 함수를 호출하여 dataframe을 전처리합니다.\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # 누락된 값을 시계열의 이전 값으로 채웁니다.\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # 숫자형 열을 [0, 1] 범위로 정규화합니다.\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # 범주형 변수를 원-핫 인코딩합니다.\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # 원래의 범주형 열을 삭제하고 인코딩된 열과 병합합니다.\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1  # 데이터셋의 전체 길이를 반환합니다.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len]  # 입력 시퀀스를 선택합니다.\n",
    "        # 마지막 56열이 전력 값이라고 가정하고 예측할 시퀀스를 선택합니다.\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] \n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # y 값을 평탄화하여 반환합니다.\n",
    "    \n",
    "# LSTM 모델을 정의합니다.\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # LSTM의 은닉층의 차원을 정의합니다.\n",
    "        self.n_layers = n_layers  # LSTM 층의 개수를 정의합니다.\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)  # LSTM 층을 정의합니다.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 완전 연결 층을 정의합니다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 은닉 상태와 셀 상태를 초기화합니다.\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM 층을 통해 데이터를 전달하고 출력을 얻습니다.\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 완전 연결 층을 통해 출력을 얻습니다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    데이터셋 읽고 set으로 분할하는 코드\n",
    "\"\"\"\n",
    "# pandas 라이브러리를 사용하여 엑셀 파일을 불러옵니다.\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data_KW.xlsx')\n",
    "\n",
    "# TimeSeriesDataset 클래스의 인스턴스를 생성합니다. 위에서 정의한 클래스를 사용하여 데이터를 전처리합니다.\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# 학습, 검증 및 테스트 데이터 세트의 크기를 정의합니다.\n",
    "train_size = int(0.7 * len(dataset))  # 전체 데이터의 70%를 학습 데이터로 사용\n",
    "val_size = int(0.2 * len(dataset))    # 전체 데이터의 20%를 검증 데이터로 사용\n",
    "test_size = len(dataset) - train_size - val_size  # 나머지 데이터를 테스트 데이터로 사용\n",
    "\n",
    "# 전체 데이터셋을 학습, 검증 및 테스트 데이터 세트로 분할합니다.\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Validation Loss: 0.1055, Save model\n",
      "Epoch [2/200], Validation Loss: 0.0520, Save model\n",
      "Epoch [3/200], Validation Loss: 0.0471, Save model\n",
      "Epoch [4/200], Validation Loss: 0.0449, Save model\n",
      "Epoch [5/200], Validation Loss: 0.0438, Save model\n",
      "Epoch [6/200], Validation Loss: 0.0434, Save model\n",
      "Epoch [7/200], Validation Loss: 0.0434\n",
      "Epoch [8/200], Validation Loss: 0.0426, Save model\n",
      "Epoch [9/200], Validation Loss: 0.0429\n",
      "Epoch [10/200], Validation Loss: 0.0432\n",
      "Epoch [11/200], Validation Loss: 0.0431\n",
      "Epoch [12/200], Validation Loss: 0.0418, Save model\n",
      "Epoch [13/200], Validation Loss: 0.0414, Save model\n",
      "Epoch [14/200], Validation Loss: 0.0410, Save model\n",
      "Epoch [15/200], Validation Loss: 0.0411\n",
      "Epoch [16/200], Validation Loss: 0.0401, Save model\n",
      "Epoch [17/200], Validation Loss: 0.0419\n",
      "Epoch [18/200], Validation Loss: 0.0400, Save model\n",
      "Epoch [19/200], Validation Loss: 0.0406\n",
      "Epoch [20/200], Validation Loss: 0.0397, Save model\n",
      "Epoch [21/200], Validation Loss: 0.0398\n",
      "Epoch [22/200], Validation Loss: 0.0407\n",
      "Epoch [23/200], Validation Loss: 0.0395, Save model\n",
      "Epoch [24/200], Validation Loss: 0.0393, Save model\n",
      "Epoch [25/200], Validation Loss: 0.0408\n",
      "Epoch [26/200], Validation Loss: 0.0387, Save model\n",
      "Epoch [27/200], Validation Loss: 0.0380, Save model\n",
      "Epoch [28/200], Validation Loss: 0.0394\n",
      "Epoch [29/200], Validation Loss: 0.0389\n",
      "Epoch [30/200], Validation Loss: 0.0378, Save model\n",
      "Epoch [31/200], Validation Loss: 0.0360, Save model\n",
      "Epoch [32/200], Validation Loss: 0.0365\n",
      "Epoch [33/200], Validation Loss: 0.0307, Save model\n",
      "Epoch [34/200], Validation Loss: 0.0248, Save model\n",
      "Epoch [35/200], Validation Loss: 0.0192, Save model\n",
      "Epoch [36/200], Validation Loss: 0.0190, Save model\n",
      "Epoch [37/200], Validation Loss: 0.0170, Save model\n",
      "Epoch [38/200], Validation Loss: 0.0164, Save model\n",
      "Epoch [39/200], Validation Loss: 0.0155, Save model\n",
      "Epoch [40/200], Validation Loss: 0.0150, Save model\n",
      "Epoch [41/200], Validation Loss: 0.0144, Save model\n",
      "Epoch [42/200], Validation Loss: 0.0143, Save model\n",
      "Epoch [43/200], Validation Loss: 0.0136, Save model\n",
      "Epoch [44/200], Validation Loss: 0.0142\n",
      "Epoch [45/200], Validation Loss: 0.0135, Save model\n",
      "Epoch [46/200], Validation Loss: 0.0133, Save model\n",
      "Epoch [47/200], Validation Loss: 0.0132, Save model\n",
      "Epoch [48/200], Validation Loss: 0.0125, Save model\n",
      "Epoch [49/200], Validation Loss: 0.0119, Save model\n",
      "Epoch [50/200], Validation Loss: 0.0117, Save model\n",
      "Epoch [51/200], Validation Loss: 0.0113, Save model\n",
      "Epoch [52/200], Validation Loss: 0.0111, Save model\n",
      "Epoch [53/200], Validation Loss: 0.0110, Save model\n",
      "Epoch [54/200], Validation Loss: 0.0113\n",
      "Epoch [55/200], Validation Loss: 0.0113\n",
      "Epoch [56/200], Validation Loss: 0.0108, Save model\n",
      "Epoch [57/200], Validation Loss: 0.0107, Save model\n",
      "Epoch [58/200], Validation Loss: 0.0105, Save model\n",
      "Epoch [59/200], Validation Loss: 0.0103, Save model\n",
      "Epoch [60/200], Validation Loss: 0.0103, Save model\n",
      "Epoch [61/200], Validation Loss: 0.0101, Save model\n",
      "Epoch [62/200], Validation Loss: 0.0102\n",
      "Epoch [63/200], Validation Loss: 0.0099, Save model\n",
      "Epoch [64/200], Validation Loss: 0.0102\n",
      "Epoch [65/200], Validation Loss: 0.0097, Save model\n",
      "Epoch [66/200], Validation Loss: 0.0095, Save model\n",
      "Epoch [67/200], Validation Loss: 0.0102\n",
      "Epoch [68/200], Validation Loss: 0.0103\n",
      "Epoch [69/200], Validation Loss: 0.0100\n",
      "Epoch [70/200], Validation Loss: 0.0097\n",
      "Epoch [71/200], Validation Loss: 0.0095, Save model\n",
      "Epoch [72/200], Validation Loss: 0.0094, Save model\n",
      "Epoch [73/200], Validation Loss: 0.0093, Save model\n",
      "Epoch [74/200], Validation Loss: 0.0092, Save model\n",
      "Epoch [75/200], Validation Loss: 0.0092\n",
      "Epoch [76/200], Validation Loss: 0.0092, Save model\n",
      "Epoch [77/200], Validation Loss: 0.0091, Save model\n",
      "Epoch [78/200], Validation Loss: 0.0089, Save model\n",
      "Epoch [79/200], Validation Loss: 0.0090\n",
      "Epoch [80/200], Validation Loss: 0.0091\n",
      "Epoch [81/200], Validation Loss: 0.0090\n",
      "Epoch [82/200], Validation Loss: 0.0091\n",
      "Epoch [83/200], Validation Loss: 0.0088, Save model\n",
      "Epoch [84/200], Validation Loss: 0.0091\n",
      "Epoch [85/200], Validation Loss: 0.0091\n",
      "Epoch [86/200], Validation Loss: 0.0088, Save model\n",
      "Epoch [87/200], Validation Loss: 0.0088\n",
      "Epoch [88/200], Validation Loss: 0.0088, Save model\n",
      "Epoch [89/200], Validation Loss: 0.0088\n",
      "Epoch [90/200], Validation Loss: 0.0086, Save model\n",
      "Epoch [91/200], Validation Loss: 0.0086, Save model\n",
      "Epoch [92/200], Validation Loss: 0.0087\n",
      "Epoch [93/200], Validation Loss: 0.0086\n",
      "Epoch [94/200], Validation Loss: 0.0086\n",
      "Epoch [95/200], Validation Loss: 0.0087\n",
      "Epoch [96/200], Validation Loss: 0.0085, Save model\n",
      "Epoch [97/200], Validation Loss: 0.0087\n",
      "Epoch [98/200], Validation Loss: 0.0085\n",
      "Epoch [99/200], Validation Loss: 0.0084, Save model\n",
      "Epoch [100/200], Validation Loss: 0.0085\n",
      "Epoch [101/200], Validation Loss: 0.0084\n",
      "Epoch [102/200], Validation Loss: 0.0084\n",
      "Epoch [103/200], Validation Loss: 0.0084\n",
      "Epoch [104/200], Validation Loss: 0.0084\n",
      "Epoch [105/200], Validation Loss: 0.0084\n",
      "Epoch [106/200], Validation Loss: 0.0084\n",
      "Epoch [107/200], Validation Loss: 0.0084\n",
      "Epoch [108/200], Validation Loss: 0.0083, Save model\n",
      "Epoch [109/200], Validation Loss: 0.0083\n",
      "Epoch [110/200], Validation Loss: 0.0083, Save model\n",
      "Epoch [111/200], Validation Loss: 0.0083\n",
      "Epoch [112/200], Validation Loss: 0.0083, Save model\n",
      "Epoch [113/200], Validation Loss: 0.0084\n",
      "Epoch [114/200], Validation Loss: 0.0082, Save model\n",
      "Epoch [115/200], Validation Loss: 0.0082\n",
      "Epoch [116/200], Validation Loss: 0.0082\n",
      "Epoch [117/200], Validation Loss: 0.0081, Save model\n",
      "Epoch [118/200], Validation Loss: 0.0081, Save model\n",
      "Epoch [119/200], Validation Loss: 0.0081\n",
      "Epoch [120/200], Validation Loss: 0.0082\n",
      "Epoch [121/200], Validation Loss: 0.0083\n",
      "Epoch [122/200], Validation Loss: 0.0083\n",
      "Epoch [123/200], Validation Loss: 0.0082\n",
      "Epoch [124/200], Validation Loss: 0.0082\n",
      "Epoch [125/200], Validation Loss: 0.0081\n",
      "Epoch [126/200], Validation Loss: 0.0082\n",
      "Epoch [127/200], Validation Loss: 0.0081\n",
      "Epoch [128/200], Validation Loss: 0.0082\n",
      "Epoch [129/200], Validation Loss: 0.0080, Save model\n",
      "Epoch [130/200], Validation Loss: 0.0081\n",
      "Epoch [131/200], Validation Loss: 0.0081\n",
      "Epoch [132/200], Validation Loss: 0.0080, Save model\n",
      "Epoch [133/200], Validation Loss: 0.0080\n",
      "Epoch [134/200], Validation Loss: 0.0080\n",
      "Epoch [135/200], Validation Loss: 0.0080, Save model\n",
      "Epoch [136/200], Validation Loss: 0.0079, Save model\n",
      "Epoch [137/200], Validation Loss: 0.0080\n",
      "Epoch [138/200], Validation Loss: 0.0081\n",
      "Epoch [139/200], Validation Loss: 0.0079, Save model\n",
      "Epoch [140/200], Validation Loss: 0.0080\n",
      "Epoch [141/200], Validation Loss: 0.0079, Save model\n",
      "Epoch [142/200], Validation Loss: 0.0078, Save model\n",
      "Epoch [143/200], Validation Loss: 0.0079\n",
      "Epoch [144/200], Validation Loss: 0.0079\n",
      "Epoch [145/200], Validation Loss: 0.0079\n",
      "Epoch [146/200], Validation Loss: 0.0078\n",
      "Epoch [147/200], Validation Loss: 0.0078, Save model\n",
      "Epoch [148/200], Validation Loss: 0.0077, Save model\n",
      "Epoch [149/200], Validation Loss: 0.0078\n",
      "Epoch [150/200], Validation Loss: 0.0077\n",
      "Epoch [151/200], Validation Loss: 0.0077, Save model\n",
      "Epoch [152/200], Validation Loss: 0.0078\n",
      "Epoch [153/200], Validation Loss: 0.0078\n",
      "Epoch [154/200], Validation Loss: 0.0078\n",
      "Epoch [155/200], Validation Loss: 0.0077\n",
      "Epoch [156/200], Validation Loss: 0.0077\n",
      "Epoch [157/200], Validation Loss: 0.0077\n",
      "Epoch [158/200], Validation Loss: 0.0077, Save model\n",
      "Epoch [159/200], Validation Loss: 0.0077\n",
      "Epoch [160/200], Validation Loss: 0.0077\n",
      "Epoch [161/200], Validation Loss: 0.0078\n",
      "Epoch [162/200], Validation Loss: 0.0076, Save model\n",
      "Epoch [163/200], Validation Loss: 0.0077\n",
      "Epoch [164/200], Validation Loss: 0.0077\n",
      "Epoch [165/200], Validation Loss: 0.0078\n",
      "Epoch [166/200], Validation Loss: 0.0076\n",
      "Epoch [167/200], Validation Loss: 0.0077\n",
      "Epoch [168/200], Validation Loss: 0.0075, Save model\n",
      "Epoch [169/200], Validation Loss: 0.0076\n",
      "Epoch [170/200], Validation Loss: 0.0077\n",
      "Epoch [171/200], Validation Loss: 0.0076\n",
      "Epoch [172/200], Validation Loss: 0.0075, Save model\n",
      "Epoch [173/200], Validation Loss: 0.0075\n",
      "Epoch [174/200], Validation Loss: 0.0076\n",
      "Epoch [175/200], Validation Loss: 0.0075\n",
      "Epoch [176/200], Validation Loss: 0.0075, Save model\n",
      "Epoch [177/200], Validation Loss: 0.0075\n",
      "Epoch [178/200], Validation Loss: 0.0075, Save model\n",
      "Epoch [179/200], Validation Loss: 0.0074, Save model\n",
      "Epoch [180/200], Validation Loss: 0.0074\n",
      "Epoch [181/200], Validation Loss: 0.0075\n",
      "Epoch [182/200], Validation Loss: 0.0075\n",
      "Epoch [183/200], Validation Loss: 0.0074\n",
      "Epoch [184/200], Validation Loss: 0.0075\n",
      "Epoch [185/200], Validation Loss: 0.0075\n",
      "Epoch [186/200], Validation Loss: 0.0075\n",
      "Epoch [187/200], Validation Loss: 0.0075\n",
      "Epoch [188/200], Validation Loss: 0.0074\n",
      "Epoch [189/200], Validation Loss: 0.0075\n",
      "Epoch [190/200], Validation Loss: 0.0075\n",
      "Epoch [191/200], Validation Loss: 0.0074\n",
      "Epoch [192/200], Validation Loss: 0.0075\n",
      "Epoch [193/200], Validation Loss: 0.0074, Save model\n",
      "Epoch [194/200], Validation Loss: 0.0075\n",
      "Epoch [195/200], Validation Loss: 0.0074, Save model\n",
      "Epoch [196/200], Validation Loss: 0.0074\n",
      "Epoch [197/200], Validation Loss: 0.0074\n",
      "Epoch [198/200], Validation Loss: 0.0074\n",
      "Epoch [199/200], Validation Loss: 0.0074\n",
      "Epoch [200/200], Validation Loss: 0.0076\n",
      "MAE: 0.0580, MSE: 0.0073, RMSE: 0.0856\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    data loader, model 초기화하는 코드\n",
    "    epoch만큼 train 실행하는 코드\n",
    "    모델 저장하는 코드\n",
    "\"\"\"\n",
    "########################################## Hyperparameters ##########################################\n",
    "input_dim = len(train_set[0][0][0]) # 63\n",
    "output_dim = 24*56\n",
    "hidden_dim = 128\n",
    "n_layers = 7\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "use_thread = True\n",
    "patience = 20 # number of epochs to wait before stopping\n",
    "########################################################################################################\n",
    "\n",
    "# 결과를 저장할 폴더의 경로를 지정합니다.\n",
    "results_folder = \"/home/kimyirum/EMS/ict-2023-ems/load/results/\"\n",
    "# 현재의 시간 정보를 가져옵니다.\n",
    "now = datetime.now()\n",
    "# 현재 시간 정보를 문자열 포맷으로 변환합니다. (예: 20230807_143000)\n",
    "now_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "# 결과 메트릭을 저장할 파일의 이름을 지정합니다.\n",
    "filename_metrics = f'{now_str}.pkl'\n",
    "# 학습된 모델을 저장할 파일의 이름을 지정합니다.\n",
    "filename_model = f'model_{now_str}.pt'\n",
    "\n",
    "# DataLoader\n",
    "# use_thread 변수의 값에 따라 DataLoader의 num_workers 값을 설정합니다.\n",
    "if use_thread:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "else:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# LSTM 모델, 손실 함수, 최적화 도구를 초기화합니다.\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "criterion = nn.MSELoss()  # MSE 손실 함수를 사용합니다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam 최적화 도구를 사용합니다.\n",
    "\n",
    "# 초기 검증 손실을 무한대로 설정합니다.\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve_epoch = 0\n",
    "\n",
    "########################################## Training loop ##########################################\n",
    "# 주어진 에포크만큼 모델을 학습합니다.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # 순전파를 수행합니다.\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # 역전파 및 최적화를 수행합니다.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 모델을 검증합니다.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data, targets in val_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}', end='')\n",
    "        \n",
    "        # 검증 손실이 감소했을 경우, 모델을 저장합니다.\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\", Save model\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), results_folder+filename_model)\n",
    "            no_improve_epoch = 0\n",
    "        else:\n",
    "            print(\"\")\n",
    "            no_improve_epoch += 1\n",
    "            \n",
    "        # 조기 종료 조건: 검증 손실이 연속으로 patience동안 개선되지 않을 때 학습을 중단합니다.\n",
    "        if no_improve_epoch > patience:\n",
    "            print('Early stopping...')\n",
    "            break\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "# 모델을 평가 모드로 설정합니다. 이는 dropout, batch normalization 등의 레이어가 \n",
    "# 학습 모드와 다르게 작동해야 할 때 필요합니다.\n",
    "model.eval()\n",
    "\n",
    "# torch.no_grad()를 사용하여 autograd의 gradient 계산을 비활성화합니다. \n",
    "# 이렇게 하면 메모리 사용량을 줄이고 속도를 높일 수 있습니다.\n",
    "with torch.no_grad():\n",
    "    all_targets = []  # 실제 목표 값들을 저장할 리스트를 초기화합니다.\n",
    "    all_outputs = []  # 모델의 예측 값을 저장할 리스트를 초기화합니다.\n",
    "    for data, targets in test_loader:  # 테스트 데이터로더에서 배치를 반복적으로 가져옵니다.\n",
    "        outputs = model(data)  # 모델을 사용하여 입력 데이터에 대한 예측값을 생성합니다.\n",
    "        all_targets.append(targets.numpy())  # 목표 값을 리스트에 추가합니다.\n",
    "        all_outputs.append(outputs.numpy())  # 예측 값을 리스트에 추가합니다.\n",
    "\n",
    "# 목표 값과 예측 값을 모두 단일 넘파이 배열로 연결(flatten)합니다.\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# 평균 절대 오차(MAE), 평균 제곱 오차(MSE) 및 제곱근 평균 제곱 오차(RMSE)를 계산합니다.\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# 계산된 지표들을 출력합니다. 이 값들은 모델의 성능을 평가하는 데 사용됩니다.\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 200, 'hidden_dim': 128, 'n_layers': 7}\n",
      "MinMaxScaler()\n",
      "{'MAE': 0.05801902, 'MSE': 0.007335685, 'RMSE': 0.08564861466211036}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    위에서 학습시킨 모델의 성능이 괜찮다면, 파라미터 정보를 pkl 파일로 저장하는 코드\n",
    "\"\"\"\n",
    "\n",
    "# 사용한 하이퍼파라미터들을 저장합니다.\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'num_epochs': num_epochs,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'n_layers': n_layers\n",
    "}\n",
    "\n",
    "# 데이터 정규화에 사용된 scaler를 저장합니다.\n",
    "scalers = dataset.scaler\n",
    "\n",
    "# 성능 지표를 저장합니다.\n",
    "metrics = {\n",
    "    'MAE': mae,\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "}\n",
    "\n",
    "# 위에서 정의한 모든 결과를 하나의 사전에 합칩니다.\n",
    "results = {\n",
    "    'Hyperparameters': hyperparams,\n",
    "    'Scalers': scalers,\n",
    "    'Metrics': metrics\n",
    "}\n",
    "\n",
    "# 결합된 결과를 pickle 파일로 저장합니다.\n",
    "with open(results_folder + filename_metrics, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# 테스트를 위해\n",
    "# pickle 파일로부터 결과를 불러옵니다.\n",
    "with open(results_folder + filename_metrics, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "\n",
    "# 불러온 결과에서 데이터에 접근할 수 있습니다.\n",
    "print(loaded_results['Hyperparameters'])\n",
    "print(loaded_results['Scalers'])\n",
    "print(loaded_results['Metrics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1344]) torch.Size([1, 1344])\n",
      "Error for next 24 hours:\n",
      "        0_SV-2      1_SV-5    2_SV-6      3_SV-7  4_HV-NM1    5_HV-NM2  \\\n",
      "0    55.119337   54.357242 -0.006374   45.354768  0.006537   63.761941   \n",
      "1    55.774660   25.734787  0.002648   16.815664 -0.007302   26.146778   \n",
      "2    12.868810   -2.931396 -0.006189   -4.936558  0.001629   22.293055   \n",
      "3    83.161757   59.323946 -0.006972    6.364716  0.013761  -34.267422   \n",
      "4   -19.356127   -3.996244  0.000817   -3.143047  0.002588   58.776626   \n",
      "5   -41.951238  -50.216364 -0.007442  -27.290220  0.001287  -18.936129   \n",
      "6    10.442949   52.436289 -0.004823   31.854352  0.015007   36.703437   \n",
      "7   -10.491514   -8.881780 -0.006200   14.832201 -0.012433   -7.354283   \n",
      "8     2.229872  -10.735801 -0.001054  -13.933305  0.000888  -15.914083   \n",
      "9   -67.290851  -78.290749 -0.017405  -54.689699 -0.000209  -67.667446   \n",
      "10  -79.108715  -49.386932 -0.004426  -59.041306  0.003913  -74.605780   \n",
      "11   41.941002   18.296055  0.005105   12.398276 -0.002677   -0.366607   \n",
      "12   51.771865   69.679930 -0.000746   53.297450  0.011060   50.597721   \n",
      "13    4.278011   48.690992  0.004258   44.754092  0.000457   21.782636   \n",
      "14   10.455336   21.338202 -0.007678   18.342493  0.007949    7.660322   \n",
      "15  -34.151080  -13.306146  0.001438   11.505644  0.001585   17.178084   \n",
      "16   16.525527   20.379713  0.001802   34.388614  0.009852   93.576742   \n",
      "17  -26.098842   69.987130 -0.004603   77.867646  0.012934   49.181611   \n",
      "18  120.177231  131.155668  0.007583  132.906568  0.014008  114.354070   \n",
      "19  144.992072  134.882661  0.006779  140.803204  0.006196  140.727770   \n",
      "20  107.611870   98.865663  0.005751   76.143115  0.001580   73.957717   \n",
      "21   64.183726   23.109482  0.000632   32.784471  0.002995   88.410874   \n",
      "22   48.161329   79.238837  0.004527   51.645999  0.001484   34.113093   \n",
      "23   21.255907   24.324603  0.009245   51.876681 -0.002672   21.038950   \n",
      "\n",
      "       6_고압콘덴샤  7_신재생에너지동     8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동   47_기숙사9동  \\\n",
      "0   220.433418  16.769342   8.834246  15.106530  ...   -35.303520  -1.452633   \n",
      "1   125.944446  24.254724  -0.527153   4.720072  ...   -16.007490  -6.285313   \n",
      "2   121.304270  21.852746  -2.724620   1.529127  ...     0.633184  -4.440880   \n",
      "3    76.258494  25.274073   4.851695  11.432834  ...   -11.416494   1.596022   \n",
      "4    16.835035 -27.254042   3.524803   9.631867  ...    -2.552641   3.280662   \n",
      "5    40.986430 -17.450555   4.159199   8.988817  ...    -7.979791   0.440257   \n",
      "6    33.084381  -4.644391   0.030103  19.584845  ...    -1.093358   0.288484   \n",
      "7   -19.133790   7.140125   1.023504  14.563003  ...    10.340268   1.102451   \n",
      "8    39.687533 -41.980125   4.542111   3.514983  ...    -3.062997   3.395841   \n",
      "9    21.663338 -29.768459   6.470006   1.180180  ...    -5.155651  -2.351957   \n",
      "10   -3.281578 -21.763951 -19.657101   0.265105  ...     8.722568  -7.505408   \n",
      "11   54.055337   3.148082  21.908656   1.643487  ...    69.800492  -2.391195   \n",
      "12   36.085450  21.613426   2.651901   7.589755  ...    81.824649  -8.295694   \n",
      "13    3.499503  18.761433   6.234105  -8.215681  ...   102.876166 -11.433170   \n",
      "14   -8.622092  38.019468  -1.683204   3.835226  ...   113.183473  10.010052   \n",
      "15    9.773819  -1.197119   0.975191  -6.674078  ...    30.629105  -2.407767   \n",
      "16   48.390259  27.496242  74.170230  20.810561  ...    79.340736  -1.344629   \n",
      "17   21.065260  60.634380  11.292851   1.312553  ...    27.779808 -11.508663   \n",
      "18  -13.961538  83.585268  19.473215  20.314090  ...    45.632375 -11.822113   \n",
      "19   16.668107  75.320056   2.117256   9.893891  ...    80.609909  -5.325211   \n",
      "20   49.714094  98.415328   2.821146  -1.183460  ...    89.436727 -11.081683   \n",
      "21   78.170741  25.117567  22.769244   4.188030  ...    16.011927   8.367284   \n",
      "22 -542.678898  92.267788  -0.627791  -2.665826  ...    26.363632   6.069847   \n",
      "23 -446.033161  35.332662  -0.323072  11.332139  ...    17.834827   1.528463   \n",
      "\n",
      "    48_고등광/극초단  49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  \\\n",
      "0    22.710616     -0.266919         15.537901    -3.811299     0.685749   \n",
      "1    28.950088      0.915512         -0.251888    -0.653959    -3.311529   \n",
      "2    17.894639      0.920693          0.530778     1.786837     1.548715   \n",
      "3    35.237963      0.028118          1.534708    -4.738904    -2.587902   \n",
      "4   -27.774466      3.024001          9.802918    -1.978326    -5.401342   \n",
      "5    34.461370      3.025028          1.260679    -8.712928     4.756237   \n",
      "6    17.343287      1.518176          2.485666    -8.041042    -1.745184   \n",
      "7     5.067526     -1.138000          4.238650    -2.973193    -3.006101   \n",
      "8    24.757900     -1.899417          0.088780    -7.449103    -2.590998   \n",
      "9    16.063376     -2.837934         -1.504731   -11.228886     4.949537   \n",
      "10  -45.231906     -6.730100         -1.762292   -21.001441    -9.698908   \n",
      "11  -37.366479     -0.214354         -1.336850   -20.810029    -1.883463   \n",
      "12   23.174325      3.142416          2.169252   -11.787806    -2.177931   \n",
      "13   -1.670498      2.538289         -2.401825     8.417694    -0.494642   \n",
      "14  -25.734220     -3.613238         -0.563464    10.167223    -1.696031   \n",
      "15   -1.264023     -4.522352          0.493054     0.817879    -2.178437   \n",
      "16   18.843505     -5.760233          2.002013    13.183094    -0.649410   \n",
      "17  -37.505111     -2.882580          0.345379    10.108361    -0.818262   \n",
      "18   45.453895     -3.559202         -4.207756   -19.711335    -0.641876   \n",
      "19   38.671847     -6.229288          3.540296    -2.788216    -0.296488   \n",
      "20  102.668336      4.309580         -1.915403    -0.919710    -1.860908   \n",
      "21  107.776857     -1.159987          1.647127    -2.056410    -0.573095   \n",
      "22  -23.880142      1.131797          3.344841   -13.717445    -0.209451   \n",
      "23    0.893720      4.676678         -3.413116    -7.950932     0.677218   \n",
      "\n",
      "    53_LG도서관(E)  54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0     -3.590795     0.560335      4.872150  \n",
      "1     -6.077651    -9.552107      8.154439  \n",
      "2      0.878708    -4.477055      5.348766  \n",
      "3     -2.213133    10.838594      7.475086  \n",
      "4      0.543998     9.566460      8.467045  \n",
      "5     -1.673613    -0.002456      8.828091  \n",
      "6      0.589864     7.744174      4.662231  \n",
      "7      3.340447    19.323384      6.524813  \n",
      "8     -1.358474    -0.294153      2.386841  \n",
      "9     -0.692428   -10.815818      1.083061  \n",
      "10     7.165581    -2.285419      7.308469  \n",
      "11     2.769870    -6.965886     12.096594  \n",
      "12    -2.724732   -16.873558     23.987381  \n",
      "13     1.615404   -51.803971      8.332832  \n",
      "14     2.436850     1.103588     -3.505018  \n",
      "15     2.301910     9.483159     -6.081315  \n",
      "16    -0.442006    -5.944842     13.880687  \n",
      "17     0.562365     1.402750     16.870564  \n",
      "18    -8.642539    18.621230     20.027761  \n",
      "19    -8.177635     3.619707     19.000435  \n",
      "20     0.763168    30.253884     28.266491  \n",
      "21    -2.566393    42.191571     13.901859  \n",
      "22    -0.098812    -2.223459      7.190467  \n",
      "23    16.034393     6.421745      7.147006  \n",
      "\n",
      "[24 rows x 56 columns]\n",
      "MAE: 21.0933, MSE: 2011.6733, RMSE: 44.8517 (no normalization)\n",
      "err_total:  13176.587883399332\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    테스트 세트의 첫 번째 시퀀스에 대한 예측을 수행한 후, 예측된 값과 실제 목표값 사이의 차이를 계산하는 코드\n",
    "    이러한 차이를 기반으로 여러 성능 지표를 계산하며, 전체 에러를 출력하는 코드\n",
    "    *df는 원래의 데이터셋이라 가정하며 'date' 컬럼을 포함한다고 가정합니다.\n",
    "\"\"\"\n",
    "# 건물 이름을 가져옵니다.\n",
    "building_names = df.columns[-56:]  # 필요에 따라 이 값을 조절하세요.\n",
    "# 테스트 세트를 위한 DataLoader를 생성합니다.\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "# 테스트 세트에서 첫 번째 시퀀스와 그 목표값을 가져옵니다.\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# 모델을 평가 모드로 전환합니다.\n",
    "model.eval()\n",
    "\n",
    "# 예측을 수행합니다.\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "print(prediction.shape, real_target.shape)\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "# 패딩을 추가합니다.\n",
    "padding = np.zeros((prediction.shape[0], 7))\n",
    "prediction_pad = np.hstack((padding, prediction))\n",
    "real_target_pad = np.hstack((padding, real_target))\n",
    "# print(prediction_pad.shape, real_target_pad.shape)\n",
    "\n",
    "# 역변환을 적용하여 정규화를 해제합니다.\n",
    "prediction_inv = dataset.scaler.inverse_transform(prediction_pad)\n",
    "real_target_inv = dataset.scaler.inverse_transform(real_target_pad)\n",
    "\n",
    "# 처음 7개의 컬럼을 삭제합니다.\n",
    "prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "real_target_inv = np.delete(real_target_inv, np.s_[:7], axis=1)\n",
    "\n",
    "# 원래의 형태로 다시 변형합니다.\n",
    "prediction = prediction_inv.reshape(prediction.shape)\n",
    "real_target = real_target_inv.reshape(real_target.shape)\n",
    "\n",
    "# 에러(실제 목표값과 예측값의 차이)를 계산합니다.\n",
    "error = real_target - prediction\n",
    "\n",
    "# 예측값을 위한 DataFrame을 생성합니다.\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "err_total = total = error_df.values.flatten().sum()\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n",
    "\n",
    "# 성능 지표를 계산합니다.\n",
    "mae = mean_absolute_error(real_target, prediction)\n",
    "mse = mean_squared_error(real_target, prediction)\n",
    "rmse = sqrt(mse)\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f} (no normalization)')\n",
    "print(\"err_total: \", err_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    최적의 하이퍼파라미터를 찾기 위해 optuna 라이브러리를 이용해서 실험하는 코드\n",
    "    *실험 log는 results/log_optuna.txt로 저장함\n",
    "\"\"\"\n",
    "# Optuna를 사용할 것인지 여부를 결정하는 플래그\n",
    "do_optuna = False\n",
    "\n",
    "# optuna 관련 라이브러리를 가져옵니다.\n",
    "import optuna\n",
    "import optuna.logging\n",
    "\n",
    "# Optuna의 기본 로깅 핸들러를 활성화합니다.\n",
    "optuna.logging.enable_default_handler()\n",
    "\n",
    "# 로깅의 상세 수준을 설정합니다.\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# Optuna를 사용하여 최적화할 목적 함수를 정의합니다.\n",
    "def objective(trial):\n",
    "    # Optuna를 사용하여 하이퍼파라미터를 추정합니다.\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 50, 300)\n",
    "    n_layers = trial.suggest_int('n_layers', 5, 9)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    \n",
    "    # 데이터 로더를 설정합니다.\n",
    "    if use_thread:\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "        test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    else:\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 모델, 손실 함수, 최적화 알고리즘을 초기화합니다.\n",
    "    model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\") # 초기에는 무한대로 설정합니다.\n",
    "    no_improve_epoch = 0\n",
    "\n",
    "    # 훈련 루프입니다.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # 순전파\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 모델을 검증합니다.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for data, targets in val_loader:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_losses.append(loss.item())\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            # 검증 손실이 줄어들면 모델을 저장합니다.\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                no_improve_epoch = 0\n",
    "            else:\n",
    "                no_improve_epoch += 1\n",
    "                \n",
    "            # 일찍 중단하기 위한 조건\n",
    "            if no_improve_epoch > patience:\n",
    "                print('Early stopping...')\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# do_optuna 플래그가 True로 설정되어 있으면 Optuna를 사용하여 하이퍼파라미터를 최적화합니다.\n",
    "if do_optuna:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=100, n_jobs=4)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\" Value: \", trial.value)\n",
    "\n",
    "    print(\" Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
