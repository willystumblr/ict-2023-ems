{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # If there are any missing values, fill them with the previous value in time-series\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Normalize numerical columns to range [0, 1]\n",
    "        scaler = MinMaxScaler()\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # One-hot encode categorical variables\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # Drop original categorical columns and merge with encoded ones\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len]\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] # Assuming last 56 columns are power values\n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # flatten y values\n",
    "    \n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize hidden state\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize cell state\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the excel file\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data.xlsx')\n",
    "\n",
    "# Initialize our dataset class\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# Define the split sizes for train, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split dataset\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Validation Loss: 0.0829\n",
      "Epoch [2/150], Validation Loss: 0.0726\n",
      "Epoch [3/150], Validation Loss: 0.0658\n",
      "Epoch [4/150], Validation Loss: 0.0618\n",
      "Epoch [5/150], Validation Loss: 0.0531\n",
      "Epoch [6/150], Validation Loss: 0.0457\n",
      "Epoch [7/150], Validation Loss: 0.0439\n",
      "Epoch [8/150], Validation Loss: 0.0440\n",
      "Epoch [9/150], Validation Loss: 0.0448\n",
      "Epoch [10/150], Validation Loss: 0.0442\n",
      "Epoch [11/150], Validation Loss: 0.0444\n",
      "Epoch [12/150], Validation Loss: 0.0447\n",
      "Epoch [13/150], Validation Loss: 0.0439\n",
      "Epoch [14/150], Validation Loss: 0.0444\n",
      "Epoch [15/150], Validation Loss: 0.0438\n",
      "Epoch [16/150], Validation Loss: 0.0443\n",
      "Epoch [17/150], Validation Loss: 0.0441\n",
      "Epoch [18/150], Validation Loss: 0.0439\n",
      "Epoch [19/150], Validation Loss: 0.0441\n",
      "Epoch [20/150], Validation Loss: 0.0453\n",
      "Epoch [21/150], Validation Loss: 0.0457\n",
      "Epoch [22/150], Validation Loss: 0.0444\n",
      "Epoch [23/150], Validation Loss: 0.0441\n",
      "Epoch [24/150], Validation Loss: 0.0450\n",
      "Epoch [25/150], Validation Loss: 0.0442\n",
      "Epoch [26/150], Validation Loss: 0.0441\n",
      "Epoch [27/150], Validation Loss: 0.0440\n",
      "Epoch [28/150], Validation Loss: 0.0447\n",
      "Epoch [29/150], Validation Loss: 0.0437\n",
      "Epoch [30/150], Validation Loss: 0.0441\n",
      "Epoch [31/150], Validation Loss: 0.0444\n",
      "Epoch [32/150], Validation Loss: 0.0462\n",
      "Epoch [33/150], Validation Loss: 0.0445\n",
      "Epoch [34/150], Validation Loss: 0.0436\n",
      "Epoch [35/150], Validation Loss: 0.0439\n",
      "Epoch [36/150], Validation Loss: 0.0438\n",
      "Epoch [37/150], Validation Loss: 0.0436\n",
      "Epoch [38/150], Validation Loss: 0.0449\n",
      "Epoch [39/150], Validation Loss: 0.0445\n",
      "Epoch [40/150], Validation Loss: 0.0435\n",
      "Epoch [41/150], Validation Loss: 0.0441\n",
      "Epoch [42/150], Validation Loss: 0.0439\n",
      "Epoch [43/150], Validation Loss: 0.0450\n",
      "Epoch [44/150], Validation Loss: 0.0437\n",
      "Epoch [45/150], Validation Loss: 0.0437\n",
      "Epoch [46/150], Validation Loss: 0.0439\n",
      "Epoch [47/150], Validation Loss: 0.0443\n",
      "Epoch [48/150], Validation Loss: 0.0443\n",
      "Epoch [49/150], Validation Loss: 0.0441\n",
      "Epoch [50/150], Validation Loss: 0.0446\n",
      "Epoch [51/150], Validation Loss: 0.0447\n",
      "Epoch [52/150], Validation Loss: 0.0437\n",
      "Epoch [53/150], Validation Loss: 0.0437\n",
      "Epoch [54/150], Validation Loss: 0.0439\n",
      "Epoch [55/150], Validation Loss: 0.0443\n",
      "Epoch [56/150], Validation Loss: 0.0437\n",
      "Epoch [57/150], Validation Loss: 0.0440\n",
      "Epoch [58/150], Validation Loss: 0.0436\n",
      "Epoch [59/150], Validation Loss: 0.0436\n",
      "Epoch [60/150], Validation Loss: 0.0439\n",
      "Epoch [61/150], Validation Loss: 0.0437\n",
      "Epoch [62/150], Validation Loss: 0.0450\n",
      "Epoch [63/150], Validation Loss: 0.0437\n",
      "Epoch [64/150], Validation Loss: 0.0439\n",
      "Epoch [65/150], Validation Loss: 0.0434\n",
      "Epoch [66/150], Validation Loss: 0.0448\n",
      "Epoch [67/150], Validation Loss: 0.0427\n",
      "Epoch [68/150], Validation Loss: 0.0426\n",
      "Epoch [69/150], Validation Loss: 0.0422\n",
      "Epoch [70/150], Validation Loss: 0.0422\n",
      "Epoch [71/150], Validation Loss: 0.0428\n",
      "Epoch [72/150], Validation Loss: 0.0423\n",
      "Epoch [73/150], Validation Loss: 0.0423\n",
      "Epoch [74/150], Validation Loss: 0.0427\n",
      "Epoch [75/150], Validation Loss: 0.0421\n",
      "Epoch [76/150], Validation Loss: 0.0421\n",
      "Epoch [77/150], Validation Loss: 0.0423\n",
      "Epoch [78/150], Validation Loss: 0.0423\n",
      "Epoch [79/150], Validation Loss: 0.0421\n",
      "Epoch [80/150], Validation Loss: 0.0426\n",
      "Epoch [81/150], Validation Loss: 0.0425\n",
      "Epoch [82/150], Validation Loss: 0.0424\n",
      "Epoch [83/150], Validation Loss: 0.0424\n",
      "Epoch [84/150], Validation Loss: 0.0425\n",
      "Epoch [85/150], Validation Loss: 0.0423\n",
      "Epoch [86/150], Validation Loss: 0.0422\n",
      "Epoch [87/150], Validation Loss: 0.0422\n",
      "Epoch [88/150], Validation Loss: 0.0424\n",
      "Epoch [89/150], Validation Loss: 0.0425\n",
      "Epoch [90/150], Validation Loss: 0.0424\n",
      "Epoch [91/150], Validation Loss: 0.0423\n",
      "Epoch [92/150], Validation Loss: 0.0424\n",
      "Epoch [93/150], Validation Loss: 0.0423\n",
      "Epoch [94/150], Validation Loss: 0.0427\n",
      "Epoch [95/150], Validation Loss: 0.0424\n",
      "Epoch [96/150], Validation Loss: 0.0424\n",
      "Epoch [97/150], Validation Loss: 0.0421\n",
      "Epoch [98/150], Validation Loss: 0.0422\n",
      "Epoch [99/150], Validation Loss: 0.0420\n",
      "Epoch [100/150], Validation Loss: 0.0428\n",
      "Epoch [101/150], Validation Loss: 0.0423\n",
      "Epoch [102/150], Validation Loss: 0.0423\n",
      "Epoch [103/150], Validation Loss: 0.0421\n",
      "Epoch [104/150], Validation Loss: 0.0425\n",
      "Epoch [105/150], Validation Loss: 0.0424\n",
      "Epoch [106/150], Validation Loss: 0.0423\n",
      "Epoch [107/150], Validation Loss: 0.0425\n",
      "Epoch [108/150], Validation Loss: 0.0422\n",
      "Epoch [109/150], Validation Loss: 0.0424\n",
      "Epoch [110/150], Validation Loss: 0.0421\n",
      "Epoch [111/150], Validation Loss: 0.0429\n",
      "Epoch [112/150], Validation Loss: 0.0422\n",
      "Epoch [113/150], Validation Loss: 0.0425\n",
      "Epoch [114/150], Validation Loss: 0.0423\n",
      "Epoch [115/150], Validation Loss: 0.0421\n",
      "Epoch [116/150], Validation Loss: 0.0425\n",
      "Epoch [117/150], Validation Loss: 0.0422\n",
      "Epoch [118/150], Validation Loss: 0.0427\n",
      "Epoch [119/150], Validation Loss: 0.0426\n",
      "Epoch [120/150], Validation Loss: 0.0427\n",
      "Epoch [121/150], Validation Loss: 0.0422\n",
      "Epoch [122/150], Validation Loss: 0.0422\n",
      "Epoch [123/150], Validation Loss: 0.0422\n",
      "Epoch [124/150], Validation Loss: 0.0422\n",
      "Epoch [125/150], Validation Loss: 0.0427\n",
      "Epoch [126/150], Validation Loss: 0.0422\n",
      "Epoch [127/150], Validation Loss: 0.0423\n",
      "Epoch [128/150], Validation Loss: 0.0425\n",
      "Epoch [129/150], Validation Loss: 0.0423\n",
      "Epoch [130/150], Validation Loss: 0.0421\n",
      "Epoch [131/150], Validation Loss: 0.0422\n",
      "Epoch [132/150], Validation Loss: 0.0422\n",
      "Epoch [133/150], Validation Loss: 0.0424\n",
      "Epoch [134/150], Validation Loss: 0.0424\n",
      "Epoch [135/150], Validation Loss: 0.0426\n",
      "Epoch [136/150], Validation Loss: 0.0420\n",
      "Epoch [137/150], Validation Loss: 0.0426\n",
      "Epoch [138/150], Validation Loss: 0.0422\n",
      "Epoch [139/150], Validation Loss: 0.0425\n",
      "Epoch [140/150], Validation Loss: 0.0423\n",
      "Epoch [141/150], Validation Loss: 0.0424\n",
      "Epoch [142/150], Validation Loss: 0.0426\n",
      "Epoch [143/150], Validation Loss: 0.0424\n",
      "Epoch [144/150], Validation Loss: 0.0423\n",
      "Epoch [145/150], Validation Loss: 0.0420\n",
      "Epoch [146/150], Validation Loss: 0.0426\n",
      "Epoch [147/150], Validation Loss: 0.0422\n",
      "Epoch [148/150], Validation Loss: 0.0420\n",
      "Epoch [149/150], Validation Loss: 0.0424\n",
      "Epoch [150/150], Validation Loss: 0.0416\n",
      "MAE: 0.1564, MSE: 0.0424, RMSE: 0.2059\n"
     ]
    }
   ],
   "source": [
    "########################################## Hyperparameters ##########################################\n",
    "input_dim = len(train_set[0][0][0]) # 63\n",
    "output_dim = 24*56\n",
    "hidden_dim = 256\n",
    "n_layers = 9\n",
    "learning_rate = 0.001\n",
    "num_epochs = 150\n",
    "batch_size = 64\n",
    "########################################################################################################\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss and optimizer\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "########################################## Training loop ##########################################\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data, targets in val_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "# DataLoader for test set\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        all_targets.append(targets.numpy())\n",
    "        all_outputs.append(outputs.numpy())\n",
    "\n",
    "# Flatten targets and outputs to calculate metrics\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# Calculate MAE, MSE and RMSE\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하고 싶으면 이 코드 실행. 저장 안하고 싶으면 그냥 패스\n",
    "results_folder = \"/home/kimyirum/EMS/ict-2023-ems/load/results\"\n",
    "hyperparams = {\n",
    "    '\\tlearning_rate': learning_rate,\n",
    "    '\\tbatch_size': batch_size,\n",
    "    '\\tnum_epochs': num_epochs,\n",
    "    '\\thidden_dim': hidden_dim,\n",
    "    '\\tn_layers': n_layers\n",
    "}\n",
    "metrics = {\n",
    "    '\\tMAE': mae,\n",
    "    '\\tMSE': mse,\n",
    "    '\\tRMSE': rmse,\n",
    "}\n",
    "\n",
    "now = datetime.now()\n",
    "now_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "filename_metrics = f'{now_str}.txt'\n",
    "filename_model = f'model_{now_str}.pt'\n",
    "\n",
    "# Save the model parameters\n",
    "torch.save(model.state_dict(), results_folder+filename_model)\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open(results_folder+filename_metrics, 'w') as f:\n",
    "    # Write hyperparameters\n",
    "    f.write('Hyperparameters:\\n')\n",
    "    for key, value in hyperparams.items():\n",
    "        f.write(f'{key}: {value}\\n')\n",
    "    \n",
    "    # Write metrics\n",
    "    f.write('\\nMetrics:\\n')\n",
    "    for key, value in metrics.items():\n",
    "        f.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for next 24 hours:\n",
      "      0_SV-2    1_SV-5    2_SV-6    3_SV-7  4_HV-NM1  5_HV-NM2   6_고압콘덴샤  \\\n",
      "0  -0.249648 -0.248628 -0.001138 -0.256933 -0.001955 -0.290911 -0.026695   \n",
      "1  -0.249591 -0.250597  0.001241 -0.253004  0.000565 -0.284044 -0.028964   \n",
      "2  -0.246761 -0.246217 -0.000619 -0.257665 -0.000331 -0.277790 -0.027177   \n",
      "3  -0.242882 -0.241013 -0.000384 -0.249653 -0.000302 -0.271895 -0.028800   \n",
      "4  -0.240177 -0.240528  0.000733 -0.244750 -0.000471 -0.270508 -0.029942   \n",
      "5  -0.241743 -0.240013  0.000709 -0.241505 -0.001707 -0.262848 -0.030394   \n",
      "6  -0.237556 -0.241275 -0.000451 -0.245925  0.001141 -0.259856 -0.032543   \n",
      "7  -0.236888 -0.237028 -0.000368 -0.244763  0.000112 -0.262263 -0.031567   \n",
      "8  -0.234855 -0.235198 -0.000201 -0.239928 -0.000184 -0.252539 -0.029305   \n",
      "9  -0.235916 -0.234361 -0.000439 -0.237787 -0.001414 -0.245139 -0.032982   \n",
      "10 -0.229464 -0.227502 -0.000296 -0.234399  0.000786 -0.247575 -0.032889   \n",
      "11 -0.228833 -0.224779 -0.001018 -0.237540  0.001602 -0.242909 -0.030011   \n",
      "12 -0.221906 -0.223336 -0.003248 -0.232904  0.001010 -0.241776 -0.034734   \n",
      "13 -0.219672 -0.223872  0.000077 -0.234001 -0.000937 -0.240063 -0.035237   \n",
      "14 -0.217755 -0.218263 -0.001259 -0.228225  0.000804 -0.238645 -0.035678   \n",
      "15 -0.214316 -0.215637  0.001027 -0.228188  0.001566 -0.239136 -0.035392   \n",
      "16 -0.216029 -0.212101  0.004038 -0.224029 -0.000225 -0.235235 -0.036943   \n",
      "17 -0.208527 -0.209030  0.000730 -0.217323  0.000255 -0.236486 -0.039703   \n",
      "18 -0.199760 -0.201742  0.000374 -0.217919  0.000206 -0.233475 -0.039654   \n",
      "19 -0.202262 -0.200888  0.001103 -0.215836  0.000687 -0.234467 -0.039150   \n",
      "20 -0.194676 -0.192091  0.000719 -0.206373  0.000187 -0.235156 -0.037214   \n",
      "21 -0.191931 -0.190518  0.001837 -0.206746  0.001175 -0.230872 -0.037163   \n",
      "22 -0.190680 -0.189137 -0.001133 -0.200318 -0.000309 -0.229007 -0.035349   \n",
      "23 -0.186230 -0.188491 -0.000926 -0.197406 -0.001282 -0.227784 -0.038988   \n",
      "\n",
      "    7_신재생에너지동    8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동  47_기숙사9동  48_고등광/극초단  \\\n",
      "0   -0.238892  0.025581   0.019549  ...     0.022965  0.020172    0.052720   \n",
      "1   -0.236953  0.025625   0.020585  ...     0.015917  0.022002    0.058384   \n",
      "2   -0.231554  0.028654   0.021850  ...     0.025343  0.017036    0.056965   \n",
      "3   -0.234591  0.023155   0.020586  ...     0.027495  0.017526    0.059949   \n",
      "4   -0.231875  0.026308   0.021747  ...     0.026877  0.015749    0.059987   \n",
      "5   -0.229016  0.029586   0.019627  ...     0.029414  0.016907    0.056864   \n",
      "6   -0.228346  0.030413   0.019197  ...     0.019146  0.021390    0.060204   \n",
      "7   -0.227639  0.026525   0.021217  ...     0.027107  0.020291    0.061218   \n",
      "8   -0.223119  0.030261   0.023465  ...     0.026158  0.021616    0.050161   \n",
      "9   -0.223215  0.029776   0.022502  ...     0.027636  0.018631    0.054579   \n",
      "10  -0.218741  0.020860   0.020482  ...     0.027096  0.022821    0.053633   \n",
      "11  -0.215314  0.017773   0.021795  ...     0.026895  0.020497    0.064522   \n",
      "12  -0.213169  0.022378   0.023138  ...     0.028208  0.019374    0.064095   \n",
      "13  -0.215120  0.019733   0.020086  ...     0.022491  0.021218    0.055772   \n",
      "14  -0.211503  0.019703   0.021865  ...     0.025514  0.019303    0.055965   \n",
      "15  -0.209263  0.022208   0.019927  ...     0.024214  0.020981    0.057187   \n",
      "16  -0.209845  0.021615   0.019976  ...     0.027892  0.018055    0.052000   \n",
      "17  -0.211795  0.026154   0.020694  ...     0.028787  0.019296    0.058088   \n",
      "18  -0.207473  0.022501   0.021781  ...     0.023464  0.021660    0.058387   \n",
      "19  -0.206469  0.022744   0.022474  ...     0.012666  0.019345    0.050356   \n",
      "20  -0.203859  0.028596   0.021353  ...     0.024555  0.018892    0.038471   \n",
      "21  -0.199223  0.020245   0.017747  ...     0.026090  0.023487    0.051775   \n",
      "22  -0.200516  0.020123   0.021437  ...     0.027897  0.017519    0.053921   \n",
      "23  -0.202617  0.023810   0.019018  ...     0.031593  0.019835    0.047316   \n",
      "\n",
      "    49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  53_LG도서관(E)  \\\n",
      "0       0.014253          0.024064     0.303498     0.009464     0.014901   \n",
      "1       0.013509          0.025121     0.305396     0.010171     0.014688   \n",
      "2       0.013053          0.027640     0.309043     0.009382     0.015080   \n",
      "3       0.015980          0.020288     0.308214     0.008280     0.015036   \n",
      "4       0.017785          0.027261     0.308125     0.005874     0.015444   \n",
      "5       0.016390          0.020994     0.309911     0.012329     0.017348   \n",
      "6       0.014729          0.023756     0.310021     0.014606     0.017197   \n",
      "7       0.018684          0.021376     0.311279     0.013121     0.018975   \n",
      "8       0.018211          0.017941     0.310426     0.016102     0.016840   \n",
      "9       0.017438          0.021915     0.310331     0.011677     0.018879   \n",
      "10      0.018737          0.023385     0.313517     0.010168     0.016324   \n",
      "11      0.020488          0.010097     0.315406     0.017742     0.018359   \n",
      "12      0.017655          0.020617     0.316612     0.016722     0.016659   \n",
      "13      0.020750          0.014873     0.316925     0.017533     0.020471   \n",
      "14      0.019772          0.019103     0.315618     0.017535     0.019243   \n",
      "15      0.016430          0.015841     0.317634     0.010474     0.019513   \n",
      "16      0.017995          0.017335     0.318893     0.013626     0.017807   \n",
      "17      0.020082          0.013525     0.325896     0.014989     0.016652   \n",
      "18      0.017768          0.017866     0.326352     0.011637     0.015655   \n",
      "19      0.015530          0.022838     0.324143     0.005420     0.017619   \n",
      "20      0.012714          0.022020     0.328138     0.009747     0.015311   \n",
      "21      0.019569          0.019798     0.330928     0.011061     0.016211   \n",
      "22      0.015307          0.015314     0.333970     0.009903     0.013521   \n",
      "23      0.015957          0.017968     0.331457     0.005072     0.017272   \n",
      "\n",
      "    54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0      0.197053      0.011991  \n",
      "1      0.195427      0.013068  \n",
      "2      0.196091      0.012230  \n",
      "3      0.193829      0.012919  \n",
      "4      0.197215      0.013732  \n",
      "5      0.200779      0.011916  \n",
      "6      0.206913      0.014390  \n",
      "7      0.205925      0.014291  \n",
      "8      0.208210      0.012871  \n",
      "9      0.208382      0.016299  \n",
      "10     0.211713      0.013554  \n",
      "11     0.211051      0.018624  \n",
      "12     0.214671      0.012944  \n",
      "13     0.214517      0.012256  \n",
      "14     0.214567      0.015420  \n",
      "15     0.218297      0.012745  \n",
      "16     0.218904      0.013923  \n",
      "17     0.220140      0.013380  \n",
      "18     0.223518      0.010530  \n",
      "19     0.225600      0.012975  \n",
      "20     0.226672      0.014663  \n",
      "21     0.229259      0.015249  \n",
      "22     0.234502      0.011827  \n",
      "23     0.234279      0.012438  \n",
      "\n",
      "[24 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is the original dataset and it includes a 'date' column\n",
    "\n",
    "# Get the building names\n",
    "building_names = df.columns[-56:]  # adjust this as necessary\n",
    "\n",
    "# DataLoader for test set\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get the first sequence and its target from the test set\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# Switch model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "# Remove batch dimension, reshape and convert prediction to numpy array\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "\n",
    "# Reshape real_target and convert it to numpy array\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "# Calculate error (difference between real target and prediction)\n",
    "error = real_target - prediction\n",
    "\n",
    "# Create DataFrame for prediction\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
