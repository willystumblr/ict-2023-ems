{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # If there are any missing values, fill them with the previous value in time-series\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Normalize numerical columns to range [0, 1]\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # One-hot encode categorical variables\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # Drop original categorical columns and merge with encoded ones\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len]\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] # Assuming last 56 columns are power values\n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # flatten y values\n",
    "    \n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize hidden state\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize cell state\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the excel file\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data_KW.xlsx')\n",
    "\n",
    "# Initialize our dataset class\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# Define the split sizes for train, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split dataset\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Validation Loss: 0.0980, Save model\n",
      "Epoch [2/200], Validation Loss: 0.0509, Save model\n",
      "Epoch [3/200], Validation Loss: 0.0442, Save model\n",
      "Epoch [4/200], Validation Loss: 0.0428, Save model\n",
      "Epoch [5/200], Validation Loss: 0.0424, Save model\n",
      "Epoch [6/200], Validation Loss: 0.0416, Save model\n",
      "Epoch [7/200], Validation Loss: 0.0410, Save model\n",
      "Epoch [8/200], Validation Loss: 0.0412\n",
      "Epoch [9/200], Validation Loss: 0.0411\n",
      "Epoch [10/200], Validation Loss: 0.0405, Save model\n",
      "Epoch [11/200], Validation Loss: 0.0408\n",
      "Epoch [12/200], Validation Loss: 0.0397, Save model\n",
      "Epoch [13/200], Validation Loss: 0.0401\n",
      "Epoch [14/200], Validation Loss: 0.0400\n",
      "Epoch [15/200], Validation Loss: 0.0400\n",
      "Epoch [16/200], Validation Loss: 0.0395, Save model\n",
      "Epoch [17/200], Validation Loss: 0.0392, Save model\n",
      "Epoch [18/200], Validation Loss: 0.0391, Save model\n",
      "Epoch [19/200], Validation Loss: 0.0381, Save model\n",
      "Epoch [20/200], Validation Loss: 0.0389\n",
      "Epoch [21/200], Validation Loss: 0.0383\n",
      "Epoch [22/200], Validation Loss: 0.0389\n",
      "Epoch [23/200], Validation Loss: 0.0387\n",
      "Epoch [24/200], Validation Loss: 0.0386\n",
      "Epoch [25/200], Validation Loss: 0.0382\n",
      "Epoch [26/200], Validation Loss: 0.0391\n",
      "Epoch [27/200], Validation Loss: 0.0393\n",
      "Epoch [28/200], Validation Loss: 0.0388\n",
      "Epoch [29/200], Validation Loss: 0.0378, Save model\n",
      "Epoch [30/200], Validation Loss: 0.0371, Save model\n",
      "Epoch [31/200], Validation Loss: 0.0340, Save model\n",
      "Epoch [32/200], Validation Loss: 0.0312, Save model\n",
      "Epoch [33/200], Validation Loss: 0.0257, Save model\n",
      "Epoch [34/200], Validation Loss: 0.0191, Save model\n",
      "Epoch [35/200], Validation Loss: 0.0180, Save model\n",
      "Epoch [36/200], Validation Loss: 0.0213\n",
      "Epoch [37/200], Validation Loss: 0.0184\n",
      "Epoch [38/200], Validation Loss: 0.0169, Save model\n",
      "Epoch [39/200], Validation Loss: 0.0182\n",
      "Epoch [40/200], Validation Loss: 0.0152, Save model\n",
      "Epoch [41/200], Validation Loss: 0.0168\n",
      "Epoch [42/200], Validation Loss: 0.0152\n",
      "Epoch [43/200], Validation Loss: 0.0141, Save model\n",
      "Epoch [44/200], Validation Loss: 0.0137, Save model\n",
      "Epoch [45/200], Validation Loss: 0.0129, Save model\n",
      "Epoch [46/200], Validation Loss: 0.0128, Save model\n",
      "Epoch [47/200], Validation Loss: 0.0127, Save model\n",
      "Epoch [48/200], Validation Loss: 0.0123, Save model\n",
      "Epoch [49/200], Validation Loss: 0.0124\n",
      "Epoch [50/200], Validation Loss: 0.0125\n",
      "Epoch [51/200], Validation Loss: 0.0118, Save model\n",
      "Epoch [52/200], Validation Loss: 0.0120\n",
      "Epoch [53/200], Validation Loss: 0.0115, Save model\n",
      "Epoch [54/200], Validation Loss: 0.0114, Save model\n",
      "Epoch [55/200], Validation Loss: 0.0112, Save model\n",
      "Epoch [56/200], Validation Loss: 0.0115\n",
      "Epoch [57/200], Validation Loss: 0.0109, Save model\n",
      "Epoch [58/200], Validation Loss: 0.0111\n",
      "Epoch [59/200], Validation Loss: 0.0106, Save model\n",
      "Epoch [60/200], Validation Loss: 0.0107\n",
      "Epoch [61/200], Validation Loss: 0.0105, Save model\n",
      "Epoch [62/200], Validation Loss: 0.0103, Save model\n",
      "Epoch [63/200], Validation Loss: 0.0102, Save model\n",
      "Epoch [64/200], Validation Loss: 0.0100, Save model\n",
      "Epoch [65/200], Validation Loss: 0.0099, Save model\n",
      "Epoch [66/200], Validation Loss: 0.0098, Save model\n",
      "Epoch [67/200], Validation Loss: 0.0097, Save model\n",
      "Epoch [68/200], Validation Loss: 0.0097\n",
      "Epoch [69/200], Validation Loss: 0.0095, Save model\n",
      "Epoch [70/200], Validation Loss: 0.0091, Save model\n",
      "Epoch [71/200], Validation Loss: 0.0092\n",
      "Epoch [72/200], Validation Loss: 0.0089, Save model\n",
      "Epoch [73/200], Validation Loss: 0.0088, Save model\n",
      "Epoch [74/200], Validation Loss: 0.0087, Save model\n",
      "Epoch [75/200], Validation Loss: 0.0090\n",
      "Epoch [76/200], Validation Loss: 0.0088\n",
      "Epoch [77/200], Validation Loss: 0.0102\n",
      "Epoch [78/200], Validation Loss: 0.0087\n",
      "Epoch [79/200], Validation Loss: 0.0087, Save model\n",
      "Epoch [80/200], Validation Loss: 0.0088\n",
      "Epoch [81/200], Validation Loss: 0.0100\n",
      "Epoch [82/200], Validation Loss: 0.0130\n",
      "Epoch [83/200], Validation Loss: 0.0112\n",
      "Epoch [84/200], Validation Loss: 0.0099\n",
      "Epoch [85/200], Validation Loss: 0.0097\n",
      "Epoch [86/200], Validation Loss: 0.0088\n",
      "Epoch [87/200], Validation Loss: 0.0091\n",
      "Epoch [88/200], Validation Loss: 0.0085, Save model\n",
      "Epoch [89/200], Validation Loss: 0.0083, Save model\n",
      "Epoch [90/200], Validation Loss: 0.0083, Save model\n",
      "Epoch [91/200], Validation Loss: 0.0083, Save model\n",
      "Epoch [92/200], Validation Loss: 0.0083\n",
      "Epoch [93/200], Validation Loss: 0.0083\n",
      "Epoch [94/200], Validation Loss: 0.0082, Save model\n",
      "Epoch [95/200], Validation Loss: 0.0084\n",
      "Epoch [96/200], Validation Loss: 0.0082, Save model\n",
      "Epoch [97/200], Validation Loss: 0.0083\n",
      "Epoch [98/200], Validation Loss: 0.0082, Save model\n",
      "Epoch [99/200], Validation Loss: 0.0082\n",
      "Epoch [100/200], Validation Loss: 0.0082\n",
      "Epoch [101/200], Validation Loss: 0.0081, Save model\n",
      "Epoch [102/200], Validation Loss: 0.0081\n",
      "Epoch [103/200], Validation Loss: 0.0081\n",
      "Epoch [104/200], Validation Loss: 0.0081, Save model\n",
      "Epoch [105/200], Validation Loss: 0.0080, Save model\n",
      "Epoch [106/200], Validation Loss: 0.0080, Save model\n",
      "Epoch [107/200], Validation Loss: 0.0081\n",
      "Epoch [108/200], Validation Loss: 0.0080\n",
      "Epoch [109/200], Validation Loss: 0.0080\n",
      "Epoch [110/200], Validation Loss: 0.0080\n",
      "Epoch [111/200], Validation Loss: 0.0080, Save model\n",
      "Epoch [112/200], Validation Loss: 0.0079, Save model\n",
      "Epoch [113/200], Validation Loss: 0.0079\n",
      "Epoch [114/200], Validation Loss: 0.0079\n",
      "Epoch [115/200], Validation Loss: 0.0080\n",
      "Epoch [116/200], Validation Loss: 0.0079\n",
      "Epoch [117/200], Validation Loss: 0.0079\n",
      "Epoch [118/200], Validation Loss: 0.0079, Save model\n",
      "Epoch [119/200], Validation Loss: 0.0079\n",
      "Epoch [120/200], Validation Loss: 0.0079, Save model\n",
      "Epoch [121/200], Validation Loss: 0.0079\n",
      "Epoch [122/200], Validation Loss: 0.0079, Save model\n",
      "Epoch [123/200], Validation Loss: 0.0078, Save model\n",
      "Epoch [124/200], Validation Loss: 0.0078\n",
      "Epoch [125/200], Validation Loss: 0.0078, Save model\n",
      "Epoch [126/200], Validation Loss: 0.0078\n",
      "Epoch [127/200], Validation Loss: 0.0078\n",
      "Epoch [128/200], Validation Loss: 0.0078\n",
      "Epoch [129/200], Validation Loss: 0.0078, Save model\n",
      "Epoch [130/200], Validation Loss: 0.0078, Save model\n",
      "Epoch [131/200], Validation Loss: 0.0077, Save model\n",
      "Epoch [132/200], Validation Loss: 0.0077, Save model\n",
      "Epoch [133/200], Validation Loss: 0.0078\n",
      "Epoch [134/200], Validation Loss: 0.0079\n",
      "Epoch [135/200], Validation Loss: 0.0079\n",
      "Epoch [136/200], Validation Loss: 0.0078\n",
      "Epoch [137/200], Validation Loss: 0.0078\n",
      "Epoch [138/200], Validation Loss: 0.0078\n",
      "Epoch [139/200], Validation Loss: 0.0077, Save model\n",
      "Epoch [140/200], Validation Loss: 0.0077, Save model\n",
      "Epoch [141/200], Validation Loss: 0.0077\n",
      "Epoch [142/200], Validation Loss: 0.0078\n",
      "Epoch [143/200], Validation Loss: 0.0077, Save model\n",
      "Epoch [144/200], Validation Loss: 0.0077, Save model\n",
      "Epoch [145/200], Validation Loss: 0.0077\n",
      "Epoch [146/200], Validation Loss: 0.0077\n",
      "Epoch [147/200], Validation Loss: 0.0077\n",
      "Epoch [148/200], Validation Loss: 0.0078\n",
      "Epoch [149/200], Validation Loss: 0.0077\n",
      "Epoch [150/200], Validation Loss: 0.0077\n",
      "Epoch [151/200], Validation Loss: 0.0077\n",
      "Epoch [152/200], Validation Loss: 0.0076, Save model\n",
      "Epoch [153/200], Validation Loss: 0.0076, Save model\n",
      "Epoch [154/200], Validation Loss: 0.0076, Save model\n",
      "Epoch [155/200], Validation Loss: 0.0076\n",
      "Epoch [156/200], Validation Loss: 0.0076, Save model\n",
      "Epoch [157/200], Validation Loss: 0.0077\n",
      "Epoch [158/200], Validation Loss: 0.0076\n",
      "Epoch [159/200], Validation Loss: 0.0076\n",
      "Epoch [160/200], Validation Loss: 0.0078\n",
      "Epoch [161/200], Validation Loss: 0.0076\n",
      "Epoch [162/200], Validation Loss: 0.0077\n",
      "Epoch [163/200], Validation Loss: 0.0075, Save model\n",
      "Epoch [164/200], Validation Loss: 0.0075, Save model\n",
      "Epoch [165/200], Validation Loss: 0.0075\n",
      "Epoch [166/200], Validation Loss: 0.0076\n",
      "Epoch [167/200], Validation Loss: 0.0075, Save model\n",
      "Epoch [168/200], Validation Loss: 0.0076\n",
      "Epoch [169/200], Validation Loss: 0.0076\n",
      "Epoch [170/200], Validation Loss: 0.0077\n",
      "Epoch [171/200], Validation Loss: 0.0075, Save model\n",
      "Epoch [172/200], Validation Loss: 0.0075, Save model\n",
      "Epoch [173/200], Validation Loss: 0.0075\n",
      "Epoch [174/200], Validation Loss: 0.0074, Save model\n",
      "Epoch [175/200], Validation Loss: 0.0075\n",
      "Epoch [176/200], Validation Loss: 0.0075\n",
      "Epoch [177/200], Validation Loss: 0.0074, Save model\n",
      "Epoch [178/200], Validation Loss: 0.0074, Save model\n",
      "Epoch [179/200], Validation Loss: 0.0074\n",
      "Epoch [180/200], Validation Loss: 0.0074\n",
      "Epoch [181/200], Validation Loss: 0.0074, Save model\n",
      "Epoch [182/200], Validation Loss: 0.0075\n",
      "Epoch [183/200], Validation Loss: 0.0074\n",
      "Epoch [184/200], Validation Loss: 0.0074\n",
      "Epoch [185/200], Validation Loss: 0.0074\n",
      "Epoch [186/200], Validation Loss: 0.0074\n",
      "Epoch [187/200], Validation Loss: 0.0075\n",
      "Epoch [188/200], Validation Loss: 0.0076\n",
      "Epoch [189/200], Validation Loss: 0.0075\n",
      "Epoch [190/200], Validation Loss: 0.0074, Save model\n",
      "Epoch [191/200], Validation Loss: 0.0075\n",
      "Epoch [192/200], Validation Loss: 0.0074\n",
      "Epoch [193/200], Validation Loss: 0.0074\n",
      "Epoch [194/200], Validation Loss: 0.0075\n",
      "Epoch [195/200], Validation Loss: 0.0076\n",
      "Epoch [196/200], Validation Loss: 0.0075\n",
      "Epoch [197/200], Validation Loss: 0.0074, Save model\n",
      "Epoch [198/200], Validation Loss: 0.0075\n",
      "Epoch [199/200], Validation Loss: 0.0074\n",
      "Epoch [200/200], Validation Loss: 0.0073, Save model\n",
      "MAE: 0.0577, MSE: 0.0074, RMSE: 0.0858\n"
     ]
    }
   ],
   "source": [
    "########################################## Hyperparameters ##########################################\n",
    "input_dim = len(train_set[0][0][0]) # 63\n",
    "output_dim = 24*56\n",
    "hidden_dim = 128\n",
    "n_layers = 7\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "use_thread = True\n",
    "patience = 20 # number of epochs to wait before stopping\n",
    "########################################################################################################\n",
    "\n",
    "results_folder = \"/home/kimyirum/EMS/ict-2023-ems/load/results/\"\n",
    "now = datetime.now()\n",
    "now_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "filename_metrics = f'{now_str}.pkl'\n",
    "filename_model = f'model_{now_str}.pt'\n",
    "\n",
    "# DataLoader\n",
    "if use_thread:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "else:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_val_loss = float(\"inf\") # initially set to infinity\n",
    "no_improve_epoch = 0\n",
    "\n",
    "########################################## Training loop ##########################################\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data, targets in val_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}', end='')\n",
    "        \n",
    "        # save model if validation loss has decreased\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\", Save model\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), results_folder+filename_model)\n",
    "            no_improve_epoch = 0\n",
    "        else:\n",
    "            print(\"\")\n",
    "            no_improve_epoch += 1\n",
    "            \n",
    "        # early stopping\n",
    "        if no_improve_epoch > patience:\n",
    "            print('Early stopping...')\n",
    "            break\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        all_targets.append(targets.numpy())\n",
    "        all_outputs.append(outputs.numpy())\n",
    "\n",
    "# Flatten targets and outputs to calculate metrics\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# Calculate MAE, MSE and RMSE\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "#### THIS IS NORMALIZATION VALUE(for hyperparams decisions) ####\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 200, 'hidden_dim': 128, 'n_layers': 7}\n",
      "MinMaxScaler()\n",
      "{'MAE': 18.218121912826643, 'MSE': 2152.984489741096, 'RMSE': 46.4002638973217}\n"
     ]
    }
   ],
   "source": [
    "# Save hyperparams\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'num_epochs': num_epochs,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'n_layers': n_layers\n",
    "}\n",
    "\n",
    "scalers = dataset.scaler\n",
    "\n",
    "metrics = {\n",
    "    'MAE': mae,\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "}\n",
    "\n",
    "# Combine all results in one dictionary\n",
    "results = {\n",
    "    'Hyperparameters': hyperparams,\n",
    "    'Scalers': scalers,\n",
    "    'Metrics': metrics\n",
    "}\n",
    "\n",
    "# Save results to a pickle file\n",
    "with open(results_folder + filename_metrics, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "\n",
    "# load test\n",
    "# Load results from a pickle file\n",
    "with open(results_folder + filename_metrics, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "\n",
    "# Now you can access your data from the loaded_results dictionary\n",
    "print(loaded_results['Hyperparameters'])\n",
    "print(loaded_results['Scalers'])\n",
    "print(loaded_results['Metrics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1344]) torch.Size([1, 1344])\n",
      "Error for next 24 hours:\n",
      "        0_SV-2      1_SV-5    2_SV-6      3_SV-7  4_HV-NM1    5_HV-NM2  \\\n",
      "0    34.999727   43.316643 -0.002724  -64.748983 -0.002985  -33.152381   \n",
      "1   -22.999336  -21.006086  0.017691  -33.645925  0.001599  -55.149559   \n",
      "2   -15.782143  -29.522147  0.003001  -21.852019 -0.003783  -45.822555   \n",
      "3   -36.413204  -37.414701  0.012502  -31.395420  0.010298  -26.476467   \n",
      "4   -34.264336  -60.623101 -0.003591  -85.109270  0.006343  -80.833973   \n",
      "5   -14.389139    2.019778 -0.002724  -66.978987 -0.009732  -48.792995   \n",
      "6   -37.347950  -67.744371 -0.003175  -58.777598  0.000348  -55.698626   \n",
      "7    -0.381227   37.641380 -0.002045   -0.886145 -0.007057  -38.450345   \n",
      "8  -118.103103  -74.996083  0.000350 -108.466762  0.006029 -113.058033   \n",
      "9   -20.065016  -33.878703  0.013398  -20.072160 -0.010602  -18.670337   \n",
      "10   38.413012   18.603810  0.007740    3.193779 -0.000165    7.416982   \n",
      "11    7.844704   -2.898762 -0.009515    3.568728  0.013795   26.311046   \n",
      "12  -26.929821  -30.645191 -0.005076  -67.893607 -0.003529  -40.763493   \n",
      "13  -50.303700 -115.808204  0.024573  -36.533059 -0.003108  -34.384832   \n",
      "14  -79.460869 -107.095277  0.001854  -68.770200  0.001758  -55.214165   \n",
      "15  -34.926155  -40.227210 -0.000636  -37.772436  0.002514   13.904445   \n",
      "16  -81.852655  -85.962308  0.007665  -58.200081 -0.003204  -77.607015   \n",
      "17  -43.889300  -32.931123  0.014320  -94.918948  0.015986  -66.951804   \n",
      "18 -133.194026 -127.686971  0.009680  -70.486711 -0.008899 -115.696699   \n",
      "19 -112.756010 -115.915442 -0.003112 -114.795839  0.006598 -108.447579   \n",
      "20  -28.323760  -30.519186  0.001274    0.698085  0.005657  -18.679566   \n",
      "21   57.791762   48.888366  0.007183   53.695648 -0.001467   51.873302   \n",
      "22   34.664540   20.491851 -0.004872   21.873510  0.001943   -5.048945   \n",
      "23  -24.623619    7.578558  0.001301   10.617922  0.009312    6.682703   \n",
      "\n",
      "       6_고압콘덴샤  7_신재생에너지동     8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동   47_기숙사9동  \\\n",
      "0    23.647461 -39.910942  -0.775522  -1.691747  ...    15.040769  -0.469018   \n",
      "1    24.411612 -10.769347  -1.932982   7.655249  ...     7.150326  -2.418927   \n",
      "2    11.413727 -10.491141 -16.461711  -1.430167  ...    23.510836  -2.760663   \n",
      "3    -3.056551   3.276024 -14.494769  -6.702381  ...     2.570248   1.028000   \n",
      "4   -34.763110 -11.303775 -15.259463 -17.721792  ...     2.813112  -1.790290   \n",
      "5    44.584882 -33.716434 -11.002131  -5.082770  ...    -2.401454  -7.175061   \n",
      "6    11.125582 -18.459743 -32.372207   5.064190  ...   -14.890917   3.787303   \n",
      "7   -20.638222 -30.665288 -11.244349  -1.231727  ...   -31.259739  -2.607512   \n",
      "8    41.882425 -50.443489 -21.272087   1.320053  ...   -45.310086 -13.000210   \n",
      "9    -1.575294 -22.208965 -32.335111   3.355480  ...   -44.206407  -4.206373   \n",
      "10   48.721284 -24.551393 -24.279790  17.370298  ...   -18.810170   1.624830   \n",
      "11   85.758532 -53.130695 -15.045541   2.116213  ...    -6.335244  -9.750220   \n",
      "12  109.670696 -39.325805 -14.853065   3.786599  ...   -24.525487  -6.650672   \n",
      "13   83.248314 -41.693517 -11.722070  -1.867230  ...   -37.859826  -4.481428   \n",
      "14  103.653553 -48.271729 -33.723361   6.499934  ...   -19.257958  -3.266472   \n",
      "15   71.972466  23.361396  10.747743  -8.304919  ...     8.504343   1.777686   \n",
      "16   92.545487 -84.720536 -30.729601  12.818979  ...    29.084514   9.621428   \n",
      "17  170.947235 -30.314590  -4.216290  -3.315115  ...   -10.478960   4.497425   \n",
      "18 -482.119572 -36.006194  -4.086292   1.606530  ...   -13.571143  -4.597890   \n",
      "19 -453.721557 -55.258848 -21.076907  -7.501508  ...   -12.092630  -0.807035   \n",
      "20 -507.596617 -15.863212  37.645972  -8.366331  ...    -4.559921  -0.179118   \n",
      "21 -528.897424 -15.460745  28.869904  10.291298  ...     1.846302   2.381774   \n",
      "22 -592.030247 -19.326767   4.300791   5.219519  ...    -5.078115   7.172912   \n",
      "23 -621.553875   4.404931 -20.989236  16.230729  ...    18.342401   4.168862   \n",
      "\n",
      "    48_고등광/극초단  49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  \\\n",
      "0    22.157787     -5.678525         -1.892295     1.346354    -3.738328   \n",
      "1     9.772139     -4.561298          1.936286     2.533462    -1.689442   \n",
      "2    16.812793     -3.754187         -4.313938     2.213487     9.642733   \n",
      "3    59.227898     -2.677683          5.371777     8.079899     0.614189   \n",
      "4    53.503883     -3.093952         -2.971760     2.077150     5.677988   \n",
      "5   -43.475650     -1.721312         -7.223988    -4.373520    -7.680066   \n",
      "6   -55.534651     -5.115905         -2.584187     8.467500    -0.487036   \n",
      "7    -9.377968     -4.614808          3.071659     0.073529     1.152018   \n",
      "8    -9.858674      2.227974          0.467573    -5.338446    -3.020384   \n",
      "9     2.156228     -5.420684         -0.850907    -8.867264     0.172809   \n",
      "10  -16.561282      2.947159         -5.967645    -2.086965    -0.946511   \n",
      "11    7.116916      2.311469         -1.599578    -4.371047    -1.074612   \n",
      "12    1.380818      5.008512         -4.671595   -10.356407    -1.871820   \n",
      "13   22.036299      0.443182         -6.906940    -6.023902    -2.342831   \n",
      "14    5.216818      0.038018          0.412691   -15.093862    -0.295775   \n",
      "15  103.930253      2.525309          0.678200    -8.903586    -1.921558   \n",
      "16   77.259451     -2.859730         -0.941643    -5.507164    -2.109975   \n",
      "17   -6.022009     -6.223767         -6.054325     1.108509    -1.110716   \n",
      "18   -0.398476     -0.891068         -3.368674    -1.069013     1.397499   \n",
      "19  -19.963409     -3.194429         -7.005972     5.909926    -1.329109   \n",
      "20   33.718697     -0.780740         -4.593067     2.108892    -1.500311   \n",
      "21   42.465338     -3.928561          3.453363     3.487830    -6.314384   \n",
      "22   58.597993     -4.654496          3.231356     1.782648    -6.480799   \n",
      "23   13.074172      1.421211         -3.941203     1.950911    -0.738299   \n",
      "\n",
      "    53_LG도서관(E)  54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0     -0.284081    -2.853632      3.078824  \n",
      "1      0.614552     0.911011      2.940400  \n",
      "2      2.900941    -2.176846      3.347251  \n",
      "3      2.636265    -2.816119     -1.370879  \n",
      "4     -9.144583   -16.213317      3.416233  \n",
      "5      5.380507    -8.367115      5.440286  \n",
      "6     -7.605968    -3.049177     -7.810576  \n",
      "7     -5.181519   -29.087871      0.358332  \n",
      "8      4.210071   -16.382258      4.035818  \n",
      "9     -0.271996    18.857098      8.108612  \n",
      "10     8.320948    10.377367     -2.937900  \n",
      "11     1.352192   -17.738700      9.275115  \n",
      "12     0.781336   -18.200920      3.714865  \n",
      "13     8.234532    -8.910337     -0.511989  \n",
      "14    -2.313440    16.830501     -1.546614  \n",
      "15    -3.961351    -7.580856      4.452076  \n",
      "16    -7.783181    27.509260      2.475257  \n",
      "17     5.300233   -27.569639      2.424628  \n",
      "18     0.279914    -7.655184      4.030105  \n",
      "19    -6.444322     4.646188     -4.048883  \n",
      "20    -4.579213     9.042106     -0.281940  \n",
      "21     7.385513    -4.380558      3.122164  \n",
      "22    -3.293885     1.763667      1.601911  \n",
      "23    -0.197265    -3.898095      1.735347  \n",
      "\n",
      "[24 rows x 56 columns]\n",
      "MAE: 18.2181, MSE: 2152.9845, RMSE: 46.4003 (no normalization)\n",
      "err_total:  -7928.902673650116\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is the original dataset and it includes a 'date' column\n",
    "\n",
    "# Get the building names\n",
    "building_names = df.columns[-56:]  # adjust this as necessary\n",
    "\n",
    "# DataLoader for test set\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get the first sequence and its target from the test set\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# Switch model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "print(prediction.shape, real_target.shape)\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "padding = np.zeros((prediction.shape[0], 7))\n",
    "prediction_pad = np.hstack((padding, prediction))\n",
    "real_target_pad = np.hstack((padding, real_target))\n",
    "# print(prediction_pad.shape, real_target_pad.shape)\n",
    "\n",
    "# Apply inverse transformation\n",
    "prediction_inv = dataset.scaler.inverse_transform(prediction_pad)\n",
    "real_target_inv = dataset.scaler.inverse_transform(real_target_pad)\n",
    "\n",
    "# Delete the first 7 columns\n",
    "prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "real_target_inv = np.delete(real_target_inv, np.s_[:7], axis=1)\n",
    "\n",
    "# Reshape them back to the original shape\n",
    "prediction = prediction_inv.reshape(prediction.shape)\n",
    "real_target = real_target_inv.reshape(real_target.shape)\n",
    "\n",
    "\n",
    "# Calculate error (difference between real target and prediction)\n",
    "error = real_target - prediction\n",
    "\n",
    "\n",
    "# Create DataFrame for prediction\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "err_total = total = error_df.values.flatten().sum()\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n",
    "\n",
    "mae = mean_absolute_error(real_target, prediction)\n",
    "mse = mean_squared_error(real_target, prediction)\n",
    "rmse = sqrt(mse)\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f} (no normalization)')\n",
    "print(\"err_total: \", err_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
