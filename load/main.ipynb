{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # If there are any missing values, fill them with the previous value in time-series\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Normalize numerical columns to range [0, 1]\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # One-hot encode categorical variables\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # Drop original categorical columns and merge with encoded ones\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len]\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] # Assuming last 56 columns are power values\n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # flatten y values\n",
    "    \n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize hidden state\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize cell state\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the excel file\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data.xlsx')\n",
    "\n",
    "# Initialize our dataset class\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# Define the split sizes for train, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split dataset\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Validation Loss: 0.0906, Save model\n",
      "Epoch [2/200], Validation Loss: 0.0855, Save model\n",
      "Epoch [3/200], Validation Loss: 0.2604\n",
      "Epoch [4/200], Validation Loss: 0.0602, Save model\n",
      "Epoch [5/200], Validation Loss: 0.0512, Save model\n",
      "Epoch [6/200], Validation Loss: 0.0544\n",
      "Epoch [7/200], Validation Loss: 0.0452, Save model\n",
      "Epoch [8/200], Validation Loss: 0.0408, Save model\n",
      "Epoch [9/200], Validation Loss: 0.0333, Save model\n",
      "Epoch [10/200], Validation Loss: 0.0300, Save model\n",
      "Epoch [11/200], Validation Loss: 0.0292, Save model\n",
      "Epoch [12/200], Validation Loss: 0.0277, Save model\n",
      "Epoch [13/200], Validation Loss: 0.0272, Save model\n",
      "Epoch [14/200], Validation Loss: 0.0237, Save model\n",
      "Epoch [15/200], Validation Loss: 0.0217, Save model\n",
      "Epoch [16/200], Validation Loss: 0.0205, Save model\n",
      "Epoch [17/200], Validation Loss: 0.0191, Save model\n",
      "Epoch [18/200], Validation Loss: 0.0190, Save model\n",
      "Epoch [19/200], Validation Loss: 0.0172, Save model\n",
      "Epoch [20/200], Validation Loss: 0.0166, Save model\n",
      "Epoch [21/200], Validation Loss: 0.0162, Save model\n",
      "Epoch [22/200], Validation Loss: 0.0166\n",
      "Epoch [23/200], Validation Loss: 0.0188\n",
      "Epoch [24/200], Validation Loss: 0.0155, Save model\n",
      "Epoch [25/200], Validation Loss: 0.0150, Save model\n",
      "Epoch [26/200], Validation Loss: 0.0150\n",
      "Epoch [27/200], Validation Loss: 0.0142, Save model\n",
      "Epoch [28/200], Validation Loss: 0.0139, Save model\n",
      "Epoch [29/200], Validation Loss: 0.0139\n",
      "Epoch [30/200], Validation Loss: 0.0148\n",
      "Epoch [31/200], Validation Loss: 0.0128, Save model\n",
      "Epoch [32/200], Validation Loss: 0.0137\n",
      "Epoch [33/200], Validation Loss: 0.0120, Save model\n",
      "Epoch [34/200], Validation Loss: 0.0125\n",
      "Epoch [35/200], Validation Loss: 0.0132\n",
      "Epoch [36/200], Validation Loss: 0.0148\n",
      "Epoch [37/200], Validation Loss: 0.0113, Save model\n",
      "Epoch [38/200], Validation Loss: 0.0124\n",
      "Epoch [39/200], Validation Loss: 0.0112, Save model\n",
      "Epoch [40/200], Validation Loss: 0.0109, Save model\n",
      "Epoch [41/200], Validation Loss: 0.0097, Save model\n",
      "Epoch [42/200], Validation Loss: 0.0094, Save model\n",
      "Epoch [43/200], Validation Loss: 0.0127\n",
      "Epoch [44/200], Validation Loss: 0.0112\n",
      "Epoch [45/200], Validation Loss: 0.0095\n",
      "Epoch [46/200], Validation Loss: 0.0083, Save model\n",
      "Epoch [47/200], Validation Loss: 0.0078, Save model\n",
      "Epoch [48/200], Validation Loss: 0.0073, Save model\n",
      "Epoch [49/200], Validation Loss: 0.0076\n",
      "Epoch [50/200], Validation Loss: 0.0112\n",
      "Epoch [51/200], Validation Loss: 0.0078\n",
      "Epoch [52/200], Validation Loss: 0.0086\n",
      "Epoch [53/200], Validation Loss: 0.0082\n",
      "Epoch [54/200], Validation Loss: 0.0072, Save model\n",
      "Epoch [55/200], Validation Loss: 0.0068, Save model\n",
      "Epoch [56/200], Validation Loss: 0.0072\n",
      "Epoch [57/200], Validation Loss: 0.0069\n",
      "Epoch [58/200], Validation Loss: 0.0074\n",
      "Epoch [59/200], Validation Loss: 0.0065, Save model\n",
      "Epoch [60/200], Validation Loss: 0.0062, Save model\n",
      "Epoch [61/200], Validation Loss: 0.0069\n",
      "Epoch [62/200], Validation Loss: 0.0061, Save model\n",
      "Epoch [63/200], Validation Loss: 0.0052, Save model\n",
      "Epoch [64/200], Validation Loss: 0.0070\n",
      "Epoch [65/200], Validation Loss: 0.0080\n",
      "Epoch [66/200], Validation Loss: 0.0067\n",
      "Epoch [67/200], Validation Loss: 0.0078\n",
      "Epoch [68/200], Validation Loss: 0.0061\n",
      "Epoch [69/200], Validation Loss: 0.0062\n",
      "Epoch [70/200], Validation Loss: 0.0078\n",
      "Epoch [71/200], Validation Loss: 0.0093\n",
      "Epoch [72/200], Validation Loss: 0.0073\n",
      "Epoch [73/200], Validation Loss: 0.0070\n",
      "Epoch [74/200], Validation Loss: 0.0060\n",
      "Epoch [75/200], Validation Loss: 0.0118\n",
      "Epoch [76/200], Validation Loss: 0.0102\n",
      "Epoch [77/200], Validation Loss: 0.0083\n",
      "Epoch [78/200], Validation Loss: 0.0073\n",
      "Epoch [79/200], Validation Loss: 0.0061\n",
      "Epoch [80/200], Validation Loss: 0.0070\n",
      "Epoch [81/200], Validation Loss: 0.0054\n",
      "Epoch [82/200], Validation Loss: 0.0051, Save model\n",
      "Epoch [83/200], Validation Loss: 0.0091\n",
      "Epoch [84/200], Validation Loss: 0.0066\n",
      "Epoch [85/200], Validation Loss: 0.0061\n",
      "Epoch [86/200], Validation Loss: 0.0048, Save model\n",
      "Epoch [87/200], Validation Loss: 0.0048\n",
      "Epoch [88/200], Validation Loss: 0.0045, Save model\n",
      "Epoch [89/200], Validation Loss: 0.0040, Save model\n",
      "Epoch [90/200], Validation Loss: 0.0042\n",
      "Epoch [91/200], Validation Loss: 0.0047\n",
      "Epoch [92/200], Validation Loss: 0.0144\n",
      "Epoch [93/200], Validation Loss: 0.0060\n",
      "Epoch [94/200], Validation Loss: 0.0049\n",
      "Epoch [95/200], Validation Loss: 0.0065\n",
      "Epoch [96/200], Validation Loss: 0.0048\n",
      "Epoch [97/200], Validation Loss: 0.0044\n",
      "Epoch [98/200], Validation Loss: 0.0039, Save model\n",
      "Epoch [99/200], Validation Loss: 0.0036, Save model\n",
      "Epoch [100/200], Validation Loss: 0.0036, Save model\n",
      "Epoch [101/200], Validation Loss: 0.0038\n",
      "Epoch [102/200], Validation Loss: 0.0039\n",
      "Epoch [103/200], Validation Loss: 0.0048\n",
      "Epoch [104/200], Validation Loss: 0.0039\n",
      "Epoch [105/200], Validation Loss: 0.0037\n",
      "Epoch [106/200], Validation Loss: 0.0038\n",
      "Epoch [107/200], Validation Loss: 0.0033, Save model\n",
      "Epoch [108/200], Validation Loss: 0.0033, Save model\n",
      "Epoch [109/200], Validation Loss: 0.0034\n",
      "Epoch [110/200], Validation Loss: 0.0032, Save model\n",
      "Epoch [111/200], Validation Loss: 0.0032\n",
      "Epoch [112/200], Validation Loss: 0.0033\n",
      "Epoch [113/200], Validation Loss: 0.0032\n",
      "Epoch [114/200], Validation Loss: 0.0036\n",
      "Epoch [115/200], Validation Loss: 0.0041\n",
      "Epoch [116/200], Validation Loss: 0.0040\n",
      "Epoch [117/200], Validation Loss: 0.0032\n",
      "Epoch [118/200], Validation Loss: 0.0033\n",
      "Epoch [119/200], Validation Loss: 0.0030, Save model\n",
      "Epoch [120/200], Validation Loss: 0.0030\n",
      "Epoch [121/200], Validation Loss: 0.0028, Save model\n",
      "Epoch [122/200], Validation Loss: 0.0030\n",
      "Epoch [123/200], Validation Loss: 0.0030\n",
      "Epoch [124/200], Validation Loss: 0.0030\n",
      "Epoch [125/200], Validation Loss: 0.0033\n",
      "Epoch [126/200], Validation Loss: 0.0031\n",
      "Epoch [127/200], Validation Loss: 0.0030\n",
      "Epoch [128/200], Validation Loss: 0.0041\n",
      "Epoch [129/200], Validation Loss: 0.0038\n",
      "Epoch [130/200], Validation Loss: 0.0033\n",
      "Epoch [131/200], Validation Loss: 0.0035\n",
      "Epoch [132/200], Validation Loss: 0.0039\n",
      "Epoch [133/200], Validation Loss: 0.0040\n",
      "Epoch [134/200], Validation Loss: 0.0035\n",
      "Epoch [135/200], Validation Loss: 0.0030\n",
      "Epoch [136/200], Validation Loss: 0.0028, Save model\n",
      "Epoch [137/200], Validation Loss: 0.0027, Save model\n",
      "Epoch [138/200], Validation Loss: 0.0029\n",
      "Epoch [139/200], Validation Loss: 0.0025, Save model\n",
      "Epoch [140/200], Validation Loss: 0.0026\n",
      "Epoch [141/200], Validation Loss: 0.0027\n",
      "Epoch [142/200], Validation Loss: 0.0027\n",
      "Epoch [143/200], Validation Loss: 0.0043\n",
      "Epoch [144/200], Validation Loss: 0.0086\n",
      "Epoch [145/200], Validation Loss: 0.0087\n",
      "Epoch [146/200], Validation Loss: 0.0081\n",
      "Epoch [147/200], Validation Loss: 0.0054\n",
      "Epoch [148/200], Validation Loss: 0.0043\n",
      "Epoch [149/200], Validation Loss: 0.0038\n",
      "Epoch [150/200], Validation Loss: 0.0041\n",
      "Epoch [151/200], Validation Loss: 0.0043\n",
      "Epoch [152/200], Validation Loss: 0.0125\n",
      "Epoch [153/200], Validation Loss: 0.0066\n",
      "Epoch [154/200], Validation Loss: 0.0074\n",
      "Epoch [155/200], Validation Loss: 0.0083\n",
      "Epoch [156/200], Validation Loss: 0.0058\n",
      "Epoch [157/200], Validation Loss: 0.0046\n",
      "Epoch [158/200], Validation Loss: 0.0042\n",
      "Epoch [159/200], Validation Loss: 0.0040\n",
      "Epoch [160/200], Validation Loss: 0.0046\n",
      "Early stopping...\n",
      "MAE: 0.0308, MSE: 0.0057, RMSE: 0.0752\n"
     ]
    }
   ],
   "source": [
    "########################################## Hyperparameters ##########################################\n",
    "input_dim = len(train_set[0][0][0]) # 63\n",
    "output_dim = 24*56\n",
    "hidden_dim = 256\n",
    "n_layers = 7\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "use_thread = True\n",
    "patience = 20 # number of epochs to wait before stopping\n",
    "########################################################################################################\n",
    "\n",
    "results_folder = \"/home/kimyirum/EMS/ict-2023-ems/load/results/\"\n",
    "now = datetime.now()\n",
    "now_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "filename_metrics = f'{now_str}.pkl'\n",
    "filename_model = f'model_{now_str}.pt'\n",
    "\n",
    "# DataLoader\n",
    "if use_thread:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "else:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_val_loss = float(\"inf\") # initially set to infinity\n",
    "no_improve_epoch = 0\n",
    "\n",
    "########################################## Training loop ##########################################\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data, targets in val_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}', end='')\n",
    "        \n",
    "        # save model if validation loss has decreased\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\", Save model\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), results_folder+filename_model)\n",
    "            no_improve_epoch = 0\n",
    "        else:\n",
    "            print(\"\")\n",
    "            no_improve_epoch += 1\n",
    "            \n",
    "        # early stopping\n",
    "        if no_improve_epoch > patience:\n",
    "            print('Early stopping...')\n",
    "            break\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        all_targets.append(targets.numpy())\n",
    "        all_outputs.append(outputs.numpy())\n",
    "\n",
    "# Flatten targets and outputs to calculate metrics\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# Calculate MAE, MSE and RMSE\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# THIS IS NORMALIZATION VALUE\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 200, 'hidden_dim': 256, 'n_layers': 7}\n",
      "MinMaxScaler()\n",
      "{'MAE': 0.030826006, 'MSE': 0.0056500575, 'RMSE': 0.07516686409174889}\n"
     ]
    }
   ],
   "source": [
    "# Save hyperparams\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'num_epochs': num_epochs,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'n_layers': n_layers\n",
    "}\n",
    "\n",
    "scalers = dataset.scaler\n",
    "\n",
    "metrics = {\n",
    "    'MAE': mae,\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "}\n",
    "\n",
    "# Combine all results in one dictionary\n",
    "results = {\n",
    "    'Hyperparameters': hyperparams,\n",
    "    'Scalers': scalers,\n",
    "    'Metrics': metrics\n",
    "}\n",
    "\n",
    "# Save results to a pickle file\n",
    "with open(results_folder + filename_metrics, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "\n",
    "# load test\n",
    "# Load results from a pickle file\n",
    "with open(results_folder + filename_metrics, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "\n",
    "# Now you can access your data from the loaded_results dictionary\n",
    "print(loaded_results['Hyperparameters'])\n",
    "print(loaded_results['Scalers'])\n",
    "print(loaded_results['Metrics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1344]) torch.Size([1, 1344])\n",
      "Error for next 24 hours:\n",
      "        0_SV-2      1_SV-5    2_SV-6      3_SV-7  4_HV-NM1    5_HV-NM2  \\\n",
      "0    91.348845  108.863337 -0.002062   96.735128 -0.004243   74.124384   \n",
      "1   122.130549   97.539897 -0.007082   97.797671  0.002816   85.080483   \n",
      "2   113.191709  113.795971 -0.001610  119.980631  0.002716   77.136793   \n",
      "3   103.615503  111.160176 -0.004209   85.685015 -0.006105   70.950913   \n",
      "4   114.848384  113.821156  0.001948  112.603048 -0.004016   72.325043   \n",
      "5   118.419181  126.947380  0.007613  114.388506 -0.002228   94.608645   \n",
      "6   109.521006  120.083060  0.003595  111.769644 -0.003589   86.763988   \n",
      "7   112.827925  106.986725  0.001788  108.196643  0.001706  109.870543   \n",
      "8   117.021918  129.018566  0.004100  109.339179  0.003622   86.395682   \n",
      "9   153.445605  142.908081 -0.009855  109.438649 -0.003228   85.875728   \n",
      "10  138.931644  139.705652 -0.000105  123.446359  0.010569  104.429736   \n",
      "11  141.491229  130.380165 -0.000672  141.369854 -0.005741   86.511638   \n",
      "12  143.233700  158.432836 -0.000212  127.296744 -0.005816  112.117679   \n",
      "13  146.394451  147.956340  0.008696  138.466758  0.002131  112.688699   \n",
      "14  130.037220  160.389290 -0.003963  167.615601 -0.005956  104.645976   \n",
      "15  133.118070  119.949931  0.002780  148.214289  0.006043  117.138818   \n",
      "16  155.094956  159.846949  0.000373  144.909619 -0.001554  130.620978   \n",
      "17  162.504356  141.146438  0.002891  155.124065  0.000891  140.606886   \n",
      "18  150.899178  167.472714  0.003385  141.248763  0.009497  124.593181   \n",
      "19  152.428737  146.710961 -0.010260  151.982895  0.006742  123.485940   \n",
      "20  157.014258  164.033975  0.007585  166.394442 -0.010483  118.712623   \n",
      "21  155.611100  163.723479  0.008395  165.075421  0.003255  143.163158   \n",
      "22  166.867202  158.639675 -0.003282  154.471255 -0.009561  153.904983   \n",
      "23  159.520913  150.483059  0.001633  144.265506  0.003421  153.751845   \n",
      "\n",
      "     6_고압콘덴샤  7_신재생에너지동     8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동  47_기숙사9동  \\\n",
      "0  -0.006807  20.197948  -8.644744   2.811995  ...    -1.753397  1.550290   \n",
      "1  -0.009793  29.974622  -5.004150   2.255550  ...     5.667030  0.996929   \n",
      "2  -0.002232  24.241961   0.411413   3.169109  ...     4.207038  1.485324   \n",
      "3  -0.000266   7.236529   1.811425   3.283824  ...    -4.056775  1.372139   \n",
      "4  -0.005363  35.481660   7.380618   3.017778  ...     3.006513  1.689972   \n",
      "5  -0.010100  29.694015  -3.919786   0.001335  ...    -5.976763  1.681565   \n",
      "6  -0.000752  30.893578  -1.130921   3.217889  ...    31.209037  1.655761   \n",
      "7  -0.009510  22.426883  -5.501377   5.701610  ...     5.324967  1.626823   \n",
      "8  -0.003295  38.392168  -9.636069   3.368739  ...     6.277944  0.996956   \n",
      "9  -0.009714  21.618193   6.579335   3.006445  ...    -3.366006  1.441158   \n",
      "10 -0.005075  40.236063   4.338857   2.147414  ...     6.269750  1.973585   \n",
      "11 -0.009441  35.831392   3.779236   2.369987  ...     2.568983  0.968661   \n",
      "12 -0.012163  42.050309   1.645698   3.113673  ...    24.567667  1.140526   \n",
      "13 -0.013391  42.559010  -0.345042   1.731458  ...    21.395725  1.347040   \n",
      "14 -0.002348  39.757548   5.956174   2.715587  ...    24.723382  0.927981   \n",
      "15 -0.006987  40.168338   7.549713   4.376015  ...    30.372296  1.817408   \n",
      "16 -0.004510  40.519618   8.409204   3.712758  ...    12.039965  1.522035   \n",
      "17 -0.011111  32.977922  20.420132   2.941504  ...    13.074435  1.167174   \n",
      "18 -0.006318  31.317199   6.043864   2.720393  ...    11.325210  1.113490   \n",
      "19 -0.007932  30.089860   9.248210   1.779160  ...     5.781269  0.722900   \n",
      "20 -0.010982  22.550784   6.090704   2.943512  ...    17.356977  0.835711   \n",
      "21 -0.009763  26.270220  12.519359   2.793100  ...    12.960643  1.385590   \n",
      "22 -0.009090  17.011178   2.794020   2.060565  ...     7.916238  0.851935   \n",
      "23 -0.004809  34.243366   8.326117   3.295029  ...    19.168953  0.861355   \n",
      "\n",
      "    48_고등광/극초단  49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  \\\n",
      "0    28.453744      1.034429         -6.366011   -29.080575     0.359710   \n",
      "1    21.959598      1.055967         -4.014277   -26.627670     0.247469   \n",
      "2    33.171723      0.855872        -12.502701   -27.532177     0.125683   \n",
      "3    31.618858      0.932968        -14.995856   -42.307091     0.230980   \n",
      "4    37.830734      0.940739         -2.408599   -42.652614     0.144620   \n",
      "5    36.567524      1.156034         -5.033034   -29.184133     0.163776   \n",
      "6    24.475238      1.013714          1.971830   -43.783184     0.147712   \n",
      "7    18.661719      0.982757         -5.792158   -38.491808     0.237550   \n",
      "8    23.720278      0.809878         -8.696811   -32.718765     0.211907   \n",
      "9     9.844917      0.761920        -19.473133   -40.364982     0.294940   \n",
      "10   12.679874      0.800277         -3.191375   -26.448821     0.300907   \n",
      "11   21.925886      0.848794        -11.140298   -36.338519     0.331136   \n",
      "12   22.696868      0.910340         -9.281802   -27.391283     0.318168   \n",
      "13   22.591263      0.502223        -11.223431   -37.842527     0.238789   \n",
      "14   27.944957      1.444834         -8.428364   -26.315004     0.278628   \n",
      "15   17.315537      0.985954        -13.997645   -36.790810     0.229578   \n",
      "16   18.462778      0.572964         -7.769262   -31.580032     0.342062   \n",
      "17   17.762140      0.829940         -8.443636    -3.873155     0.197728   \n",
      "18   21.779778      0.727805         -8.929924     3.170890     0.258076   \n",
      "19   21.366411      0.690548          4.676634    -6.142283     0.163565   \n",
      "20   20.143227      0.108491        -12.171623    -0.909789     0.191995   \n",
      "21   21.511328      1.043553          3.407957    15.516801     0.242843   \n",
      "22    7.064937      0.485542         -7.456548    20.651611     0.238055   \n",
      "23   19.793176      0.591492         11.343373    -6.409667     0.310647   \n",
      "\n",
      "    53_LG도서관(E)  54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0      1.242213   -64.613429      1.767645  \n",
      "1      1.202112   -87.125490      1.806059  \n",
      "2      1.236683   -90.491393      2.716458  \n",
      "3      0.997006   -73.209025      2.211472  \n",
      "4      1.174202   -61.246437      1.940644  \n",
      "5      1.063497   -60.885194      1.980277  \n",
      "6      1.689255   -59.956870      2.250656  \n",
      "7      1.043946   -39.781582      1.166785  \n",
      "8      1.496053   -40.547275      2.312317  \n",
      "9      0.912746   -18.394885      1.331433  \n",
      "10     1.693196   -22.484828      1.660171  \n",
      "11     0.593511   -22.755585      1.582561  \n",
      "12     1.486876   -15.550658      2.263733  \n",
      "13     1.423901    -7.907354      1.735708  \n",
      "14     1.804535    -0.708230      0.770113  \n",
      "15     1.561968     6.855533      1.758845  \n",
      "16     1.032852     1.804794      1.317219  \n",
      "17     1.777264    17.876703      2.337765  \n",
      "18     1.319455    30.745905      0.991382  \n",
      "19     1.128709    17.207174      1.789720  \n",
      "20     0.969014    14.061560      1.376073  \n",
      "21     0.792590     2.233005      1.936629  \n",
      "22     1.366446    11.541464      2.180965  \n",
      "23     0.757379    14.084996      1.002253  \n",
      "\n",
      "[24 rows x 56 columns]\n",
      "MAE: 23.7812, MSE: 2975.5909, RMSE: 54.5490\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is the original dataset and it includes a 'date' column\n",
    "\n",
    "# Get the building names\n",
    "building_names = df.columns[-56:]  # adjust this as necessary\n",
    "\n",
    "# DataLoader for test set\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get the first sequence and its target from the test set\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# Switch model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "print(prediction.shape, real_target.shape)\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "padding = np.zeros((prediction.shape[0], 7))\n",
    "prediction_pad = np.hstack((padding, prediction))\n",
    "real_target_pad = np.hstack((padding, real_target))\n",
    "# print(prediction_pad.shape, real_target_pad.shape)\n",
    "\n",
    "# Apply inverse transformation\n",
    "prediction_inv = dataset.scaler.inverse_transform(prediction_pad)\n",
    "real_target_inv = dataset.scaler.inverse_transform(real_target_pad)\n",
    "\n",
    "# Delete the first 7 columns\n",
    "prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "real_target_inv = np.delete(real_target_inv, np.s_[:7], axis=1)\n",
    "\n",
    "# Reshape them back to the original shape\n",
    "prediction = prediction_inv.reshape(prediction.shape)\n",
    "real_target = real_target_inv.reshape(real_target.shape)\n",
    "\n",
    "\n",
    "# Calculate error (difference between real target and prediction)\n",
    "error = real_target - prediction\n",
    "\n",
    "# Create DataFrame for prediction\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n",
    "\n",
    "mae = mean_absolute_error(real_target, prediction)\n",
    "mse = mean_squared_error(real_target, prediction)\n",
    "rmse = sqrt(mse)\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f} (no normalization)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
