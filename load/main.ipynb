{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # If there are any missing values, fill them with the previous value in time-series\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Normalize numerical columns to range [0, 1]\n",
    "        scaler = MinMaxScaler()\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # One-hot encode categorical variables\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # Drop original categorical columns and merge with encoded ones\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len]\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] # Assuming last 56 columns are power values\n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # flatten y values\n",
    "    \n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize hidden state\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize cell state\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the excel file\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data.xlsx')\n",
    "\n",
    "# Initialize our dataset class\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# Define the split sizes for train, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split dataset\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Validation Loss: 0.0703\n",
      "Epoch [2/150], Validation Loss: 0.0501\n",
      "Epoch [3/150], Validation Loss: 0.0411\n",
      "Epoch [4/150], Validation Loss: 0.0328\n",
      "Epoch [5/150], Validation Loss: 0.0261\n",
      "Epoch [6/150], Validation Loss: 0.0212\n",
      "Epoch [7/150], Validation Loss: 0.0196\n",
      "Epoch [8/150], Validation Loss: 0.0182\n",
      "Epoch [9/150], Validation Loss: 0.0179\n",
      "Epoch [10/150], Validation Loss: 0.0177\n",
      "Epoch [11/150], Validation Loss: 0.0157\n",
      "Epoch [12/150], Validation Loss: 0.0153\n",
      "Epoch [13/150], Validation Loss: 0.0138\n",
      "Epoch [14/150], Validation Loss: 0.0148\n",
      "Epoch [15/150], Validation Loss: 0.0147\n",
      "Epoch [16/150], Validation Loss: 0.0215\n",
      "Epoch [17/150], Validation Loss: 0.0154\n",
      "Epoch [18/150], Validation Loss: 0.0131\n",
      "Epoch [19/150], Validation Loss: 0.0116\n",
      "Epoch [20/150], Validation Loss: 0.0105\n",
      "Epoch [21/150], Validation Loss: 0.0100\n",
      "Epoch [22/150], Validation Loss: 0.0093\n",
      "Epoch [23/150], Validation Loss: 0.0079\n",
      "Epoch [24/150], Validation Loss: 0.0080\n",
      "Epoch [25/150], Validation Loss: 0.0085\n",
      "Epoch [26/150], Validation Loss: 0.0076\n",
      "Epoch [27/150], Validation Loss: 0.0089\n",
      "Epoch [28/150], Validation Loss: 0.0256\n",
      "Epoch [29/150], Validation Loss: 0.0163\n",
      "Epoch [30/150], Validation Loss: 0.0134\n",
      "Epoch [31/150], Validation Loss: 0.0128\n",
      "Epoch [32/150], Validation Loss: 0.0101\n",
      "Epoch [33/150], Validation Loss: 0.0099\n",
      "Epoch [34/150], Validation Loss: 0.0101\n",
      "Epoch [35/150], Validation Loss: 0.0089\n",
      "Epoch [36/150], Validation Loss: 0.0072\n",
      "Epoch [37/150], Validation Loss: 0.0068\n",
      "Epoch [38/150], Validation Loss: 0.0066\n",
      "Epoch [39/150], Validation Loss: 0.0091\n",
      "Epoch [40/150], Validation Loss: 0.0063\n",
      "Epoch [41/150], Validation Loss: 0.0061\n",
      "Epoch [42/150], Validation Loss: 0.0055\n",
      "Epoch [43/150], Validation Loss: 0.0091\n",
      "Epoch [44/150], Validation Loss: 0.0100\n",
      "Epoch [45/150], Validation Loss: 0.0066\n",
      "Epoch [46/150], Validation Loss: 0.0062\n",
      "Epoch [47/150], Validation Loss: 0.0127\n",
      "Epoch [48/150], Validation Loss: 0.0144\n",
      "Epoch [49/150], Validation Loss: 0.0085\n",
      "Epoch [50/150], Validation Loss: 0.0068\n",
      "Epoch [51/150], Validation Loss: 0.0057\n",
      "Epoch [52/150], Validation Loss: 0.0063\n",
      "Epoch [53/150], Validation Loss: 0.0055\n",
      "Epoch [54/150], Validation Loss: 0.0064\n",
      "Epoch [55/150], Validation Loss: 0.0065\n",
      "Epoch [56/150], Validation Loss: 0.0085\n",
      "Epoch [57/150], Validation Loss: 0.0061\n",
      "Epoch [58/150], Validation Loss: 0.0056\n",
      "Epoch [59/150], Validation Loss: 0.0054\n",
      "Epoch [60/150], Validation Loss: 0.0053\n",
      "Epoch [61/150], Validation Loss: 0.0047\n",
      "Epoch [62/150], Validation Loss: 0.0056\n",
      "Epoch [63/150], Validation Loss: 0.0045\n",
      "Epoch [64/150], Validation Loss: 0.0051\n",
      "Epoch [65/150], Validation Loss: 0.0044\n",
      "Epoch [66/150], Validation Loss: 0.0052\n",
      "Epoch [67/150], Validation Loss: 0.0048\n",
      "Epoch [68/150], Validation Loss: 0.0048\n",
      "Epoch [69/150], Validation Loss: 0.0063\n",
      "Epoch [70/150], Validation Loss: 0.0073\n",
      "Epoch [71/150], Validation Loss: 0.0051\n",
      "Epoch [72/150], Validation Loss: 0.0043\n",
      "Epoch [73/150], Validation Loss: 0.0042\n",
      "Epoch [74/150], Validation Loss: 0.0046\n",
      "Epoch [75/150], Validation Loss: 0.0042\n",
      "Epoch [76/150], Validation Loss: 0.0051\n",
      "Epoch [77/150], Validation Loss: 0.0043\n",
      "Epoch [78/150], Validation Loss: 0.0035\n",
      "Epoch [79/150], Validation Loss: 0.0041\n",
      "Epoch [80/150], Validation Loss: 0.0034\n",
      "Epoch [81/150], Validation Loss: 0.0031\n",
      "Epoch [82/150], Validation Loss: 0.0030\n",
      "Epoch [83/150], Validation Loss: 0.0030\n",
      "Epoch [84/150], Validation Loss: 0.0030\n",
      "Epoch [85/150], Validation Loss: 0.0032\n",
      "Epoch [86/150], Validation Loss: 0.0031\n",
      "Epoch [87/150], Validation Loss: 0.0034\n",
      "Epoch [88/150], Validation Loss: 0.0034\n",
      "Epoch [89/150], Validation Loss: 0.0031\n",
      "Epoch [90/150], Validation Loss: 0.0030\n",
      "Epoch [91/150], Validation Loss: 0.0027\n",
      "Epoch [92/150], Validation Loss: 0.0027\n",
      "Epoch [93/150], Validation Loss: 0.0027\n",
      "Epoch [94/150], Validation Loss: 0.0027\n",
      "Epoch [95/150], Validation Loss: 0.0027\n",
      "Epoch [96/150], Validation Loss: 0.0026\n",
      "Epoch [97/150], Validation Loss: 0.0029\n",
      "Epoch [98/150], Validation Loss: 0.0034\n",
      "Epoch [99/150], Validation Loss: 0.0027\n",
      "Epoch [100/150], Validation Loss: 0.0035\n",
      "Epoch [101/150], Validation Loss: 0.0054\n",
      "Epoch [102/150], Validation Loss: 0.0111\n",
      "Epoch [103/150], Validation Loss: 0.0069\n",
      "Epoch [104/150], Validation Loss: 0.0042\n",
      "Epoch [105/150], Validation Loss: 0.0031\n",
      "Epoch [106/150], Validation Loss: 0.0028\n",
      "Epoch [107/150], Validation Loss: 0.0025\n",
      "Epoch [108/150], Validation Loss: 0.0025\n",
      "Epoch [109/150], Validation Loss: 0.0024\n",
      "Epoch [110/150], Validation Loss: 0.0039\n",
      "Epoch [111/150], Validation Loss: 0.0070\n",
      "Epoch [112/150], Validation Loss: 0.0040\n",
      "Epoch [113/150], Validation Loss: 0.0032\n",
      "Epoch [114/150], Validation Loss: 0.0029\n",
      "Epoch [115/150], Validation Loss: 0.0027\n",
      "Epoch [116/150], Validation Loss: 0.0027\n",
      "Epoch [117/150], Validation Loss: 0.0028\n",
      "Epoch [118/150], Validation Loss: 0.0026\n",
      "Epoch [119/150], Validation Loss: 0.0023\n",
      "Epoch [120/150], Validation Loss: 0.0022\n",
      "Epoch [121/150], Validation Loss: 0.0025\n",
      "Epoch [122/150], Validation Loss: 0.0023\n",
      "Epoch [123/150], Validation Loss: 0.0022\n",
      "Epoch [124/150], Validation Loss: 0.0020\n",
      "Epoch [125/150], Validation Loss: 0.0020\n",
      "Epoch [126/150], Validation Loss: 0.0020\n",
      "Epoch [127/150], Validation Loss: 0.0020\n",
      "Epoch [128/150], Validation Loss: 0.0019\n",
      "Epoch [129/150], Validation Loss: 0.0020\n",
      "Epoch [130/150], Validation Loss: 0.0020\n",
      "Epoch [131/150], Validation Loss: 0.0020\n",
      "Epoch [132/150], Validation Loss: 0.0018\n",
      "Epoch [133/150], Validation Loss: 0.0023\n",
      "Epoch [134/150], Validation Loss: 0.0020\n",
      "Epoch [135/150], Validation Loss: 0.0019\n",
      "Epoch [136/150], Validation Loss: 0.0020\n",
      "Epoch [137/150], Validation Loss: 0.0020\n",
      "Epoch [138/150], Validation Loss: 0.0019\n",
      "Epoch [139/150], Validation Loss: 0.0018\n",
      "Epoch [140/150], Validation Loss: 0.0019\n",
      "Epoch [141/150], Validation Loss: 0.0025\n",
      "Epoch [142/150], Validation Loss: 0.0024\n",
      "Epoch [143/150], Validation Loss: 0.0020\n",
      "Epoch [144/150], Validation Loss: 0.0021\n",
      "Epoch [145/150], Validation Loss: 0.0021\n",
      "Epoch [146/150], Validation Loss: 0.0018\n",
      "Epoch [147/150], Validation Loss: 0.0019\n",
      "Epoch [148/150], Validation Loss: 0.0018\n",
      "Epoch [149/150], Validation Loss: 0.0019\n",
      "Epoch [150/150], Validation Loss: 0.0021\n",
      "MAE: 0.0131, MSE: 0.0018, RMSE: 0.0421\n"
     ]
    }
   ],
   "source": [
    "########################################## Hyperparameters ##########################################\n",
    "input_dim = len(train_set[0][0][0]) # 63\n",
    "output_dim = 24*56\n",
    "hidden_dim = 256\n",
    "n_layers = 7\n",
    "learning_rate = 0.001\n",
    "num_epochs = 150\n",
    "batch_size = 64\n",
    "########################################################################################################\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss and optimizer\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "########################################## Training loop ##########################################\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data, targets in val_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "# DataLoader for test set\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        all_targets.append(targets.numpy())\n",
    "        all_outputs.append(outputs.numpy())\n",
    "\n",
    "# Flatten targets and outputs to calculate metrics\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# Calculate MAE, MSE and RMSE\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = \"/home/kimyirum/EMS/ict-2023-ems/load/results\"\n",
    "hyperparams = {\n",
    "    '\\tlearning_rate': learning_rate,\n",
    "    '\\tbatch_size': batch_size,\n",
    "    '\\tnum_epochs': num_epochs,\n",
    "    '\\thidden_dim': hidden_dim,\n",
    "    '\\tn_layers': n_layers\n",
    "}\n",
    "metrics = {\n",
    "    '\\tMAE': mae,\n",
    "    '\\tMSE': mse,\n",
    "    '\\tRMSE': rmse,\n",
    "}\n",
    "\n",
    "now = datetime.now()\n",
    "now_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "filename_metrics = f'{now_str}.txt'\n",
    "filename_model = f'model_{now_str}.pt'\n",
    "\n",
    "# Save the model parameters\n",
    "torch.save(model.state_dict(), results_folder+filename_model)\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open(results_folder+filename_metrics, 'w') as f:\n",
    "    # Write hyperparameters\n",
    "    f.write('Hyperparameters:\\n')\n",
    "    for key, value in hyperparams.items():\n",
    "        f.write(f'{key}: {value}\\n')\n",
    "    \n",
    "    # Write metrics\n",
    "    f.write('\\nMetrics:\\n')\n",
    "    for key, value in metrics.items():\n",
    "        f.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for next 24 hours:\n",
      "      0_SV-2    1_SV-5    2_SV-6    3_SV-7  4_HV-NM1  5_HV-NM2   6_고압콘덴샤  \\\n",
      "0  -0.010489 -0.023625  0.000900 -0.028915 -0.002541 -0.014882 -0.006714   \n",
      "1  -0.016766 -0.023331  0.000096 -0.030150  0.002016 -0.007350  0.002063   \n",
      "2  -0.020153 -0.022179 -0.002090 -0.026695 -0.004948 -0.024024 -0.011096   \n",
      "3  -0.028973 -0.026115 -0.002143 -0.013792  0.003501 -0.021451 -0.005518   \n",
      "4  -0.025696 -0.022664 -0.001272 -0.032025  0.000949 -0.029233 -0.001101   \n",
      "5  -0.035808 -0.024842 -0.002239 -0.022164 -0.003136 -0.022036 -0.008269   \n",
      "6  -0.012555 -0.017803  0.002190 -0.020562  0.006935 -0.024767 -0.010656   \n",
      "7  -0.002873 -0.015469  0.000654 -0.032108  0.001787 -0.033026 -0.009859   \n",
      "8  -0.020079 -0.014451  0.000765 -0.013958 -0.003318 -0.030697 -0.011499   \n",
      "9  -0.010661 -0.010832  0.003604 -0.016078  0.002204 -0.011631 -0.003296   \n",
      "10 -0.020291 -0.018524  0.002049 -0.023115  0.000176 -0.034037 -0.017167   \n",
      "11 -0.018572 -0.021845 -0.007100 -0.013061  0.001176 -0.025750 -0.011085   \n",
      "12 -0.022397 -0.019869 -0.002620 -0.009334  0.006716 -0.021454 -0.016432   \n",
      "13 -0.015549 -0.020122 -0.000246 -0.026184  0.003945 -0.010388 -0.020466   \n",
      "14 -0.016925 -0.012585 -0.001665 -0.023118 -0.001898 -0.016400 -0.018633   \n",
      "15 -0.024905 -0.029718 -0.004156 -0.028917 -0.001358 -0.019644 -0.015754   \n",
      "16 -0.020088 -0.026856  0.000556 -0.019851  0.005245 -0.014694 -0.019404   \n",
      "17 -0.016941 -0.020301  0.003457 -0.024903 -0.005249 -0.017669 -0.016398   \n",
      "18 -0.005129 -0.015658 -0.002599 -0.015003 -0.002907 -0.028394 -0.022220   \n",
      "19 -0.003343 -0.012007 -0.000702 -0.002237 -0.001183 -0.026474 -0.014127   \n",
      "20 -0.003160 -0.010146  0.002317 -0.022397 -0.005489 -0.024110 -0.008574   \n",
      "21 -0.018636 -0.011176  0.001397 -0.016137  0.001033 -0.028030 -0.015901   \n",
      "22 -0.007765 -0.003286  0.001838 -0.011420  0.001555 -0.037041 -0.009730   \n",
      "23 -0.004125 -0.011655  0.001434 -0.015397  0.001477 -0.029792 -0.005357   \n",
      "\n",
      "    7_신재생에너지동    8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동  47_기숙사9동  48_고등광/극초단  \\\n",
      "0   -0.008654 -0.017581  -0.000105  ...    -0.009104 -0.005866    0.001372   \n",
      "1    0.003386 -0.015015  -0.006960  ...    -0.006626 -0.007704    0.001341   \n",
      "2   -0.005808 -0.013393   0.000635  ...    -0.004838 -0.006665   -0.009349   \n",
      "3   -0.006918 -0.009623  -0.006144  ...    -0.006071 -0.007229   -0.006916   \n",
      "4   -0.001644 -0.013162  -0.002613  ...    -0.009418 -0.013429   -0.005051   \n",
      "5   -0.005737 -0.008898  -0.007006  ...    -0.005066 -0.003305   -0.003690   \n",
      "6   -0.004142 -0.006474  -0.004922  ...    -0.004202 -0.006346   -0.002379   \n",
      "7   -0.000533 -0.014101  -0.014670  ...    -0.007103 -0.004450    0.001657   \n",
      "8    0.006847 -0.015057  -0.002445  ...    -0.006612 -0.005654    0.005771   \n",
      "9   -0.005755 -0.007723  -0.005060  ...    -0.002566 -0.008129    0.001344   \n",
      "10   0.005820 -0.009705  -0.005900  ...    -0.002680 -0.005092   -0.003531   \n",
      "11  -0.016834 -0.005198  -0.009407  ...    -0.007330 -0.008057   -0.008345   \n",
      "12  -0.007035 -0.007132  -0.009082  ...    -0.001211 -0.009914   -0.003318   \n",
      "13  -0.007117 -0.003638  -0.003574  ...     0.001282 -0.007991   -0.012049   \n",
      "14  -0.010122 -0.005586  -0.009461  ...     0.001445 -0.012234   -0.009007   \n",
      "15  -0.000809 -0.011115  -0.004991  ...     0.004526 -0.002380   -0.004271   \n",
      "16   0.003046 -0.010091  -0.008183  ...     0.004708 -0.004061   -0.005887   \n",
      "17   0.003118  0.003109  -0.002120  ...     0.006694 -0.008740   -0.005297   \n",
      "18   0.006226 -0.013489  -0.005503  ...     0.006048 -0.009856   -0.006335   \n",
      "19   0.007090  0.009220  -0.003726  ...     0.004611 -0.011469   -0.010907   \n",
      "20   0.010770 -0.012510  -0.004833  ...     0.006330 -0.005976   -0.005620   \n",
      "21   0.011542 -0.007119   0.000350  ...     0.003267 -0.004875   -0.004076   \n",
      "22   0.005777 -0.003736  -0.009021  ...     0.000573 -0.003012   -0.006018   \n",
      "23   0.002977 -0.002076  -0.004072  ...     0.004738 -0.006553   -0.008984   \n",
      "\n",
      "    49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  53_LG도서관(E)  \\\n",
      "0      -0.004221         -0.006237    -0.011124     0.000683    -0.002922   \n",
      "1      -0.004233         -0.012360    -0.010461    -0.004353    -0.010665   \n",
      "2      -0.002997         -0.010788    -0.007516    -0.007999    -0.006211   \n",
      "3      -0.006100         -0.012751    -0.018278    -0.008875    -0.011657   \n",
      "4      -0.002008         -0.009180    -0.002971    -0.003898    -0.008009   \n",
      "5      -0.001882         -0.007611    -0.005305    -0.006999    -0.009496   \n",
      "6      -0.004892         -0.006110    -0.005612    -0.005066    -0.008589   \n",
      "7      -0.008601         -0.013391     0.000892    -0.002053    -0.010344   \n",
      "8      -0.005000         -0.019599    -0.006010     0.000263    -0.007778   \n",
      "9      -0.003927         -0.008927    -0.003560    -0.006325    -0.008795   \n",
      "10     -0.002899         -0.010908    -0.000408    -0.007636    -0.001047   \n",
      "11     -0.004933         -0.009482    -0.005328    -0.004604     0.000578   \n",
      "12     -0.005706         -0.009327    -0.000330    -0.007672    -0.007098   \n",
      "13     -0.006396         -0.000102    -0.006906     0.005283    -0.005225   \n",
      "14     -0.008774          0.002590    -0.006887     0.004699    -0.002906   \n",
      "15     -0.000773          0.001582    -0.012516    -0.001999    -0.003011   \n",
      "16     -0.001993          0.003135    -0.007162     0.003237    -0.002767   \n",
      "17     -0.004203          0.004039    -0.003587     0.005688    -0.012005   \n",
      "18     -0.008665          0.001054    -0.005402    -0.004953    -0.009625   \n",
      "19     -0.006679          0.003399     0.002942    -0.004299    -0.008071   \n",
      "20     -0.005673          0.008863     0.003324    -0.000083    -0.009232   \n",
      "21     -0.002350          0.003344     0.005173     0.001375    -0.005530   \n",
      "22     -0.009988          0.002403     0.018057    -0.003496    -0.012303   \n",
      "23     -0.007329          0.004124     0.013342    -0.005849    -0.010735   \n",
      "\n",
      "    54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0     -0.007753     -0.003891  \n",
      "1     -0.015425     -0.007503  \n",
      "2     -0.013055     -0.008222  \n",
      "3     -0.025846     -0.007037  \n",
      "4     -0.017882     -0.001499  \n",
      "5     -0.019819     -0.003770  \n",
      "6     -0.014114     -0.001629  \n",
      "7     -0.013737     -0.006420  \n",
      "8     -0.032211     -0.007667  \n",
      "9     -0.031668     -0.003773  \n",
      "10    -0.023086     -0.008594  \n",
      "11    -0.015609     -0.006439  \n",
      "12    -0.022417      0.001815  \n",
      "13    -0.023527     -0.008276  \n",
      "14    -0.024674     -0.003091  \n",
      "15    -0.022420     -0.007871  \n",
      "16    -0.021831     -0.005402  \n",
      "17    -0.020498     -0.007866  \n",
      "18    -0.018504     -0.002839  \n",
      "19    -0.023773     -0.010088  \n",
      "20    -0.013909     -0.003545  \n",
      "21    -0.025283     -0.006420  \n",
      "22    -0.020412     -0.006526  \n",
      "23    -0.015180     -0.004161  \n",
      "\n",
      "[24 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is the original dataset and it includes a 'date' column\n",
    "\n",
    "# Get the building names\n",
    "building_names = df.columns[-56:]  # adjust this as necessary\n",
    "\n",
    "# DataLoader for test set\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get the first sequence and its target from the test set\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# Switch model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "# Remove batch dimension, reshape and convert prediction to numpy array\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "\n",
    "# Reshape real_target and convert it to numpy array\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "# Calculate error (difference between real target and prediction)\n",
    "error = real_target - prediction\n",
    "\n",
    "# Create DataFrame for prediction\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
