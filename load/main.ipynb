{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 실행 시\n",
    "- 3번째 셀의 Hyperparameter 값을 원하는대로 설정하기\n",
    "- 저장위치를 바꾸고 싶다면 results_folder 변수에 경로 설정하기\n",
    "- 자세한 설명은 각 코드 셀의 설명 및 주석 참고!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    모델 및 데이터셋 클래스 정의하는 코드\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Use\", device)\n",
    "\n",
    "# 시계열 데이터를 처리하는 클래스를 정의합니다.\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len  # 입력 시퀀스의 길이를 정의합니다.\n",
    "        self.pred_len = pred_len  # 예측할 시퀀스의 길이를 정의합니다.\n",
    "        self.scaler = MinMaxScaler()  # 데이터 정규화를 위한 MinMaxScaler 객체를 생성합니다.\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)  # 데이터 전처리 함수를 호출하여 dataframe을 전처리합니다.\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # 누락된 값을 시계열의 이전 값으로 채웁니다.\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # 숫자형 열을 [0, 1] 범위로 정규화합니다.\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # 범주형 변수를 원-핫 인코딩합니다.\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # 원래의 범주형 열을 삭제하고 인코딩된 열과 병합합니다.\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1  # 데이터셋의 전체 길이를 반환합니다.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len, :7]  # 입력 시퀀스의 앞 7열만 선택합니다.\n",
    "        # 마지막 56열이 전력 값이라고 가정하고 예측할 시퀀스를 선택합니다.\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] \n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # y 값을 평탄화하여 반환합니다.\n",
    "    \n",
    "# LSTM 모델을 정의합니다.\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # LSTM의 은닉층의 차원을 정의합니다.\n",
    "        self.n_layers = n_layers  # LSTM 층의 개수를 정의합니다.\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)  # LSTM 층을 정의합니다.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 완전 연결 층을 정의합니다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 은닉 상태와 셀 상태를 초기화합니다.\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM 층을 통해 데이터를 전달하고 출력을 얻습니다.\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 완전 연결 층을 통해 출력을 얻습니다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    데이터셋 읽고 set으로 분할하는 코드\n",
    "\"\"\"\n",
    "# pandas 라이브러리를 사용하여 엑셀 파일을 불러옵니다.\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data_KW.xlsx')\n",
    "\n",
    "# TimeSeriesDataset 클래스의 인스턴스를 생성합니다. 위에서 정의한 클래스를 사용하여 데이터를 전처리합니다.\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# 학습, 검증 및 테스트 데이터 세트의 크기를 정의합니다.\n",
    "train_size = int(0.7 * len(dataset))  # 전체 데이터의 70%를 학습 데이터로 사용\n",
    "val_size = int(0.2 * len(dataset))    # 전체 데이터의 20%를 검증 데이터로 사용\n",
    "test_size = len(dataset) - train_size - val_size  # 나머지 데이터를 테스트 데이터로 사용\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "# 전체 데이터셋을 학습, 검증 및 테스트 데이터 세트로 분할합니다.\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/700], Validation Loss: 0.0930, Save model\n",
      "Epoch [2/700], Validation Loss: 0.0476, Save model\n",
      "Epoch [3/700], Validation Loss: 0.0424, Save model\n",
      "Epoch [4/700], Validation Loss: 0.0396, Save model\n",
      "Epoch [5/700], Validation Loss: 0.0394, Save model\n",
      "Epoch [6/700], Validation Loss: 0.0383, Save model\n",
      "Epoch [7/700], Validation Loss: 0.0385\n",
      "Epoch [8/700], Validation Loss: 0.0376, Save model\n",
      "Epoch [9/700], Validation Loss: 0.0365, Save model\n",
      "Epoch [10/700], Validation Loss: 0.0360, Save model\n",
      "Epoch [11/700], Validation Loss: 0.0361\n",
      "Epoch [12/700], Validation Loss: 0.0358, Save model\n",
      "Epoch [13/700], Validation Loss: 0.0356, Save model\n",
      "Epoch [14/700], Validation Loss: 0.0356, Save model\n",
      "Epoch [15/700], Validation Loss: 0.0355, Save model\n",
      "Epoch [16/700], Validation Loss: 0.0354, Save model\n",
      "Epoch [17/700], Validation Loss: 0.0353, Save model\n",
      "Epoch [18/700], Validation Loss: 0.0353, Save model\n",
      "Epoch [19/700], Validation Loss: 0.0352, Save model\n",
      "Epoch [20/700], Validation Loss: 0.0351, Save model\n",
      "Epoch [21/700], Validation Loss: 0.0349, Save model\n",
      "Epoch [22/700], Validation Loss: 0.0346, Save model\n",
      "Epoch [23/700], Validation Loss: 0.0341, Save model\n",
      "Epoch [24/700], Validation Loss: 0.0332, Save model\n",
      "Epoch [25/700], Validation Loss: 0.0331, Save model\n",
      "Epoch [26/700], Validation Loss: 0.0330, Save model\n",
      "Epoch [27/700], Validation Loss: 0.0329, Save model\n",
      "Epoch [28/700], Validation Loss: 0.0324, Save model\n",
      "Epoch [29/700], Validation Loss: 0.0325\n",
      "Epoch [30/700], Validation Loss: 0.0321, Save model\n",
      "Epoch [31/700], Validation Loss: 0.0321\n",
      "Epoch [32/700], Validation Loss: 0.0320, Save model\n",
      "Epoch [33/700], Validation Loss: 0.0320, Save model\n",
      "Epoch [34/700], Validation Loss: 0.0317, Save model\n",
      "Epoch [35/700], Validation Loss: 0.0316, Save model\n",
      "Epoch [36/700], Validation Loss: 0.0314, Save model\n",
      "Epoch [37/700], Validation Loss: 0.0313, Save model\n",
      "Epoch [38/700], Validation Loss: 0.0316\n",
      "Epoch [39/700], Validation Loss: 0.0317\n",
      "Epoch [40/700], Validation Loss: 0.0311, Save model\n",
      "Epoch [41/700], Validation Loss: 0.0311, Save model\n",
      "Epoch [42/700], Validation Loss: 0.0310, Save model\n",
      "Epoch [43/700], Validation Loss: 0.0308, Save model\n",
      "Epoch [44/700], Validation Loss: 0.0307, Save model\n",
      "Epoch [45/700], Validation Loss: 0.0305, Save model\n",
      "Epoch [46/700], Validation Loss: 0.0301, Save model\n",
      "Epoch [47/700], Validation Loss: 0.0296, Save model\n",
      "Epoch [48/700], Validation Loss: 0.0260, Save model\n",
      "Epoch [49/700], Validation Loss: 0.0204, Save model\n",
      "Epoch [50/700], Validation Loss: 0.0171, Save model\n",
      "Epoch [51/700], Validation Loss: 0.0147, Save model\n",
      "Epoch [52/700], Validation Loss: 0.0138, Save model\n",
      "Epoch [53/700], Validation Loss: 0.0136, Save model\n",
      "Epoch [54/700], Validation Loss: 0.0130, Save model\n",
      "Epoch [55/700], Validation Loss: 0.0127, Save model\n",
      "Epoch [56/700], Validation Loss: 0.0126, Save model\n",
      "Epoch [57/700], Validation Loss: 0.0122, Save model\n",
      "Epoch [58/700], Validation Loss: 0.0120, Save model\n",
      "Epoch [59/700], Validation Loss: 0.0118, Save model\n",
      "Epoch [60/700], Validation Loss: 0.0115, Save model\n",
      "Epoch [61/700], Validation Loss: 0.0113, Save model\n",
      "Epoch [62/700], Validation Loss: 0.0112, Save model\n",
      "Epoch [63/700], Validation Loss: 0.0111, Save model\n",
      "Epoch [64/700], Validation Loss: 0.0109, Save model\n",
      "Epoch [65/700], Validation Loss: 0.0108, Save model\n",
      "Epoch [66/700], Validation Loss: 0.0108, Save model\n",
      "Epoch [67/700], Validation Loss: 0.0106, Save model\n",
      "Epoch [68/700], Validation Loss: 0.0101, Save model\n",
      "Epoch [69/700], Validation Loss: 0.0101\n",
      "Epoch [70/700], Validation Loss: 0.0098, Save model\n",
      "Epoch [71/700], Validation Loss: 0.0096, Save model\n",
      "Epoch [72/700], Validation Loss: 0.0094, Save model\n",
      "Epoch [73/700], Validation Loss: 0.0093, Save model\n",
      "Epoch [74/700], Validation Loss: 0.0093, Save model\n",
      "Epoch [75/700], Validation Loss: 0.0092, Save model\n",
      "Epoch [76/700], Validation Loss: 0.0092\n",
      "Epoch [77/700], Validation Loss: 0.0092, Save model\n",
      "Epoch [78/700], Validation Loss: 0.0090, Save model\n",
      "Epoch [79/700], Validation Loss: 0.0089, Save model\n",
      "Epoch [80/700], Validation Loss: 0.0090\n",
      "Epoch [81/700], Validation Loss: 0.0089\n",
      "Epoch [82/700], Validation Loss: 0.0089, Save model\n",
      "Epoch [83/700], Validation Loss: 0.0089, Save model\n",
      "Epoch [84/700], Validation Loss: 0.0086, Save model\n",
      "Epoch [85/700], Validation Loss: 0.0086, Save model\n",
      "Epoch [86/700], Validation Loss: 0.0085, Save model\n",
      "Epoch [87/700], Validation Loss: 0.0084, Save model\n",
      "Epoch [88/700], Validation Loss: 0.0084, Save model\n",
      "Epoch [89/700], Validation Loss: 0.0084, Save model\n",
      "Epoch [90/700], Validation Loss: 0.0083, Save model\n",
      "Epoch [91/700], Validation Loss: 0.0083, Save model\n",
      "Epoch [92/700], Validation Loss: 0.0083\n",
      "Epoch [93/700], Validation Loss: 0.0083, Save model\n",
      "Epoch [94/700], Validation Loss: 0.0083\n",
      "Epoch [95/700], Validation Loss: 0.0082, Save model\n",
      "Epoch [96/700], Validation Loss: 0.0086\n",
      "Epoch [97/700], Validation Loss: 0.0087\n",
      "Epoch [98/700], Validation Loss: 0.0086\n",
      "Epoch [99/700], Validation Loss: 0.0085\n",
      "Epoch [100/700], Validation Loss: 0.0082, Save model\n",
      "Epoch [101/700], Validation Loss: 0.0081, Save model\n",
      "Epoch [102/700], Validation Loss: 0.0080, Save model\n",
      "Epoch [103/700], Validation Loss: 0.0080, Save model\n",
      "Epoch [104/700], Validation Loss: 0.0079, Save model\n",
      "Epoch [105/700], Validation Loss: 0.0079\n",
      "Epoch [106/700], Validation Loss: 0.0079\n",
      "Epoch [107/700], Validation Loss: 0.0079, Save model\n",
      "Epoch [108/700], Validation Loss: 0.0078, Save model\n",
      "Epoch [109/700], Validation Loss: 0.0078\n",
      "Epoch [110/700], Validation Loss: 0.0078\n",
      "Epoch [111/700], Validation Loss: 0.0079\n",
      "Epoch [112/700], Validation Loss: 0.0078, Save model\n",
      "Epoch [113/700], Validation Loss: 0.0078, Save model\n",
      "Epoch [114/700], Validation Loss: 0.0078, Save model\n",
      "Epoch [115/700], Validation Loss: 0.0078, Save model\n",
      "Epoch [116/700], Validation Loss: 0.0078, Save model\n",
      "Epoch [117/700], Validation Loss: 0.0077, Save model\n",
      "Epoch [118/700], Validation Loss: 0.0077, Save model\n",
      "Epoch [119/700], Validation Loss: 0.0077, Save model\n",
      "Epoch [120/700], Validation Loss: 0.0077, Save model\n",
      "Epoch [121/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [122/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [123/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [124/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [125/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [126/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [127/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [128/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [129/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [130/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [131/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [132/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [133/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [134/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [135/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [136/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [137/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [138/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [139/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [140/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [141/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [142/700], Validation Loss: 0.0074\n",
      "Epoch [143/700], Validation Loss: 0.0074\n",
      "Epoch [144/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [145/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [146/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [147/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [148/700], Validation Loss: 0.0073\n",
      "Epoch [149/700], Validation Loss: 0.0074\n",
      "Epoch [150/700], Validation Loss: 0.0074\n",
      "Epoch [151/700], Validation Loss: 0.0074\n",
      "Epoch [152/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [153/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [154/700], Validation Loss: 0.0072, Save model\n",
      "Epoch [155/700], Validation Loss: 0.0074\n",
      "Epoch [156/700], Validation Loss: 0.0072\n",
      "Epoch [157/700], Validation Loss: 0.0076\n",
      "Epoch [158/700], Validation Loss: 0.0078\n",
      "Epoch [159/700], Validation Loss: 0.0077\n",
      "Epoch [160/700], Validation Loss: 0.0077\n",
      "Epoch [161/700], Validation Loss: 0.0076\n",
      "Epoch [162/700], Validation Loss: 0.0073\n",
      "Epoch [163/700], Validation Loss: 0.0072, Save model\n",
      "Epoch [164/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [165/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [166/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [167/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [168/700], Validation Loss: 0.0071\n",
      "Epoch [169/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [170/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [171/700], Validation Loss: 0.0069, Save model\n",
      "Epoch [172/700], Validation Loss: 0.0069, Save model\n",
      "Epoch [173/700], Validation Loss: 0.0069, Save model\n",
      "Epoch [174/700], Validation Loss: 0.0069, Save model\n",
      "Epoch [175/700], Validation Loss: 0.0069, Save model\n",
      "Epoch [176/700], Validation Loss: 0.0068, Save model\n",
      "Epoch [177/700], Validation Loss: 0.0068, Save model\n",
      "Epoch [178/700], Validation Loss: 0.0068, Save model\n",
      "Epoch [179/700], Validation Loss: 0.0068, Save model\n",
      "Epoch [180/700], Validation Loss: 0.0067, Save model\n",
      "Epoch [181/700], Validation Loss: 0.0067, Save model\n",
      "Epoch [182/700], Validation Loss: 0.0067, Save model\n",
      "Epoch [183/700], Validation Loss: 0.0067, Save model\n",
      "Epoch [184/700], Validation Loss: 0.0067, Save model\n",
      "Epoch [185/700], Validation Loss: 0.0067, Save model\n",
      "Epoch [186/700], Validation Loss: 0.0067, Save model\n",
      "Epoch [187/700], Validation Loss: 0.0067\n",
      "Epoch [188/700], Validation Loss: 0.0067\n",
      "Epoch [189/700], Validation Loss: 0.0067, Save model\n",
      "Epoch [190/700], Validation Loss: 0.0066, Save model\n",
      "Epoch [191/700], Validation Loss: 0.0066, Save model\n",
      "Epoch [192/700], Validation Loss: 0.0066, Save model\n",
      "Epoch [193/700], Validation Loss: 0.0066, Save model\n",
      "Epoch [194/700], Validation Loss: 0.0066\n",
      "Epoch [195/700], Validation Loss: 0.0066\n",
      "Epoch [196/700], Validation Loss: 0.0066\n",
      "Epoch [197/700], Validation Loss: 0.0067\n",
      "Epoch [198/700], Validation Loss: 0.0067\n",
      "Epoch [199/700], Validation Loss: 0.0067\n",
      "Epoch [200/700], Validation Loss: 0.0066\n",
      "Epoch [201/700], Validation Loss: 0.0068\n",
      "Epoch [202/700], Validation Loss: 0.0067\n",
      "Epoch [203/700], Validation Loss: 0.0070\n",
      "Epoch [204/700], Validation Loss: 0.0066\n",
      "Epoch [205/700], Validation Loss: 0.0067\n",
      "Epoch [206/700], Validation Loss: 0.0066\n",
      "Epoch [207/700], Validation Loss: 0.0066\n",
      "Epoch [208/700], Validation Loss: 0.0066, Save model\n",
      "Epoch [209/700], Validation Loss: 0.0065, Save model\n",
      "Epoch [210/700], Validation Loss: 0.0065\n",
      "Epoch [211/700], Validation Loss: 0.0065\n",
      "Epoch [212/700], Validation Loss: 0.0065, Save model\n",
      "Epoch [213/700], Validation Loss: 0.0065, Save model\n",
      "Epoch [214/700], Validation Loss: 0.0064, Save model\n",
      "Epoch [215/700], Validation Loss: 0.0064\n",
      "Epoch [216/700], Validation Loss: 0.0064, Save model\n",
      "Epoch [217/700], Validation Loss: 0.0064, Save model\n",
      "Epoch [218/700], Validation Loss: 0.0064\n",
      "Epoch [219/700], Validation Loss: 0.0064, Save model\n",
      "Epoch [220/700], Validation Loss: 0.0064, Save model\n",
      "Epoch [221/700], Validation Loss: 0.0064, Save model\n",
      "Epoch [222/700], Validation Loss: 0.0063, Save model\n",
      "Epoch [223/700], Validation Loss: 0.0063, Save model\n",
      "Epoch [224/700], Validation Loss: 0.0063, Save model\n",
      "Epoch [225/700], Validation Loss: 0.0064\n",
      "Epoch [226/700], Validation Loss: 0.0063\n",
      "Epoch [227/700], Validation Loss: 0.0063\n",
      "Epoch [228/700], Validation Loss: 0.0064\n",
      "Epoch [229/700], Validation Loss: 0.0063, Save model\n",
      "Epoch [230/700], Validation Loss: 0.0063, Save model\n",
      "Epoch [231/700], Validation Loss: 0.0063\n",
      "Epoch [232/700], Validation Loss: 0.0062, Save model\n",
      "Epoch [233/700], Validation Loss: 0.0062, Save model\n",
      "Epoch [234/700], Validation Loss: 0.0062\n",
      "Epoch [235/700], Validation Loss: 0.0062, Save model\n",
      "Epoch [236/700], Validation Loss: 0.0062\n",
      "Epoch [237/700], Validation Loss: 0.0062, Save model\n",
      "Epoch [238/700], Validation Loss: 0.0063\n",
      "Epoch [239/700], Validation Loss: 0.0063\n",
      "Epoch [240/700], Validation Loss: 0.0062, Save model\n",
      "Epoch [241/700], Validation Loss: 0.0064\n",
      "Epoch [242/700], Validation Loss: 0.0063\n",
      "Epoch [243/700], Validation Loss: 0.0062\n",
      "Epoch [244/700], Validation Loss: 0.0064\n",
      "Epoch [245/700], Validation Loss: 0.0064\n",
      "Epoch [246/700], Validation Loss: 0.0062\n",
      "Epoch [247/700], Validation Loss: 0.0062\n",
      "Epoch [248/700], Validation Loss: 0.0062\n",
      "Epoch [249/700], Validation Loss: 0.0062, Save model\n",
      "Epoch [250/700], Validation Loss: 0.0061, Save model\n",
      "Epoch [251/700], Validation Loss: 0.0061\n",
      "Epoch [252/700], Validation Loss: 0.0061\n",
      "Epoch [253/700], Validation Loss: 0.0061\n",
      "Epoch [254/700], Validation Loss: 0.0061, Save model\n",
      "Epoch [255/700], Validation Loss: 0.0061, Save model\n",
      "Epoch [256/700], Validation Loss: 0.0061, Save model\n",
      "Epoch [257/700], Validation Loss: 0.0061, Save model\n",
      "Epoch [258/700], Validation Loss: 0.0061\n",
      "Epoch [259/700], Validation Loss: 0.0061\n",
      "Epoch [260/700], Validation Loss: 0.0061\n",
      "Epoch [261/700], Validation Loss: 0.0061\n",
      "Epoch [262/700], Validation Loss: 0.0061\n",
      "Epoch [263/700], Validation Loss: 0.0061\n",
      "Epoch [264/700], Validation Loss: 0.0061\n",
      "Epoch [265/700], Validation Loss: 0.0061\n",
      "Epoch [266/700], Validation Loss: 0.0061\n",
      "Epoch [267/700], Validation Loss: 0.0061, Save model\n",
      "Epoch [268/700], Validation Loss: 0.0062\n",
      "Epoch [269/700], Validation Loss: 0.0062\n",
      "Epoch [270/700], Validation Loss: 0.0061\n",
      "Epoch [271/700], Validation Loss: 0.0061\n",
      "Epoch [272/700], Validation Loss: 0.0063\n",
      "Epoch [273/700], Validation Loss: 0.0062\n",
      "Epoch [274/700], Validation Loss: 0.0061\n",
      "Epoch [275/700], Validation Loss: 0.0061\n",
      "Epoch [276/700], Validation Loss: 0.0061\n",
      "Epoch [277/700], Validation Loss: 0.0061\n",
      "Epoch [278/700], Validation Loss: 0.0060, Save model\n",
      "Epoch [279/700], Validation Loss: 0.0060, Save model\n",
      "Epoch [280/700], Validation Loss: 0.0060\n",
      "Epoch [281/700], Validation Loss: 0.0060\n",
      "Epoch [282/700], Validation Loss: 0.0060\n",
      "Epoch [283/700], Validation Loss: 0.0060, Save model\n",
      "Epoch [284/700], Validation Loss: 0.0060\n",
      "Epoch [285/700], Validation Loss: 0.0060\n",
      "Epoch [286/700], Validation Loss: 0.0060, Save model\n",
      "Epoch [287/700], Validation Loss: 0.0059, Save model\n",
      "Epoch [288/700], Validation Loss: 0.0059\n",
      "Epoch [289/700], Validation Loss: 0.0060\n",
      "Epoch [290/700], Validation Loss: 0.0060\n",
      "Epoch [291/700], Validation Loss: 0.0059, Save model\n",
      "Epoch [292/700], Validation Loss: 0.0060\n",
      "Epoch [293/700], Validation Loss: 0.0060\n",
      "Epoch [294/700], Validation Loss: 0.0060\n",
      "Epoch [295/700], Validation Loss: 0.0059\n",
      "Epoch [296/700], Validation Loss: 0.0060\n",
      "Epoch [297/700], Validation Loss: 0.0060\n",
      "Epoch [298/700], Validation Loss: 0.0060\n",
      "Epoch [299/700], Validation Loss: 0.0060\n",
      "Epoch [300/700], Validation Loss: 0.0060\n",
      "Epoch [301/700], Validation Loss: 0.0060\n",
      "Epoch [302/700], Validation Loss: 0.0059\n",
      "Epoch [303/700], Validation Loss: 0.0060\n",
      "Epoch [304/700], Validation Loss: 0.0060\n",
      "Epoch [305/700], Validation Loss: 0.0059\n",
      "Epoch [306/700], Validation Loss: 0.0059, Save model\n",
      "Epoch [307/700], Validation Loss: 0.0060\n",
      "Epoch [308/700], Validation Loss: 0.0060\n",
      "Epoch [309/700], Validation Loss: 0.0060\n",
      "Epoch [310/700], Validation Loss: 0.0060\n",
      "Epoch [311/700], Validation Loss: 0.0061\n",
      "Epoch [312/700], Validation Loss: 0.0061\n",
      "Epoch [313/700], Validation Loss: 0.0060\n",
      "Epoch [314/700], Validation Loss: 0.0059\n",
      "Epoch [315/700], Validation Loss: 0.0059\n",
      "Epoch [316/700], Validation Loss: 0.0059\n",
      "Epoch [317/700], Validation Loss: 0.0059\n",
      "Epoch [318/700], Validation Loss: 0.0059, Save model\n",
      "Epoch [319/700], Validation Loss: 0.0059\n",
      "Epoch [320/700], Validation Loss: 0.0059\n",
      "Epoch [321/700], Validation Loss: 0.0059\n",
      "Epoch [322/700], Validation Loss: 0.0059\n",
      "Epoch [323/700], Validation Loss: 0.0059\n",
      "Epoch [324/700], Validation Loss: 0.0059, Save model\n",
      "Epoch [325/700], Validation Loss: 0.0059\n",
      "Epoch [326/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [327/700], Validation Loss: 0.0059\n",
      "Epoch [328/700], Validation Loss: 0.0059\n",
      "Epoch [329/700], Validation Loss: 0.0059\n",
      "Epoch [330/700], Validation Loss: 0.0058\n",
      "Epoch [331/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [332/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [333/700], Validation Loss: 0.0058\n",
      "Epoch [334/700], Validation Loss: 0.0059\n",
      "Epoch [335/700], Validation Loss: 0.0058\n",
      "Epoch [336/700], Validation Loss: 0.0058\n",
      "Epoch [337/700], Validation Loss: 0.0059\n",
      "Epoch [338/700], Validation Loss: 0.0059\n",
      "Epoch [339/700], Validation Loss: 0.0058\n",
      "Epoch [340/700], Validation Loss: 0.0059\n",
      "Epoch [341/700], Validation Loss: 0.0059\n",
      "Epoch [342/700], Validation Loss: 0.0059\n",
      "Epoch [343/700], Validation Loss: 0.0060\n",
      "Epoch [344/700], Validation Loss: 0.0060\n",
      "Epoch [345/700], Validation Loss: 0.0059\n",
      "Epoch [346/700], Validation Loss: 0.0058\n",
      "Epoch [347/700], Validation Loss: 0.0059\n",
      "Epoch [348/700], Validation Loss: 0.0059\n",
      "Epoch 00348: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [349/700], Validation Loss: 0.0058\n",
      "Epoch [350/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [351/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [352/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [353/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [354/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [355/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [356/700], Validation Loss: 0.0058\n",
      "Epoch [357/700], Validation Loss: 0.0058\n",
      "Epoch [358/700], Validation Loss: 0.0059\n",
      "Epoch [359/700], Validation Loss: 0.0061\n",
      "Epoch [360/700], Validation Loss: 0.0061\n",
      "Epoch [361/700], Validation Loss: 0.0060\n",
      "Epoch [362/700], Validation Loss: 0.0058\n",
      "Epoch [363/700], Validation Loss: 0.0058\n",
      "Epoch [364/700], Validation Loss: 0.0062\n",
      "Epoch [365/700], Validation Loss: 0.0064\n",
      "Epoch [366/700], Validation Loss: 0.0060\n",
      "Epoch [367/700], Validation Loss: 0.0058\n",
      "Epoch [368/700], Validation Loss: 0.0063\n",
      "Epoch [369/700], Validation Loss: 0.0060\n",
      "Epoch [370/700], Validation Loss: 0.0058\n",
      "Epoch [371/700], Validation Loss: 0.0061\n",
      "Epoch 00371: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [372/700], Validation Loss: 0.0059\n",
      "Epoch [373/700], Validation Loss: 0.0057\n",
      "Epoch [374/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [375/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [376/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [377/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [378/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [379/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [380/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [381/700], Validation Loss: 0.0057\n",
      "Epoch [382/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [383/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [384/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [385/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [386/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [387/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [388/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [389/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [390/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [391/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [392/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [393/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [394/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [395/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [396/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [397/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [398/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [399/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [400/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [401/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [402/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [403/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [404/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [405/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [406/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [407/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [408/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [409/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [410/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [411/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [412/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [413/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [414/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [415/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [416/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [417/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [418/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [419/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [420/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [421/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [422/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [423/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [424/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [425/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [426/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [427/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [428/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [429/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [430/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [431/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [432/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [433/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [434/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [435/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [436/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [437/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [438/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [439/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [440/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [441/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [442/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [443/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [444/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [445/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [446/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [447/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [448/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [449/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [450/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [451/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [452/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [453/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [454/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [455/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [456/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [457/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [458/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [459/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [460/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [461/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [462/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [463/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [464/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [465/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [466/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [467/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [468/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [469/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [470/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [471/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [472/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [473/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [474/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [475/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [476/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [477/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [478/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [479/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [480/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [481/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [482/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [483/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [484/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [485/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [486/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [487/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [488/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [489/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [490/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [491/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [492/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [493/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [494/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [495/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [496/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [497/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [498/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [499/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [500/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [501/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [502/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [503/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [504/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [505/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [506/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [507/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [508/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [509/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [510/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [511/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [512/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [513/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [514/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [515/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [516/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [517/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [518/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [519/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [520/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [521/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [522/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [523/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [524/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [525/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [526/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [527/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [528/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [529/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [530/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [531/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [532/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [533/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [534/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [535/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [536/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [537/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [538/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [539/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [540/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [541/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [542/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [543/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [544/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [545/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [546/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [547/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [548/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [549/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [550/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [551/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [552/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [553/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [554/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [555/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [556/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [557/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [558/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [559/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [560/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [561/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [562/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [563/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [564/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [565/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [566/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [567/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [568/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [569/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [570/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [571/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [572/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [573/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [574/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [575/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [576/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [577/700], Validation Loss: 0.0056\n",
      "Epoch [578/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [579/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [580/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [581/700], Validation Loss: 0.0056\n",
      "Epoch [582/700], Validation Loss: 0.0056\n",
      "Epoch [583/700], Validation Loss: 0.0056\n",
      "Epoch [584/700], Validation Loss: 0.0056\n",
      "Epoch [585/700], Validation Loss: 0.0056\n",
      "Epoch [586/700], Validation Loss: 0.0056\n",
      "Epoch [587/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [588/700], Validation Loss: 0.0056\n",
      "Epoch [589/700], Validation Loss: 0.0056\n",
      "Epoch [590/700], Validation Loss: 0.0056\n",
      "Epoch [591/700], Validation Loss: 0.0056\n",
      "Epoch [592/700], Validation Loss: 0.0056\n",
      "Epoch [593/700], Validation Loss: 0.0056\n",
      "Epoch [594/700], Validation Loss: 0.0056\n",
      "Epoch [595/700], Validation Loss: 0.0057\n",
      "Epoch [596/700], Validation Loss: 0.0057\n",
      "Epoch [597/700], Validation Loss: 0.0056\n",
      "Epoch [598/700], Validation Loss: 0.0057\n",
      "Epoch [599/700], Validation Loss: 0.0058\n",
      "Epoch [600/700], Validation Loss: 0.0057\n",
      "Epoch [601/700], Validation Loss: 0.0057\n",
      "Epoch [602/700], Validation Loss: 0.0058\n",
      "Epoch [603/700], Validation Loss: 0.0057\n",
      "Epoch 00603: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [604/700], Validation Loss: 0.0057\n",
      "Epoch [605/700], Validation Loss: 0.0056\n",
      "Epoch [606/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [607/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [608/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [609/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [610/700], Validation Loss: 0.0055\n",
      "Epoch [611/700], Validation Loss: 0.0055\n",
      "Epoch [612/700], Validation Loss: 0.0055\n",
      "Epoch [613/700], Validation Loss: 0.0055\n",
      "Epoch [614/700], Validation Loss: 0.0055\n",
      "Epoch [615/700], Validation Loss: 0.0055\n",
      "Epoch [616/700], Validation Loss: 0.0055\n",
      "Epoch [617/700], Validation Loss: 0.0055\n",
      "Epoch [618/700], Validation Loss: 0.0055\n",
      "Epoch [619/700], Validation Loss: 0.0055\n",
      "Epoch [620/700], Validation Loss: 0.0055\n",
      "Epoch [621/700], Validation Loss: 0.0055\n",
      "Epoch [622/700], Validation Loss: 0.0055\n",
      "Epoch [623/700], Validation Loss: 0.0055\n",
      "Epoch [624/700], Validation Loss: 0.0055\n",
      "Epoch [625/700], Validation Loss: 0.0055\n",
      "Epoch 00625: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch [626/700], Validation Loss: 0.0055\n",
      "Epoch [627/700], Validation Loss: 0.0055\n",
      "Epoch [628/700], Validation Loss: 0.0055\n",
      "Epoch [629/700], Validation Loss: 0.0055\n",
      "Epoch [630/700], Validation Loss: 0.0055\n",
      "Epoch [631/700], Validation Loss: 0.0055\n",
      "Epoch [632/700], Validation Loss: 0.0055\n",
      "Epoch [633/700], Validation Loss: 0.0055\n",
      "Epoch [634/700], Validation Loss: 0.0055\n",
      "Epoch [635/700], Validation Loss: 0.0055\n",
      "Early stopping...\n",
      "MAE: 0.0495, MSE: 0.0057, RMSE: 0.0754\n",
      "MAE: 0.0495, MSE: 0.0057, RMSE: 0.0754\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    data loader, model 초기화하는 코드\n",
    "    epoch만큼 train 실행하는 코드\n",
    "    모델 저장하는 코드\n",
    "\"\"\"\n",
    "########################################## Hyperparameters ##########################################\n",
    "hidden_dim = 128\n",
    "n_layers = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 700\n",
    "batch_size = 128\n",
    "use_thread = True # GPU 사용시 num_workers 사용 유무\n",
    "scheduler_factor = 0.5 # scheduler를 위한 파라미터\n",
    "scheduler_patience = 15 # scheduler_patience동안 val loss 감소하지 않으면 learning rate를 scheduler_factor배 수행함\n",
    "patience = 25 # number of epochs to wait before stopping(early stopping을 위한 파라미터로, scheduler_patience보다 높게 설정하기)\n",
    "pretrained_model_path = \"\"\n",
    "########################################################################################################\n",
    "input_dim = len(train_set[0][0][0]) # 7\n",
    "output_dim = 24*56\n",
    "stop_epoch = -1\n",
    "\n",
    "if pretrained_model_path != \"\":\n",
    "    assert os.path.exists(pretrained_model_path)\n",
    "    pretrained_params_path = pretrained_model_path.replace(\"model_\", \"\")\n",
    "    pretrained_params_path = pretrained_params_path.replace(\"pt\", \"pkl\")\n",
    "    assert os.path.exists(pretrained_params_path)\n",
    "\n",
    "\n",
    "# 결과를 저장할 폴더의 경로를 지정합니다.\n",
    "results_folder = \"/home/kimyirum/EMS/ict-2023-ems/load/results/\"\n",
    "# 현재의 시간 정보를 가져옵니다.\n",
    "now = datetime.now()\n",
    "# 현재 시간 정보를 문자열 포맷으로 변환합니다. (예: 20230807_143000)\n",
    "now_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "# 결과 메트릭을 저장할 파일의 이름을 지정합니다.\n",
    "filename_metrics = f'{now_str}.pkl'\n",
    "# 학습된 모델을 저장할 파일의 이름을 지정합니다.\n",
    "filename_model = f'model_{now_str}.pt'\n",
    "\n",
    "# DataLoader\n",
    "# use_thread 변수의 값에 따라 DataLoader의 num_workers 값을 설정합니다.\n",
    "if use_thread:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "else:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# LSTM 모델, 손실 함수, 최적화 도구를 초기화합니다.\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
    "\n",
    "# 만약 pretrained 모델이 존재한다면, 해당 모델을 로드합니다.\n",
    "if os.path.exists(pretrained_model_path):\n",
    "    model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    print(\"Loaded pretrained model.\")\n",
    "\n",
    "    with open(pretrained_params_path, 'rb') as f:\n",
    "        loaded_results = pickle.load(f)\n",
    "        learning_rate = loaded_results['Hyperparameters']['final_learning_rate']\n",
    "        print(\"Use lr:\", learning_rate)\n",
    "\n",
    "criterion = nn.MSELoss()  # MSE 손실 함수를 사용합니다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam 최적화 도구를 사용합니다.\n",
    "# ReduceLROnPlateau 스케줄러는 검증 손실이 개선되지 않을 때 학습률을 동적으로 감소시킵니다. 학습률 감소의 타이밍을 검증 성능에 기반하여 자동으로 조절할 수 있습니다.\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience, verbose=True)\n",
    "\n",
    "# 초기 검증 손실을 무한대로 설정합니다.\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve_epoch = 0\n",
    "\n",
    "########################################## Training loop ##########################################\n",
    "# 주어진 에포크만큼 모델을 학습합니다.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # 순전파를 수행합니다.\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # 역전파 및 최적화를 수행합니다.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 모델을 검증합니다.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data, targets in val_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}', end='')\n",
    "        \n",
    "        # 검증 손실이 감소했을 경우, 모델을 저장합니다.\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\", Save model\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), results_folder+filename_model)\n",
    "            no_improve_epoch = 0\n",
    "        else:\n",
    "            print(\"\")\n",
    "            no_improve_epoch += 1\n",
    "            \n",
    "        # 조기 종료 조건: 검증 손실이 연속으로 patience동안 개선되지 않을 때 학습을 중단합니다.\n",
    "        if no_improve_epoch > patience:\n",
    "            print('Early stopping...')\n",
    "            stop_epoch = epoch\n",
    "            break\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "final_learning_rate = current_lr\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "# 모델을 평가 모드로 설정합니다. 이는 dropout, batch normalization 등의 레이어가 \n",
    "# 학습 모드와 다르게 작동해야 할 때 필요합니다.\n",
    "model.eval()\n",
    "\n",
    "# torch.no_grad()를 사용하여 autograd의 gradient 계산을 비활성화합니다. \n",
    "# 이렇게 하면 메모리 사용량을 줄이고 속도를 높일 수 있습니다.\n",
    "with torch.no_grad():\n",
    "    all_targets = []  # 실제 목표 값들을 저장할 리스트를 초기화합니다.\n",
    "    all_outputs = []  # 모델의 예측 값을 저장할 리스트를 초기화합니다.\n",
    "    for data, targets in test_loader:  # 테스트 데이터로더에서 배치를 반복적으로 가져옵니다.\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(data)  # 모델을 사용하여 입력 데이터에 대한 예측값을 생성합니다.\n",
    "        all_targets.append(targets.cpu().numpy())  # 목표 값을 리스트에 추가합니다.\n",
    "        all_outputs.append(outputs.cpu().numpy())  # 예측 값을 리스트에 추가합니다.\n",
    "\n",
    "# 목표 값과 예측 값을 모두 단일 넘파이 배열로 연결(flatten)합니다.\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# 평균 절대 오차(MAE), 평균 제곱 오차(MSE) 및 제곱근 평균 제곱 오차(RMSE)를 계산합니다.\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# 계산된 지표들을 출력합니다. 이 값들은 모델의 성능을 평가하는 데 사용됩니다.\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "# 모델을 평가 모드로 설정합니다. 이는 dropout, batch normalization 등의 레이어가 \n",
    "# 학습 모드와 다르게 작동해야 할 때 필요합니다.\n",
    "model.eval()\n",
    "\n",
    "# torch.no_grad()를 사용하여 autograd의 gradient 계산을 비활성화합니다. \n",
    "# 이렇게 하면 메모리 사용량을 줄이고 속도를 높일 수 있습니다.\n",
    "with torch.no_grad():\n",
    "    all_targets = []  # 실제 목표 값들을 저장할 리스트를 초기화합니다.\n",
    "    all_outputs = []  # 모델의 예측 값을 저장할 리스트를 초기화합니다.\n",
    "    for data, targets in test_loader:  # 테스트 데이터로더에서 배치를 반복적으로 가져옵니다.\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(data)  # 모델을 사용하여 입력 데이터에 대한 예측값을 생성합니다.\n",
    "        all_targets.append(targets.cpu().numpy())  # 목표 값을 리스트에 추가합니다.\n",
    "        all_outputs.append(outputs.cpu().numpy())  # 예측 값을 리스트에 추가합니다.\n",
    "\n",
    "# 목표 값과 예측 값을 모두 단일 넘파이 배열로 연결(flatten)합니다.\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# 평균 절대 오차(MAE), 평균 제곱 오차(MSE) 및 제곱근 평균 제곱 오차(RMSE)를 계산합니다.\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# 계산된 지표들을 출력합니다. 이 값들은 모델의 성능을 평가하는 데 사용됩니다.\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'final_learning_rate': 6.25e-05, 'batch_size': 128, 'max_epochs': 700, 'stop_epoch': 634, 'hidden_dim': 128, 'n_layers': 5}\n",
      "MinMaxScaler()\n",
      "{'MAE': 0.049479816, 'MSE': 0.005687788, 'RMSE': 0.07541742589649188}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    위에서 학습시킨 모델의 성능이 괜찮다면, 파라미터 정보를 pkl 파일로 저장하는 코드\n",
    "\"\"\"\n",
    "\n",
    "# 사용한 하이퍼파라미터들을 저장합니다.\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'final_learning_rate': final_learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'max_epochs': num_epochs,\n",
    "    'stop_epoch': stop_epoch,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'n_layers': n_layers\n",
    "}\n",
    "\n",
    "# 성능 지표를 저장합니다.\n",
    "metrics = {\n",
    "    'MAE': mae,\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "}\n",
    "\n",
    "# 데이터 정규화에 사용된 scaler를 저장합니다.\n",
    "scalers = dataset.scaler\n",
    "\n",
    "# 위에서 정의한 모든 결과를 하나의 사전에 합칩니다.\n",
    "results = {\n",
    "    'Hyperparameters': hyperparams,\n",
    "    'Scalers': scalers,\n",
    "    'Metrics': metrics,\n",
    "    'pretrained_model': pretrained_model_path\n",
    "}\n",
    "\n",
    "# 결합된 결과를 pickle 파일로 저장합니다.\n",
    "with open(results_folder + filename_metrics, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# 테스트를 위해\n",
    "# pickle 파일로부터 결과를 불러옵니다.\n",
    "with open(results_folder + filename_metrics, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "\n",
    "# 불러온 결과에서 데이터에 접근할 수 있습니다.\n",
    "print(loaded_results['Hyperparameters'])\n",
    "print(loaded_results['Scalers'])\n",
    "print(loaded_results['Metrics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1344]) torch.Size([1, 1344])\n",
      "Error for next 24 hours:\n",
      "        0_SV-2      1_SV-5    2_SV-6      3_SV-7  4_HV-NM1    5_HV-NM2  \\\n",
      "0    50.527500   28.966450  0.000819   46.182881 -0.008046   55.165710   \n",
      "1   -13.960328    2.582501 -0.002125   -7.434911 -0.005898   16.882650   \n",
      "2    37.978686   48.710406  0.002438   29.686524 -0.002795   20.764150   \n",
      "3    48.597899   19.981914 -0.004999   20.408947 -0.005826   39.824878   \n",
      "4   -18.034218   24.736916  0.007464   28.212995  0.000402  -37.333174   \n",
      "5     8.859285  -22.291005 -0.003332   15.098882 -0.003268    3.976591   \n",
      "6    -7.066777    1.073585  0.000137   14.835310 -0.000541    7.787449   \n",
      "7    -5.714813   -9.631570 -0.006254  -31.641669 -0.003880  -18.331352   \n",
      "8    31.163661   45.958010 -0.002350   46.938434  0.006098   -1.510224   \n",
      "9    -9.729386    2.710956  0.001591   -6.178728  0.001307  -11.279135   \n",
      "10    7.679938  -35.584023  0.000678   -2.033271  0.004229   15.901060   \n",
      "11   75.809948   66.153176 -0.002680   52.032414 -0.000497   88.643986   \n",
      "12  -67.777108  -86.202069 -0.000393  -94.957020  0.008783  -73.942187   \n",
      "13  -88.772429 -112.362249  0.001149  -91.979325 -0.005554  -84.991826   \n",
      "14  -92.817852  -41.752859 -0.003540  -49.275197 -0.002842  -85.542802   \n",
      "15  -43.969510  -49.196214  0.014530  -68.990249 -0.010222  -39.444559   \n",
      "16   64.106321   71.135227 -0.002618   53.456675  0.003202   46.572143   \n",
      "17 -160.275759 -150.177892 -0.001456 -113.140333  0.002588 -107.188859   \n",
      "18  -36.419374  -26.569119 -0.005293  -40.652902 -0.000911    3.206991   \n",
      "19   63.866161   59.725350  0.000976   54.723446 -0.000027   70.836010   \n",
      "20   46.892845   35.220199  0.003878   43.149769 -0.002897   40.044567   \n",
      "21  -81.786936  -76.624811 -0.008892  -26.854402 -0.010312   -2.572373   \n",
      "22   10.575089    5.792255 -0.003764  -30.104140  0.000737  -45.946044   \n",
      "23   66.403969   61.282802  0.002926   29.214481  0.007611   47.657498   \n",
      "\n",
      "       6_고압콘덴샤  7_신재생에너지동     8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동  47_기숙사9동  \\\n",
      "0    50.394193  19.261376  45.995859 -12.510435  ...    18.334950 -4.902944   \n",
      "1    49.366514 -14.749827  27.848417 -18.555924  ...    23.520086 -7.790222   \n",
      "2     9.002085  14.331758  36.210473 -12.145750  ...     8.685478 -0.877742   \n",
      "3   -25.124286  11.761511  16.660449  -3.772368  ...    -1.939635  0.170250   \n",
      "4  -117.805035 -10.010941  17.323496   5.049784  ...    -5.952011  8.101851   \n",
      "5   -37.469466 -16.006336  10.711866  -5.172644  ...    -3.348294  4.878108   \n",
      "6     7.778215 -11.189296   4.337688   8.865832  ...     7.388200 -2.836057   \n",
      "7     8.762587  -1.426447  27.729169  -5.981579  ...    21.150270  0.839424   \n",
      "8     0.713101 -16.756590  10.514363   2.249240  ...    43.295123 -0.928797   \n",
      "9    14.290962   0.558693  12.725696   6.114832  ...    33.433993 -4.224854   \n",
      "10   -6.237951 -32.023143   7.100745  12.250580  ...    25.526302 -0.199180   \n",
      "11   -1.973603  -8.437340  -7.220020   3.346216  ...    16.971987  4.105204   \n",
      "12    9.928785 -44.273035  -0.665364   2.294904  ...    20.328013 -1.961344   \n",
      "13   33.705794 -46.790563   4.954118  -8.909799  ...   -41.783903 -1.812554   \n",
      "14  -28.451628 -31.416252  29.745432  -3.216975  ...   -31.318764 -0.688074   \n",
      "15   27.002090 -15.980331   9.482763  -4.979794  ...   -40.628904  7.679148   \n",
      "16  -30.754227  25.330483   5.188112   7.440669  ...   -40.935550 -1.099580   \n",
      "17   14.673144 -30.476536  -9.888662  -4.526059  ...   -84.494974  1.555111   \n",
      "18   -5.209685 -45.049674  20.486401  -6.861713  ...   -43.230888  3.121707   \n",
      "19   13.353688  -5.602049  31.708428   2.159363  ...   -29.568940 -5.017216   \n",
      "20   51.600388  -2.399998  14.128893   0.969713  ...     7.180671 -2.883997   \n",
      "21   40.465462 -27.890787   7.583563   0.260522  ...   -16.585458 -0.229741   \n",
      "22   52.922407  15.140072   6.427911  15.555584  ...   -49.863462 -2.301481   \n",
      "23   42.443871 -20.298928  -4.921942  10.290800  ...    -8.976637  1.564666   \n",
      "\n",
      "    48_고등광/극초단  49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  \\\n",
      "0   -85.434547      0.530040         -5.200732    -3.230631    -0.237261   \n",
      "1   -59.759907      9.427675          2.695165     0.760129    -2.420558   \n",
      "2   -33.432398      8.231046         -3.474900     2.466049    -5.725890   \n",
      "3   -52.810130     -0.267384          0.501851     3.792504    -5.835632   \n",
      "4   -15.526712      1.020103          1.998063     4.405981    -3.501550   \n",
      "5   -27.919176      2.509588          0.262302    -0.481483     1.009073   \n",
      "6     9.275176     -2.091194         -2.069834     1.041293     3.893532   \n",
      "7    63.186495      5.635574          0.160378     5.421657    -4.431489   \n",
      "8    27.504277      7.505304         -0.403703     0.183242    -0.977254   \n",
      "9    33.606243      4.158232          9.646215    -5.082751     0.750069   \n",
      "10   22.570968     -3.193287         -1.757079     2.608935    -0.119195   \n",
      "11  -37.354545     -4.086200          2.095493     4.834269     7.311216   \n",
      "12  -17.150672     -1.415259          2.161917    -6.301185    -6.746867   \n",
      "13  -57.373321     -4.835812         -5.867551     1.843071    -9.181808   \n",
      "14   23.884779      5.037157          9.997578   -20.137512     1.772331   \n",
      "15  -12.286883      0.617908         -0.861364    -3.232818     1.127153   \n",
      "16  -11.056521      4.398038          3.066047    11.918955     2.285553   \n",
      "17  -22.755953      5.543541          0.332612    -1.805453     2.154955   \n",
      "18  -28.863276      2.391158         -5.072182    -1.981877     2.310501   \n",
      "19    7.105979      0.214304         -4.470998     3.854217    -2.656908   \n",
      "20   40.345633      4.815566          5.529682    -2.022271     0.422992   \n",
      "21   -1.860304     -2.961468          1.535063    -7.744993     0.979212   \n",
      "22   28.703251     -1.131227          2.624102   -13.639582    -0.011217   \n",
      "23   88.849995      1.012548          6.654790    -1.778317     0.383124   \n",
      "\n",
      "    53_LG도서관(E)  54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0    -10.027988    -1.519493     -3.089788  \n",
      "1     -5.100740     4.089482     -4.392848  \n",
      "2     -5.039020     0.154163     -1.903707  \n",
      "3     -7.556005    -1.347241     -1.083977  \n",
      "4     -1.663167    -5.808759      1.475500  \n",
      "5     -1.451687    -4.104776     -0.072471  \n",
      "6      4.367154    -5.934312      8.441558  \n",
      "7      1.209976     2.825169     -0.522560  \n",
      "8      0.765893    -0.480161      4.302915  \n",
      "9      2.087251    -5.868232      4.902716  \n",
      "10     4.187980    -0.601702      2.174118  \n",
      "11     2.407488     7.660113      3.264114  \n",
      "12    -3.136501    -4.667595     -9.826128  \n",
      "13    -7.115153   -11.287295     -1.763801  \n",
      "14    -2.535998    10.030014     -4.939228  \n",
      "15    -5.637404   -10.844033      1.049265  \n",
      "16    -8.682526     6.629433     10.042292  \n",
      "17    -4.496517   -18.515893      2.196897  \n",
      "18    -0.206324   -27.413733      2.929889  \n",
      "19    14.799848    -4.169855     -0.755743  \n",
      "20     2.609843    -9.387938     -1.410873  \n",
      "21     1.734152    -9.046948      1.483682  \n",
      "22    14.907135   -18.700716      9.417005  \n",
      "23    -1.375570     3.977238     11.256939  \n",
      "\n",
      "[24 rows x 56 columns]\n",
      "MAE: 15.7970, MSE: 837.0111, RMSE: 28.9311 (no normalization)\n",
      "err_total:  -1485.148358016859\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    테스트 세트의 첫 번째 시퀀스에 대한 예측을 수행한 후, 예측된 값과 실제 목표값 사이의 차이를 계산하는 코드\n",
    "    이러한 차이를 기반으로 여러 성능 지표를 계산하며, 전체 에러를 출력하는 코드\n",
    "    *df는 원래의 데이터셋이라 가정하며 'date' 컬럼을 포함한다고 가정합니다.\n",
    "\"\"\"\n",
    "# 건물 이름을 가져옵니다.\n",
    "building_names = df.columns[-56:]  # 필요에 따라 이 값을 조절하세요.\n",
    "# 테스트 세트를 위한 DataLoader를 생성합니다.\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "# 테스트 세트에서 첫 번째 시퀀스와 그 목표값을 가져옵니다.\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# 모델을 평가 모드로 전환합니다.\n",
    "model = model.to('cpu')  \n",
    "model.eval()\n",
    "\n",
    "# 예측을 수행합니다.\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "print(prediction.shape, real_target.shape)\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "# 패딩을 추가합니다.\n",
    "padding = np.zeros((prediction.shape[0], 7))\n",
    "prediction_pad = np.hstack((padding, prediction))\n",
    "real_target_pad = np.hstack((padding, real_target))\n",
    "# print(prediction_pad.shape, real_target_pad.shape)\n",
    "\n",
    "# 역변환을 적용하여 정규화를 해제합니다.\n",
    "prediction_inv = dataset.scaler.inverse_transform(prediction_pad)\n",
    "real_target_inv = dataset.scaler.inverse_transform(real_target_pad)\n",
    "\n",
    "# 처음 7개의 컬럼을 삭제합니다.\n",
    "prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "real_target_inv = np.delete(real_target_inv, np.s_[:7], axis=1)\n",
    "\n",
    "# 원래의 형태로 다시 변형합니다.\n",
    "prediction = prediction_inv.reshape(prediction.shape)\n",
    "real_target = real_target_inv.reshape(real_target.shape)\n",
    "\n",
    "# 에러(실제 목표값과 예측값의 차이)를 계산합니다.\n",
    "error = real_target - prediction\n",
    "\n",
    "# 예측값을 위한 DataFrame을 생성합니다.\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "err_total = total = error_df.values.flatten().sum()\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n",
    "\n",
    "# 성능 지표를 계산합니다.\n",
    "mae_n = mean_absolute_error(real_target, prediction)\n",
    "mse_n = mean_squared_error(real_target, prediction)\n",
    "rmse_n = sqrt(mse_n)\n",
    "print(f'MAE: {mae_n:.4f}, MSE: {mse_n:.4f}, RMSE: {rmse_n:.4f} (no normalization)')\n",
    "print(\"err_total: \", err_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    최적의 하이퍼파라미터를 찾기 위해 optuna 라이브러리를 이용해서 실험하는 코드\n",
    "    *실험 log는 따로 기록되지 않으므로 결과 복붙해서 log_optuna.txt로 따로 저장함\n",
    "\"\"\"\n",
    "# Optuna를 사용할 것인지 여부를 결정하는 플래그\n",
    "do_optuna = False\n",
    "\n",
    "# optuna 관련 라이브러리를 가져옵니다.\n",
    "import optuna\n",
    "import optuna.logging\n",
    "\n",
    "# Optuna의 기본 로깅 핸들러를 활성화합니다.\n",
    "optuna.logging.enable_default_handler()\n",
    "\n",
    "# 로깅의 상세 수준을 설정합니다.\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# Optuna를 사용하여 최적화할 목적 함수를 정의합니다.\n",
    "def objective(trial):\n",
    "    # Optuna를 사용하여 하이퍼파라미터를 추정합니다.\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 50, 300)\n",
    "    n_layers = trial.suggest_int('n_layers', 5, 9)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    \n",
    "    # 데이터 로더를 설정합니다.\n",
    "    if use_thread:\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "        test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    else:\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 모델, 손실 함수, 최적화 알고리즘을 초기화합니다.\n",
    "    model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\") # 초기에는 무한대로 설정합니다.\n",
    "    no_improve_epoch = 0\n",
    "\n",
    "    # 훈련 루프입니다.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # 순전파\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 모델을 검증합니다.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for data, targets in val_loader:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_losses.append(loss.item())\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            # 검증 손실이 줄어들면 모델을 저장합니다.\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                no_improve_epoch = 0\n",
    "            else:\n",
    "                no_improve_epoch += 1\n",
    "                \n",
    "            # 일찍 중단하기 위한 조건\n",
    "            if no_improve_epoch > patience:\n",
    "                print('Early stopping...')\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# do_optuna 플래그가 True로 설정되어 있으면 Optuna를 사용하여 하이퍼파라미터를 최적화합니다.\n",
    "if do_optuna:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=100, n_jobs=4)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\" Value: \", trial.value)\n",
    "\n",
    "    print(\" Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
