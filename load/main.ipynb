{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 실행 시\n",
    "- 3번째 셀의 Hyperparameter 값을 원하는대로 설정하기\n",
    "- 저장위치를 바꾸고 싶다면 results_folder 변수에 경로 설정하기\n",
    "- 자세한 설명은 각 코드 셀의 설명 및 주석 참고!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    모델 및 데이터셋 클래스 정의하는 코드\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 시계열 데이터를 처리하는 클래스를 정의합니다.\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len  # 입력 시퀀스의 길이를 정의합니다.\n",
    "        self.pred_len = pred_len  # 예측할 시퀀스의 길이를 정의합니다.\n",
    "        self.scaler = MinMaxScaler()  # 데이터 정규화를 위한 MinMaxScaler 객체를 생성합니다.\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)  # 데이터 전처리 함수를 호출하여 dataframe을 전처리합니다.\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # 누락된 값을 시계열의 이전 값으로 채웁니다.\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # 숫자형 열을 [0, 1] 범위로 정규화합니다.\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # 범주형 변수를 원-핫 인코딩합니다.\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # 원래의 범주형 열을 삭제하고 인코딩된 열과 병합합니다.\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1  # 데이터셋의 전체 길이를 반환합니다.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len, :7]  # 입력 시퀀스의 앞 7열만 선택합니다.\n",
    "        # 마지막 56열이 전력 값이라고 가정하고 예측할 시퀀스를 선택합니다.\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] \n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # y 값을 평탄화하여 반환합니다.\n",
    "    \n",
    "# LSTM 모델을 정의합니다.\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # LSTM의 은닉층의 차원을 정의합니다.\n",
    "        self.n_layers = n_layers  # LSTM 층의 개수를 정의합니다.\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)  # LSTM 층을 정의합니다.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 완전 연결 층을 정의합니다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 은닉 상태와 셀 상태를 초기화합니다.\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM 층을 통해 데이터를 전달하고 출력을 얻습니다.\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 완전 연결 층을 통해 출력을 얻습니다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    데이터셋 읽고 set으로 분할하는 코드\n",
    "\"\"\"\n",
    "# pandas 라이브러리를 사용하여 엑셀 파일을 불러옵니다.\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data_KW.xlsx')\n",
    "\n",
    "# TimeSeriesDataset 클래스의 인스턴스를 생성합니다. 위에서 정의한 클래스를 사용하여 데이터를 전처리합니다.\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# 학습, 검증 및 테스트 데이터 세트의 크기를 정의합니다.\n",
    "train_size = int(0.7 * len(dataset))  # 전체 데이터의 70%를 학습 데이터로 사용\n",
    "val_size = int(0.2 * len(dataset))    # 전체 데이터의 20%를 검증 데이터로 사용\n",
    "test_size = len(dataset) - train_size - val_size  # 나머지 데이터를 테스트 데이터로 사용\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "# 전체 데이터셋을 학습, 검증 및 테스트 데이터 세트로 분할합니다.\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model.\n",
      "Use lr: 0.00025\n",
      "Epoch [1/700], Validation Loss: 0.0061, Save model\n",
      "Epoch [2/700], Validation Loss: 0.0061\n",
      "Epoch [3/700], Validation Loss: 0.0059, Save model\n",
      "Epoch [4/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [5/700], Validation Loss: 0.0059\n",
      "Epoch [6/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [7/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [8/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [9/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [10/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [11/700], Validation Loss: 0.0058, Save model\n",
      "Epoch [12/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [13/700], Validation Loss: 0.0058\n",
      "Epoch [14/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [15/700], Validation Loss: 0.0057\n",
      "Epoch [16/700], Validation Loss: 0.0057\n",
      "Epoch [17/700], Validation Loss: 0.0057\n",
      "Epoch [18/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [19/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [20/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [21/700], Validation Loss: 0.0057\n",
      "Epoch [22/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [23/700], Validation Loss: 0.0057\n",
      "Epoch [24/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [25/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [26/700], Validation Loss: 0.0057\n",
      "Epoch [27/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [28/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [29/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [30/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [31/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [32/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [33/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [34/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [35/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [36/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [37/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [38/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [39/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [40/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [41/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [42/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [43/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [44/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [45/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [46/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [47/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [48/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [49/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [50/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [51/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [52/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [53/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [54/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [55/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [56/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [57/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [58/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [59/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [60/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [61/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [62/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [63/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [64/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [65/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [66/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [67/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [68/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [69/700], Validation Loss: 0.0057\n",
      "Epoch [70/700], Validation Loss: 0.0057\n",
      "Epoch [71/700], Validation Loss: 0.0057\n",
      "Epoch [72/700], Validation Loss: 0.0057\n",
      "Epoch [73/700], Validation Loss: 0.0057\n",
      "Epoch [74/700], Validation Loss: 0.0057\n",
      "Epoch [75/700], Validation Loss: 0.0057\n",
      "Epoch [76/700], Validation Loss: 0.0057\n",
      "Epoch [77/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [78/700], Validation Loss: 0.0057\n",
      "Epoch [79/700], Validation Loss: 0.0059\n",
      "Epoch [80/700], Validation Loss: 0.0058\n",
      "Epoch [81/700], Validation Loss: 0.0057\n",
      "Epoch [82/700], Validation Loss: 0.0058\n",
      "Epoch [83/700], Validation Loss: 0.0057\n",
      "Epoch [84/700], Validation Loss: 0.0059\n",
      "Epoch [85/700], Validation Loss: 0.0057\n",
      "Epoch [86/700], Validation Loss: 0.0057\n",
      "Epoch [87/700], Validation Loss: 0.0057\n",
      "Epoch [88/700], Validation Loss: 0.0057\n",
      "Epoch [89/700], Validation Loss: 0.0057\n",
      "Epoch [90/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [91/700], Validation Loss: 0.0057\n",
      "Epoch [92/700], Validation Loss: 0.0057\n",
      "Epoch [93/700], Validation Loss: 0.0057\n",
      "Epoch [94/700], Validation Loss: 0.0057\n",
      "Epoch [95/700], Validation Loss: 0.0057\n",
      "Epoch [96/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [97/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [98/700], Validation Loss: 0.0057\n",
      "Epoch [99/700], Validation Loss: 0.0057\n",
      "Epoch [100/700], Validation Loss: 0.0057\n",
      "Epoch [101/700], Validation Loss: 0.0057\n",
      "Epoch [102/700], Validation Loss: 0.0057\n",
      "Epoch [103/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [104/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [105/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [106/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [107/700], Validation Loss: 0.0057\n",
      "Epoch [108/700], Validation Loss: 0.0057\n",
      "Epoch [109/700], Validation Loss: 0.0057\n",
      "Epoch [110/700], Validation Loss: 0.0057\n",
      "Epoch [111/700], Validation Loss: 0.0057\n",
      "Epoch [112/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [113/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [114/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [115/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [116/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [117/700], Validation Loss: 0.0057, Save model\n",
      "Epoch [118/700], Validation Loss: 0.0057\n",
      "Epoch [119/700], Validation Loss: 0.0057\n",
      "Epoch [120/700], Validation Loss: 0.0057\n",
      "Epoch [121/700], Validation Loss: 0.0057\n",
      "Epoch [122/700], Validation Loss: 0.0057\n",
      "Epoch [123/700], Validation Loss: 0.0057\n",
      "Epoch [124/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [125/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [126/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [127/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [128/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [129/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [130/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [131/700], Validation Loss: 0.0056\n",
      "Epoch [132/700], Validation Loss: 0.0056\n",
      "Epoch [133/700], Validation Loss: 0.0056\n",
      "Epoch [134/700], Validation Loss: 0.0057\n",
      "Epoch [135/700], Validation Loss: 0.0057\n",
      "Epoch [136/700], Validation Loss: 0.0057\n",
      "Epoch [137/700], Validation Loss: 0.0056\n",
      "Epoch [138/700], Validation Loss: 0.0056\n",
      "Epoch [139/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [140/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [141/700], Validation Loss: 0.0056\n",
      "Epoch [142/700], Validation Loss: 0.0056\n",
      "Epoch [143/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [144/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [145/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [146/700], Validation Loss: 0.0056\n",
      "Epoch [147/700], Validation Loss: 0.0056\n",
      "Epoch [148/700], Validation Loss: 0.0057\n",
      "Epoch [149/700], Validation Loss: 0.0057\n",
      "Epoch [150/700], Validation Loss: 0.0056\n",
      "Epoch [151/700], Validation Loss: 0.0056\n",
      "Epoch [152/700], Validation Loss: 0.0056\n",
      "Epoch [153/700], Validation Loss: 0.0056\n",
      "Epoch [154/700], Validation Loss: 0.0056\n",
      "Epoch [155/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [156/700], Validation Loss: 0.0056\n",
      "Epoch [157/700], Validation Loss: 0.0057\n",
      "Epoch [158/700], Validation Loss: 0.0057\n",
      "Epoch [159/700], Validation Loss: 0.0056\n",
      "Epoch [160/700], Validation Loss: 0.0056\n",
      "Epoch [161/700], Validation Loss: 0.0057\n",
      "Epoch [162/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [163/700], Validation Loss: 0.0056\n",
      "Epoch [164/700], Validation Loss: 0.0057\n",
      "Epoch [165/700], Validation Loss: 0.0056\n",
      "Epoch [166/700], Validation Loss: 0.0056\n",
      "Epoch [167/700], Validation Loss: 0.0056\n",
      "Epoch [168/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [169/700], Validation Loss: 0.0056\n",
      "Epoch [170/700], Validation Loss: 0.0056\n",
      "Epoch [171/700], Validation Loss: 0.0056\n",
      "Epoch [172/700], Validation Loss: 0.0056\n",
      "Epoch [173/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [174/700], Validation Loss: 0.0056\n",
      "Epoch [175/700], Validation Loss: 0.0056\n",
      "Epoch [176/700], Validation Loss: 0.0056\n",
      "Epoch [177/700], Validation Loss: 0.0056\n",
      "Epoch [178/700], Validation Loss: 0.0056\n",
      "Epoch [179/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [180/700], Validation Loss: 0.0056\n",
      "Epoch [181/700], Validation Loss: 0.0056\n",
      "Epoch [182/700], Validation Loss: 0.0056\n",
      "Epoch [183/700], Validation Loss: 0.0056\n",
      "Epoch [184/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [185/700], Validation Loss: 0.0056\n",
      "Epoch [186/700], Validation Loss: 0.0056\n",
      "Epoch [187/700], Validation Loss: 0.0056\n",
      "Epoch [188/700], Validation Loss: 0.0056\n",
      "Epoch [189/700], Validation Loss: 0.0056\n",
      "Epoch [190/700], Validation Loss: 0.0056\n",
      "Epoch [191/700], Validation Loss: 0.0056\n",
      "Epoch [192/700], Validation Loss: 0.0056\n",
      "Epoch [193/700], Validation Loss: 0.0056\n",
      "Epoch [194/700], Validation Loss: 0.0056\n",
      "Epoch [195/700], Validation Loss: 0.0056\n",
      "Epoch [196/700], Validation Loss: 0.0056\n",
      "Epoch [197/700], Validation Loss: 0.0056\n",
      "Epoch [198/700], Validation Loss: 0.0056\n",
      "Epoch [199/700], Validation Loss: 0.0056\n",
      "Epoch [200/700], Validation Loss: 0.0056\n",
      "Epoch 00200: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [201/700], Validation Loss: 0.0056\n",
      "Epoch [202/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [203/700], Validation Loss: 0.0056\n",
      "Epoch [204/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [205/700], Validation Loss: 0.0056\n",
      "Epoch [206/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [207/700], Validation Loss: 0.0056\n",
      "Epoch [208/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [209/700], Validation Loss: 0.0056\n",
      "Epoch [210/700], Validation Loss: 0.0056\n",
      "Epoch [211/700], Validation Loss: 0.0056\n",
      "Epoch [212/700], Validation Loss: 0.0056\n",
      "Epoch [213/700], Validation Loss: 0.0056\n",
      "Epoch [214/700], Validation Loss: 0.0056\n",
      "Epoch [215/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [216/700], Validation Loss: 0.0056\n",
      "Epoch [217/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [218/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [219/700], Validation Loss: 0.0056\n",
      "Epoch [220/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [221/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [222/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [223/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [224/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [225/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [226/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [227/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [228/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [229/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [230/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [231/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [232/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [233/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [234/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [235/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [236/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [237/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [238/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [239/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [240/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [241/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [242/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [243/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [244/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [245/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [246/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [247/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [248/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [249/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [250/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [251/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [252/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [253/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [254/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [255/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [256/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [257/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [258/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [259/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [260/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [261/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [262/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [263/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [264/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [265/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [266/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [267/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [268/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [269/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [270/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [271/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [272/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [273/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [274/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [275/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [276/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [277/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [278/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [279/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [280/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [281/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [282/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [283/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [284/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [285/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [286/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [287/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [288/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [289/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [290/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [291/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [292/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [293/700], Validation Loss: 0.0056, Save model\n",
      "Epoch [294/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [295/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [296/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [297/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [298/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [299/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [300/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [301/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [302/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [303/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [304/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [305/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [306/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [307/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [308/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [309/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [310/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [311/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [312/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [313/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [314/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [315/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [316/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [317/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [318/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [319/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [320/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [321/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [322/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [323/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [324/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [325/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [326/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [327/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [328/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [329/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [330/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [331/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [332/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [333/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [334/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [335/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [336/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [337/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [338/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [339/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [340/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [341/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [342/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [343/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [344/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [345/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [346/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [347/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [348/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [349/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [350/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [351/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [352/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [353/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [354/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [355/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [356/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [357/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [358/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [359/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [360/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [361/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [362/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [363/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [364/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [365/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [366/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [367/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [368/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [369/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [370/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [371/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [372/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [373/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [374/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [375/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [376/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [377/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [378/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [379/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [380/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [381/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [382/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [383/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [384/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [385/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [386/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [387/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [388/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [389/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [390/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [391/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [392/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [393/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [394/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [395/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [396/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [397/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [398/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [399/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [400/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [401/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [402/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [403/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [404/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [405/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [406/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [407/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [408/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [409/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [410/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [411/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [412/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [413/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [414/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [415/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [416/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [417/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [418/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [419/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [420/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [421/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [422/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [423/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [424/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [425/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [426/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [427/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [428/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [429/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [430/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [431/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [432/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [433/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [434/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [435/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [436/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [437/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [438/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [439/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [440/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [441/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [442/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [443/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [444/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [445/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [446/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [447/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [448/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [449/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [450/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [451/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [452/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [453/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [454/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [455/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [456/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [457/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [458/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [459/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [460/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [461/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [462/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [463/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [464/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [465/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [466/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [467/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [468/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [469/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [470/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [471/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [472/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [473/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [474/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [475/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [476/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [477/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [478/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [479/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [480/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [481/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [482/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [483/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [484/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [485/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [486/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [487/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [488/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [489/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [490/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [491/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [492/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [493/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [494/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [495/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [496/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [497/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [498/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [499/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [500/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [501/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [502/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [503/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [504/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [505/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [506/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [507/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [508/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [509/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [510/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [511/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [512/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [513/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [514/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [515/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [516/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [517/700], Validation Loss: 0.0055\n",
      "Epoch [518/700], Validation Loss: 0.0055\n",
      "Epoch [519/700], Validation Loss: 0.0055\n",
      "Epoch [520/700], Validation Loss: 0.0055\n",
      "Epoch [521/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [522/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [523/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [524/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [525/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [526/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [527/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [528/700], Validation Loss: 0.0055\n",
      "Epoch [529/700], Validation Loss: 0.0055\n",
      "Epoch [530/700], Validation Loss: 0.0055\n",
      "Epoch [531/700], Validation Loss: 0.0055\n",
      "Epoch [532/700], Validation Loss: 0.0055\n",
      "Epoch [533/700], Validation Loss: 0.0055\n",
      "Epoch [534/700], Validation Loss: 0.0055\n",
      "Epoch [535/700], Validation Loss: 0.0055\n",
      "Epoch [536/700], Validation Loss: 0.0055\n",
      "Epoch [537/700], Validation Loss: 0.0055\n",
      "Epoch [538/700], Validation Loss: 0.0055\n",
      "Epoch [539/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [540/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [541/700], Validation Loss: 0.0055\n",
      "Epoch [542/700], Validation Loss: 0.0055\n",
      "Epoch [543/700], Validation Loss: 0.0055\n",
      "Epoch [544/700], Validation Loss: 0.0055\n",
      "Epoch [545/700], Validation Loss: 0.0055\n",
      "Epoch [546/700], Validation Loss: 0.0055\n",
      "Epoch [547/700], Validation Loss: 0.0055\n",
      "Epoch [548/700], Validation Loss: 0.0055\n",
      "Epoch [549/700], Validation Loss: 0.0056\n",
      "Epoch [550/700], Validation Loss: 0.0055\n",
      "Epoch [551/700], Validation Loss: 0.0055\n",
      "Epoch [552/700], Validation Loss: 0.0055\n",
      "Epoch [553/700], Validation Loss: 0.0055\n",
      "Epoch [554/700], Validation Loss: 0.0055\n",
      "Epoch [555/700], Validation Loss: 0.0055\n",
      "Epoch [556/700], Validation Loss: 0.0055\n",
      "Epoch 00556: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch [557/700], Validation Loss: 0.0055\n",
      "Epoch [558/700], Validation Loss: 0.0055\n",
      "Epoch [559/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [560/700], Validation Loss: 0.0055\n",
      "Epoch [561/700], Validation Loss: 0.0055\n",
      "Epoch [562/700], Validation Loss: 0.0055\n",
      "Epoch [563/700], Validation Loss: 0.0055\n",
      "Epoch [564/700], Validation Loss: 0.0055, Save model\n",
      "Epoch [565/700], Validation Loss: 0.0055\n",
      "Epoch [566/700], Validation Loss: 0.0055\n",
      "Epoch [567/700], Validation Loss: 0.0055\n",
      "Epoch [568/700], Validation Loss: 0.0055\n",
      "Epoch [569/700], Validation Loss: 0.0055\n",
      "Epoch [570/700], Validation Loss: 0.0055\n",
      "Epoch [571/700], Validation Loss: 0.0055\n",
      "Epoch [572/700], Validation Loss: 0.0055\n",
      "Epoch [573/700], Validation Loss: 0.0055\n",
      "Epoch [574/700], Validation Loss: 0.0055\n",
      "Epoch [575/700], Validation Loss: 0.0055\n",
      "Epoch [576/700], Validation Loss: 0.0055\n",
      "Epoch [577/700], Validation Loss: 0.0055\n",
      "Epoch [578/700], Validation Loss: 0.0055\n",
      "Epoch [579/700], Validation Loss: 0.0055\n",
      "Epoch [580/700], Validation Loss: 0.0055\n",
      "Epoch 00580: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch [581/700], Validation Loss: 0.0055\n",
      "Epoch [582/700], Validation Loss: 0.0055\n",
      "Epoch [583/700], Validation Loss: 0.0055\n",
      "Epoch [584/700], Validation Loss: 0.0055\n",
      "Epoch [585/700], Validation Loss: 0.0055\n",
      "Epoch [586/700], Validation Loss: 0.0055\n",
      "Epoch [587/700], Validation Loss: 0.0055\n",
      "Epoch [588/700], Validation Loss: 0.0055\n",
      "Epoch [589/700], Validation Loss: 0.0055\n",
      "Epoch [590/700], Validation Loss: 0.0055\n",
      "Early stopping...\n",
      "MAE: 0.0490, MSE: 0.0056, RMSE: 0.0747\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    data loader, model 초기화하는 코드\n",
    "    epoch만큼 train 실행하는 코드\n",
    "    모델 저장하는 코드\n",
    "\"\"\"\n",
    "########################################## Hyperparameters ##########################################\n",
    "hidden_dim = 128\n",
    "n_layers = 7\n",
    "learning_rate = 0.001\n",
    "num_epochs = 700\n",
    "batch_size = 256\n",
    "use_thread = True # GPU 사용시 num_workers 사용 유무\n",
    "scheduler_factor = 0.5 # scheduler를 위한 파라미터\n",
    "scheduler_patience = 15 # scheduler_patience동안 val loss 감소하지 않으면 learning rate를 scheduler_factor배 수행함\n",
    "patience = 25 # number of epochs to wait before stopping(early stopping을 위한 파라미터로, scheduler_patience보다 높게 설정하기)\n",
    "pretrained_model_path = \"/home/kimyirum/EMS/ict-2023-ems/load/results/model_20230807_180431.pt\"\n",
    "########################################################################################################\n",
    "input_dim = len(train_set[0][0][0])\n",
    "output_dim = 24*56\n",
    "stop_epoch = -1\n",
    "\n",
    "if pretrained_model_path != \"\":\n",
    "    assert os.path.exists(pretrained_model_path)\n",
    "    pretrained_params_path = pretrained_model_path.replace(\"model_\", \"\")\n",
    "    pretrained_params_path = pretrained_params_path.replace(\"pt\", \"pkl\")\n",
    "    assert os.path.exists(pretrained_params_path)\n",
    "\n",
    "\n",
    "# 결과를 저장할 폴더의 경로를 지정합니다.\n",
    "results_folder = \"/home/kimyirum/EMS/ict-2023-ems/load/results/\"\n",
    "# 현재의 시간 정보를 가져옵니다.\n",
    "now = datetime.now()\n",
    "# 현재 시간 정보를 문자열 포맷으로 변환합니다. (예: 20230807_143000)\n",
    "now_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "# 결과 메트릭을 저장할 파일의 이름을 지정합니다.\n",
    "filename_metrics = f'{now_str}.pkl'\n",
    "# 학습된 모델을 저장할 파일의 이름을 지정합니다.\n",
    "filename_model = f'model_{now_str}.pt'\n",
    "\n",
    "# DataLoader\n",
    "# use_thread 변수의 값에 따라 DataLoader의 num_workers 값을 설정합니다.\n",
    "if use_thread:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "else:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# LSTM 모델, 손실 함수, 최적화 도구를 초기화합니다.\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "\n",
    "# 만약 pretrained 모델이 존재한다면, 해당 모델을 로드합니다.\n",
    "if os.path.exists(pretrained_model_path):\n",
    "    model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    print(\"Loaded pretrained model.\")\n",
    "\n",
    "    with open(pretrained_params_path, 'rb') as f:\n",
    "        loaded_results = pickle.load(f)\n",
    "        learning_rate = loaded_results['Hyperparameters']['final_learning_rate']\n",
    "        print(\"Use lr:\", learning_rate)\n",
    "\n",
    "criterion = nn.MSELoss()  # MSE 손실 함수를 사용합니다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam 최적화 도구를 사용합니다.\n",
    "# ReduceLROnPlateau 스케줄러는 검증 손실이 개선되지 않을 때 학습률을 동적으로 감소시킵니다. 학습률 감소의 타이밍을 검증 성능에 기반하여 자동으로 조절할 수 있습니다.\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience, verbose=True)\n",
    "\n",
    "# 초기 검증 손실을 무한대로 설정합니다.\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve_epoch = 0\n",
    "\n",
    "########################################## Training loop ##########################################\n",
    "# 주어진 에포크만큼 모델을 학습합니다.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # 순전파를 수행합니다.\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # 역전파 및 최적화를 수행합니다.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 모델을 검증합니다.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data, targets in val_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}', end='')\n",
    "        \n",
    "        # 검증 손실이 감소했을 경우, 모델을 저장합니다.\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\", Save model\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), results_folder+filename_model)\n",
    "            no_improve_epoch = 0\n",
    "        else:\n",
    "            print(\"\")\n",
    "            no_improve_epoch += 1\n",
    "            \n",
    "        # 조기 종료 조건: 검증 손실이 연속으로 patience동안 개선되지 않을 때 학습을 중단합니다.\n",
    "        if no_improve_epoch > patience:\n",
    "            print('Early stopping...')\n",
    "            stop_epoch = epoch\n",
    "            break\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "final_learning_rate = current_lr\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "# 모델을 평가 모드로 설정합니다. 이는 dropout, batch normalization 등의 레이어가 \n",
    "# 학습 모드와 다르게 작동해야 할 때 필요합니다.\n",
    "model.eval()\n",
    "\n",
    "# torch.no_grad()를 사용하여 autograd의 gradient 계산을 비활성화합니다. \n",
    "# 이렇게 하면 메모리 사용량을 줄이고 속도를 높일 수 있습니다.\n",
    "with torch.no_grad():\n",
    "    all_targets = []  # 실제 목표 값들을 저장할 리스트를 초기화합니다.\n",
    "    all_outputs = []  # 모델의 예측 값을 저장할 리스트를 초기화합니다.\n",
    "    for data, targets in test_loader:  # 테스트 데이터로더에서 배치를 반복적으로 가져옵니다.\n",
    "        outputs = model(data)  # 모델을 사용하여 입력 데이터에 대한 예측값을 생성합니다.\n",
    "        all_targets.append(targets.numpy())  # 목표 값을 리스트에 추가합니다.\n",
    "        all_outputs.append(outputs.numpy())  # 예측 값을 리스트에 추가합니다.\n",
    "\n",
    "# 목표 값과 예측 값을 모두 단일 넘파이 배열로 연결(flatten)합니다.\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# 평균 절대 오차(MAE), 평균 제곱 오차(MSE) 및 제곱근 평균 제곱 오차(RMSE)를 계산합니다.\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# 계산된 지표들을 출력합니다. 이 값들은 모델의 성능을 평가하는 데 사용됩니다.\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.00025, 'final_learning_rate': 3.125e-05, 'batch_size': 256, 'max_epochs': 700, 'stop_epoch': 589, 'hidden_dim': 128, 'n_layers': 7}\n",
      "MinMaxScaler()\n",
      "{'MAE': 0.049040135, 'MSE': 0.005578987, 'RMSE': 0.0746926156640312}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    위에서 학습시킨 모델의 성능이 괜찮다면, 파라미터 정보를 pkl 파일로 저장하는 코드\n",
    "\"\"\"\n",
    "\n",
    "# 사용한 하이퍼파라미터들을 저장합니다.\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'final_learning_rate': final_learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'max_epochs': num_epochs,\n",
    "    'stop_epoch': stop_epoch,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'n_layers': n_layers\n",
    "}\n",
    "\n",
    "# 성능 지표를 저장합니다.\n",
    "metrics = {\n",
    "    'MAE': mae,\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "}\n",
    "\n",
    "# 데이터 정규화에 사용된 scaler를 저장합니다.\n",
    "scalers = dataset.scaler\n",
    "\n",
    "# 위에서 정의한 모든 결과를 하나의 사전에 합칩니다.\n",
    "results = {\n",
    "    'Hyperparameters': hyperparams,\n",
    "    'Scalers': scalers,\n",
    "    'Metrics': metrics,\n",
    "    'pretrained_model': pretrained_model_path\n",
    "}\n",
    "\n",
    "# 결합된 결과를 pickle 파일로 저장합니다.\n",
    "with open(results_folder + filename_metrics, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# 테스트를 위해\n",
    "# pickle 파일로부터 결과를 불러옵니다.\n",
    "with open(results_folder + filename_metrics, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "\n",
    "# 불러온 결과에서 데이터에 접근할 수 있습니다.\n",
    "print(loaded_results['Hyperparameters'])\n",
    "print(loaded_results['Scalers'])\n",
    "print(loaded_results['Metrics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1344]) torch.Size([1, 1344])\n",
      "Error for next 24 hours:\n",
      "        0_SV-2      1_SV-5    2_SV-6      3_SV-7  4_HV-NM1    5_HV-NM2  \\\n",
      "0    28.590002   57.030156 -0.004655   28.082966  0.005632   61.872286   \n",
      "1   -28.931686  -22.493971 -0.006997   -3.034284 -0.003102    7.657216   \n",
      "2    27.229530   22.845824 -0.005409   23.292702 -0.006497   13.728197   \n",
      "3    74.263209   48.991028  0.003815   53.948137 -0.001020   47.358293   \n",
      "4    14.826597   21.443776 -0.007123   41.459750 -0.000087  -41.187696   \n",
      "5    12.569521  -23.426898 -0.000068   11.201837  0.002354    4.895061   \n",
      "6    16.944242   -2.991486 -0.001131   -3.131108 -0.002833   12.697286   \n",
      "7   -40.606425  -41.293306  0.000279  -45.781147 -0.006382  -59.793625   \n",
      "8    26.040041    4.296743 -0.008384   24.138072 -0.000382   11.610111   \n",
      "9   -16.068718  -28.508329  0.003309  -16.705819 -0.002629  -22.717857   \n",
      "10    9.840866   -4.305109 -0.006392   -9.300329 -0.006678  -17.088406   \n",
      "11   89.726058   69.654368 -0.007028   78.584564 -0.001350   99.742813   \n",
      "12  -62.570146  -82.175317  0.005608  -77.933021  0.003800  -75.670902   \n",
      "13  -82.275810  -85.953618  0.005990  -69.457877  0.000644  -74.568729   \n",
      "14  -58.173270  -31.766663  0.000712  -44.899621  0.004506  -58.674081   \n",
      "15  -65.120622  -64.271329  0.006580  -75.290041 -0.005292  -70.495583   \n",
      "16   67.701900   55.406236  0.002236   45.506784 -0.005783   56.458198   \n",
      "17 -167.506927 -168.153618  0.004491 -137.816047 -0.003269 -108.103558   \n",
      "18  -39.669436  -51.789901  0.003147  -42.651909 -0.001498  -44.160133   \n",
      "19   45.263982   54.269787 -0.006067   16.448374  0.007681   62.028566   \n",
      "20   47.734481   24.484259  0.000921   20.520098 -0.001159   24.570128   \n",
      "21  -93.878801  -70.270018 -0.001400  -48.654156  0.008070  -14.946493   \n",
      "22  -14.755690    3.175085  0.000520  -63.252025  0.002052  -54.238410   \n",
      "23   49.276218   32.282516  0.009838   31.446423 -0.008623   -9.180618   \n",
      "\n",
      "      6_고압콘덴샤  7_신재생에너지동     8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동  47_기숙사9동  \\\n",
      "0   63.823282   0.596259  49.171421  -5.610715  ...    35.209989 -4.285389   \n",
      "1   56.673423 -44.923973  29.981278 -11.744041  ...    48.512746 -7.005969   \n",
      "2   64.531791  -6.380059  42.211148  -2.587946  ...    20.895738  1.903505   \n",
      "3  -18.273252   8.458643  22.805243   4.612753  ...    15.644369  0.420845   \n",
      "4  -86.515688 -23.987500  26.754144   8.770612  ...    -2.385900  9.002884   \n",
      "5  -14.205469 -14.829000  15.621140  -1.841567  ...    -1.256136  6.329414   \n",
      "6   37.745169 -18.833265   5.085850  10.923165  ...    14.024682 -2.622598   \n",
      "7    6.909881  -8.198530  25.532456  -5.168641  ...    19.086548  0.901320   \n",
      "8    1.617213 -11.233974   7.784396   1.049607  ...    45.468747 -3.109012   \n",
      "9   -2.328605  -4.233232  10.433654   3.012192  ...    36.606395 -5.491050   \n",
      "10  -4.622447 -35.455953  11.302322  13.109292  ...    31.212697 -1.444092   \n",
      "11 -32.600258 -22.790341   7.133203   3.046622  ...    14.665743  3.268577   \n",
      "12   4.040229 -47.216784   6.938715   5.183631  ...     1.524478 -3.314561   \n",
      "13  24.624357 -64.045606   6.256150  -6.253667  ...   -58.524886 -2.057883   \n",
      "14 -18.356395 -30.183728  34.891361  -2.400027  ...   -33.405932 -2.147240   \n",
      "15  13.270652 -30.299827  13.016044  -8.539455  ...   -60.487250  5.521169   \n",
      "16 -34.132619   4.297353   4.149745   4.325802  ...   -53.675914 -3.217240   \n",
      "17  -4.252066 -43.603234 -10.107960  -9.285174  ...   -83.608362 -1.121237   \n",
      "18 -17.461308 -68.614224  18.938637 -12.863311  ...   -47.326562  2.123335   \n",
      "19   4.255911 -14.152428  27.284722  -4.793208  ...   -33.809620 -6.173680   \n",
      "20  14.379552 -28.737088  10.642760  -4.809590  ...     7.081863 -2.192147   \n",
      "21  65.741457 -44.471058  11.569938  -4.712942  ...    -9.716191 -0.168530   \n",
      "22  36.566315  -5.964830  -3.278916   9.411360  ...   -37.245827 -2.295325   \n",
      "23  17.703636 -25.577964 -14.116956  13.204576  ...     0.864833  2.551479   \n",
      "\n",
      "    48_고등광/극초단  49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  \\\n",
      "0   -97.907339      0.595764         -5.528495    -5.198938     0.737791   \n",
      "1   -65.281776      9.750805          1.253945    -5.905262    -0.232484   \n",
      "2   -52.013511      8.265398         -4.509772    -1.536924    -5.727574   \n",
      "3   -57.404488     -0.718142         -0.299350     2.104083    -5.805589   \n",
      "4   -27.376045      1.988789          0.199429     7.771803    -2.795167   \n",
      "5   -37.777037      3.285380          0.411559     4.146762     0.271431   \n",
      "6    -6.541751     -2.411013         -2.375314     0.700533     4.232802   \n",
      "7    52.070856      5.935232         -1.672952     4.353858    -5.647072   \n",
      "8     4.943987      6.768452         -0.831470     2.586487    -0.811142   \n",
      "9     6.758641      4.020109          6.999872    -0.705554     0.486434   \n",
      "10   11.041834     -1.220927         -1.907675     5.639232    -0.139229   \n",
      "11  -40.625558     -2.738146          0.971274    13.330012     7.343145   \n",
      "12    3.673831     -0.027229          1.894354    -3.741207    -5.576053   \n",
      "13  -31.621827     -3.521659         -5.146499     6.165365   -10.761185   \n",
      "14   50.555913      6.129217         10.332670   -21.008259     1.495785   \n",
      "15    6.898142      1.939852         -2.114025    -9.349949     1.807604   \n",
      "16   -7.987913      4.094212          2.547226     6.123621     1.076385   \n",
      "17    6.730763      6.237253         -0.211720    -7.858322     0.973079   \n",
      "18  -15.448491      2.185801         -5.176352    -8.422586     1.390136   \n",
      "19   24.656753      0.228112         -5.181114     1.754346    -3.985521   \n",
      "20   50.538621      4.311149          6.163008    -8.753110    -1.072526   \n",
      "21   11.180041     -3.214267          0.904817   -18.313396     0.753992   \n",
      "22   42.795162     -2.252027          3.846539   -17.577265     0.626243   \n",
      "23   99.284998      1.377356          6.310762    -4.524477     1.661677   \n",
      "\n",
      "    53_LG도서관(E)  54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0     -7.346574    -5.069919     -4.147583  \n",
      "1     -2.188865     7.530814     -7.200457  \n",
      "2     -5.740299    -1.064789     -3.081160  \n",
      "3     -6.182848    -2.770953     -2.804526  \n",
      "4      0.748561    -2.932653     -0.132898  \n",
      "5     -2.193657     0.784960     -0.173860  \n",
      "6      1.632692    -0.797941      7.611244  \n",
      "7     -0.270897     7.314815     -1.056574  \n",
      "8     -1.944867    -1.176716      3.198480  \n",
      "9      1.114900    -3.042784      4.035220  \n",
      "10     3.261296    -7.428235      3.596904  \n",
      "11     1.708498     7.743987      6.602041  \n",
      "12    -1.646989     3.277683     -7.416804  \n",
      "13    -5.692962     1.873508      2.128159  \n",
      "14    -0.568378    10.905229     -3.468777  \n",
      "15    -4.624615    -8.548881      2.604173  \n",
      "16    -5.444055    11.943717      7.294540  \n",
      "17    -3.257455   -17.105321      1.256309  \n",
      "18    -3.997912   -30.318185      4.816599  \n",
      "19    13.052101   -11.140187     -0.855094  \n",
      "20     1.874388   -12.371154     -2.759969  \n",
      "21     3.641013    -4.196246      1.090927  \n",
      "22    15.726711   -15.604724      8.923478  \n",
      "23    -0.436745     9.569808      8.770505  \n",
      "\n",
      "[24 rows x 56 columns]\n",
      "MAE: 15.8779, MSE: 852.6268, RMSE: 29.1998 (no normalization)\n",
      "err_total:  -657.9251158329678\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    테스트 세트의 첫 번째 시퀀스에 대한 예측을 수행한 후, 예측된 값과 실제 목표값 사이의 차이를 계산하는 코드\n",
    "    이러한 차이를 기반으로 여러 성능 지표를 계산하며, 전체 에러를 출력하는 코드\n",
    "    *df는 원래의 데이터셋이라 가정하며 'date' 컬럼을 포함한다고 가정합니다.\n",
    "\"\"\"\n",
    "# 건물 이름을 가져옵니다.\n",
    "building_names = df.columns[-56:]  # 필요에 따라 이 값을 조절하세요.\n",
    "# 테스트 세트를 위한 DataLoader를 생성합니다.\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "# 테스트 세트에서 첫 번째 시퀀스와 그 목표값을 가져옵니다.\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# 모델을 평가 모드로 전환합니다.\n",
    "model.eval()\n",
    "\n",
    "# 예측을 수행합니다.\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "print(prediction.shape, real_target.shape)\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "# 패딩을 추가합니다.\n",
    "padding = np.zeros((prediction.shape[0], 7))\n",
    "prediction_pad = np.hstack((padding, prediction))\n",
    "real_target_pad = np.hstack((padding, real_target))\n",
    "# print(prediction_pad.shape, real_target_pad.shape)\n",
    "\n",
    "# 역변환을 적용하여 정규화를 해제합니다.\n",
    "prediction_inv = dataset.scaler.inverse_transform(prediction_pad)\n",
    "real_target_inv = dataset.scaler.inverse_transform(real_target_pad)\n",
    "\n",
    "# 처음 7개의 컬럼을 삭제합니다.\n",
    "prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "real_target_inv = np.delete(real_target_inv, np.s_[:7], axis=1)\n",
    "\n",
    "# 원래의 형태로 다시 변형합니다.\n",
    "prediction = prediction_inv.reshape(prediction.shape)\n",
    "real_target = real_target_inv.reshape(real_target.shape)\n",
    "\n",
    "# 에러(실제 목표값과 예측값의 차이)를 계산합니다.\n",
    "error = real_target - prediction\n",
    "\n",
    "# 예측값을 위한 DataFrame을 생성합니다.\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "err_total = total = error_df.values.flatten().sum()\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n",
    "\n",
    "# 성능 지표를 계산합니다.\n",
    "mae_n = mean_absolute_error(real_target, prediction)\n",
    "mse_n = mean_squared_error(real_target, prediction)\n",
    "rmse_n = sqrt(mse_n)\n",
    "print(f'MAE: {mae_n:.4f}, MSE: {mse_n:.4f}, RMSE: {rmse_n:.4f} (no normalization)')\n",
    "print(\"err_total: \", err_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    최적의 하이퍼파라미터를 찾기 위해 optuna 라이브러리를 이용해서 실험하는 코드\n",
    "    *실험 log는 따로 기록되지 않으므로 결과 복붙해서 log_optuna.txt로 따로 저장함\n",
    "\"\"\"\n",
    "# Optuna를 사용할 것인지 여부를 결정하는 플래그\n",
    "do_optuna = False\n",
    "\n",
    "# optuna 관련 라이브러리를 가져옵니다.\n",
    "import optuna\n",
    "import optuna.logging\n",
    "\n",
    "# Optuna의 기본 로깅 핸들러를 활성화합니다.\n",
    "optuna.logging.enable_default_handler()\n",
    "\n",
    "# 로깅의 상세 수준을 설정합니다.\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# Optuna를 사용하여 최적화할 목적 함수를 정의합니다.\n",
    "def objective(trial):\n",
    "    # Optuna를 사용하여 하이퍼파라미터를 추정합니다.\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 50, 300)\n",
    "    n_layers = trial.suggest_int('n_layers', 5, 9)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    \n",
    "    # 데이터 로더를 설정합니다.\n",
    "    if use_thread:\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "        test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    else:\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 모델, 손실 함수, 최적화 알고리즘을 초기화합니다.\n",
    "    model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\") # 초기에는 무한대로 설정합니다.\n",
    "    no_improve_epoch = 0\n",
    "\n",
    "    # 훈련 루프입니다.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # 순전파\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 모델을 검증합니다.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for data, targets in val_loader:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_losses.append(loss.item())\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            # 검증 손실이 줄어들면 모델을 저장합니다.\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                no_improve_epoch = 0\n",
    "            else:\n",
    "                no_improve_epoch += 1\n",
    "                \n",
    "            # 일찍 중단하기 위한 조건\n",
    "            if no_improve_epoch > patience:\n",
    "                print('Early stopping...')\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# do_optuna 플래그가 True로 설정되어 있으면 Optuna를 사용하여 하이퍼파라미터를 최적화합니다.\n",
    "if do_optuna:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=100, n_jobs=4)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\" Value: \", trial.value)\n",
    "\n",
    "    print(\" Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
