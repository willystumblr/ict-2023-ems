{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 실행 시\n",
    "- 3번째 셀의 Hyperparameter 값을 원하는대로 설정하기\n",
    "- 저장위치를 바꾸고 싶다면 results_folder 변수에 경로 설정하기\n",
    "- 자세한 설명은 각 코드 셀의 설명 및 주석 참고!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    모델 및 데이터셋 클래스 정의하는 코드\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Use\", device)\n",
    "\n",
    "# 시계열 데이터를 처리하는 클래스를 정의합니다.\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len  # 입력 시퀀스의 길이를 정의합니다.\n",
    "        self.pred_len = pred_len  # 예측할 시퀀스의 길이를 정의합니다.\n",
    "        self.scaler = MinMaxScaler()  # 데이터 정규화를 위한 MinMaxScaler 객체를 생성합니다.\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)  # 데이터 전처리 함수를 호출하여 dataframe을 전처리합니다.\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # 누락된 값을 시계열의 이전 값으로 채웁니다.\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # 숫자형 열을 [0, 1] 범위로 정규화합니다.\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # 범주형 변수를 원-핫 인코딩합니다.\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # 원래의 범주형 열을 삭제하고 인코딩된 열과 병합합니다.\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1  # 데이터셋의 전체 길이를 반환합니다.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len, :7]  # 입력 시퀀스의 앞 7열만 선택합니다.\n",
    "        # 마지막 56열이 전력 값이라고 가정하고 예측할 시퀀스를 선택합니다.\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] \n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # y 값을 평탄화하여 반환합니다.\n",
    "    \n",
    "# LSTM 모델을 정의합니다.\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # LSTM의 은닉층의 차원을 정의합니다.\n",
    "        self.n_layers = n_layers  # LSTM 층의 개수를 정의합니다.\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)  # LSTM 층을 정의합니다.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 완전 연결 층을 정의합니다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 은닉 상태와 셀 상태를 초기화합니다.\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM 층을 통해 데이터를 전달하고 출력을 얻습니다.\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 완전 연결 층을 통해 출력을 얻습니다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    데이터셋 읽고 set으로 분할하는 코드\n",
    "\"\"\"\n",
    "# pandas 라이브러리를 사용하여 엑셀 파일을 불러옵니다.\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data_KW.xlsx')\n",
    "\n",
    "# TimeSeriesDataset 클래스의 인스턴스를 생성합니다. 위에서 정의한 클래스를 사용하여 데이터를 전처리합니다.\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# 학습, 검증 및 테스트 데이터 세트의 크기를 정의합니다.\n",
    "train_size = int(0.7 * len(dataset))  # 전체 데이터의 70%를 학습 데이터로 사용\n",
    "val_size = int(0.2 * len(dataset))    # 전체 데이터의 20%를 검증 데이터로 사용\n",
    "test_size = len(dataset) - train_size - val_size  # 나머지 데이터를 테스트 데이터로 사용\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "# 전체 데이터셋을 학습, 검증 및 테스트 데이터 세트로 분할합니다.\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/700], Validation Loss: 0.1750, Save model\n",
      "Epoch [2/700], Validation Loss: 0.0994, Save model\n",
      "Epoch [3/700], Validation Loss: 0.0634, Save model\n",
      "Epoch [4/700], Validation Loss: 0.0462, Save model\n",
      "Epoch [5/700], Validation Loss: 0.0461, Save model\n",
      "Epoch [6/700], Validation Loss: 0.0396, Save model\n",
      "Epoch [7/700], Validation Loss: 0.0389, Save model\n",
      "Epoch [8/700], Validation Loss: 0.0401\n",
      "Epoch [9/700], Validation Loss: 0.0384, Save model\n",
      "Epoch [10/700], Validation Loss: 0.0375, Save model\n",
      "Epoch [11/700], Validation Loss: 0.0380\n",
      "Epoch [12/700], Validation Loss: 0.0381\n",
      "Epoch [13/700], Validation Loss: 0.0374, Save model\n",
      "Epoch [14/700], Validation Loss: 0.0375\n",
      "Epoch [15/700], Validation Loss: 0.0378\n",
      "Epoch [16/700], Validation Loss: 0.0374\n",
      "Epoch [17/700], Validation Loss: 0.0374, Save model\n",
      "Epoch [18/700], Validation Loss: 0.0376\n",
      "Epoch [19/700], Validation Loss: 0.0374\n",
      "Epoch [20/700], Validation Loss: 0.0374\n",
      "Epoch [21/700], Validation Loss: 0.0375\n",
      "Epoch [22/700], Validation Loss: 0.0374\n",
      "Epoch [23/700], Validation Loss: 0.0374\n",
      "Epoch [24/700], Validation Loss: 0.0375\n",
      "Epoch [25/700], Validation Loss: 0.0374\n",
      "Epoch [26/700], Validation Loss: 0.0374\n",
      "Epoch [27/700], Validation Loss: 0.0375\n",
      "Epoch [28/700], Validation Loss: 0.0374\n",
      "Epoch [29/700], Validation Loss: 0.0374\n",
      "Epoch [30/700], Validation Loss: 0.0374\n",
      "Epoch [31/700], Validation Loss: 0.0374\n",
      "Epoch [32/700], Validation Loss: 0.0374\n",
      "Epoch [33/700], Validation Loss: 0.0373, Save model\n",
      "Epoch [34/700], Validation Loss: 0.0372, Save model\n",
      "Epoch [35/700], Validation Loss: 0.0368, Save model\n",
      "Epoch [36/700], Validation Loss: 0.0357, Save model\n",
      "Epoch [37/700], Validation Loss: 0.0343, Save model\n",
      "Epoch [38/700], Validation Loss: 0.0334, Save model\n",
      "Epoch [39/700], Validation Loss: 0.0337\n",
      "Epoch [40/700], Validation Loss: 0.0340\n",
      "Epoch [41/700], Validation Loss: 0.0333, Save model\n",
      "Epoch [42/700], Validation Loss: 0.0330, Save model\n",
      "Epoch [43/700], Validation Loss: 0.0330\n",
      "Epoch [44/700], Validation Loss: 0.0331\n",
      "Epoch [45/700], Validation Loss: 0.0330, Save model\n",
      "Epoch [46/700], Validation Loss: 0.0329, Save model\n",
      "Epoch [47/700], Validation Loss: 0.0329, Save model\n",
      "Epoch [48/700], Validation Loss: 0.0328, Save model\n",
      "Epoch [49/700], Validation Loss: 0.0326, Save model\n",
      "Epoch [50/700], Validation Loss: 0.0324, Save model\n",
      "Epoch [51/700], Validation Loss: 0.0323, Save model\n",
      "Epoch [52/700], Validation Loss: 0.0322, Save model\n",
      "Epoch [53/700], Validation Loss: 0.0337\n",
      "Epoch [54/700], Validation Loss: 0.0317, Save model\n",
      "Epoch [55/700], Validation Loss: 0.0314, Save model\n",
      "Epoch [56/700], Validation Loss: 0.0311, Save model\n",
      "Epoch [57/700], Validation Loss: 0.0308, Save model\n",
      "Epoch [58/700], Validation Loss: 0.0304, Save model\n",
      "Epoch [59/700], Validation Loss: 0.0302, Save model\n",
      "Epoch [60/700], Validation Loss: 0.0301, Save model\n",
      "Epoch [61/700], Validation Loss: 0.0305\n",
      "Epoch [62/700], Validation Loss: 0.0302\n",
      "Epoch [63/700], Validation Loss: 0.0304\n",
      "Epoch [64/700], Validation Loss: 0.0301\n",
      "Epoch [65/700], Validation Loss: 0.0304\n",
      "Epoch [66/700], Validation Loss: 0.0300, Save model\n",
      "Epoch [67/700], Validation Loss: 0.0297, Save model\n",
      "Epoch [68/700], Validation Loss: 0.0297\n",
      "Epoch [69/700], Validation Loss: 0.0298\n",
      "Epoch [70/700], Validation Loss: 0.0296, Save model\n",
      "Epoch [71/700], Validation Loss: 0.0296, Save model\n",
      "Epoch [72/700], Validation Loss: 0.0296, Save model\n",
      "Epoch [73/700], Validation Loss: 0.0296, Save model\n",
      "Epoch [74/700], Validation Loss: 0.0298\n",
      "Epoch [75/700], Validation Loss: 0.0295, Save model\n",
      "Epoch [76/700], Validation Loss: 0.0295, Save model\n",
      "Epoch [77/700], Validation Loss: 0.0293, Save model\n",
      "Epoch [78/700], Validation Loss: 0.0290, Save model\n",
      "Epoch [79/700], Validation Loss: 0.0293\n",
      "Epoch [80/700], Validation Loss: 0.0310\n",
      "Epoch [81/700], Validation Loss: 0.0297\n",
      "Epoch [82/700], Validation Loss: 0.0267, Save model\n",
      "Epoch [83/700], Validation Loss: 0.0265, Save model\n",
      "Epoch [84/700], Validation Loss: 0.0213, Save model\n",
      "Epoch [85/700], Validation Loss: 0.0175, Save model\n",
      "Epoch [86/700], Validation Loss: 0.0173, Save model\n",
      "Epoch [87/700], Validation Loss: 0.0175\n",
      "Epoch [88/700], Validation Loss: 0.0162, Save model\n",
      "Epoch [89/700], Validation Loss: 0.0150, Save model\n",
      "Epoch [90/700], Validation Loss: 0.0158\n",
      "Epoch [91/700], Validation Loss: 0.0145, Save model\n",
      "Epoch [92/700], Validation Loss: 0.0148\n",
      "Epoch [93/700], Validation Loss: 0.0137, Save model\n",
      "Epoch [94/700], Validation Loss: 0.0133, Save model\n",
      "Epoch [95/700], Validation Loss: 0.0136\n",
      "Epoch [96/700], Validation Loss: 0.0134\n",
      "Epoch [97/700], Validation Loss: 0.0128, Save model\n",
      "Epoch [98/700], Validation Loss: 0.0129\n",
      "Epoch [99/700], Validation Loss: 0.0123, Save model\n",
      "Epoch [100/700], Validation Loss: 0.0122, Save model\n",
      "Epoch [101/700], Validation Loss: 0.0121, Save model\n",
      "Epoch [102/700], Validation Loss: 0.0120, Save model\n",
      "Epoch [103/700], Validation Loss: 0.0117, Save model\n",
      "Epoch [104/700], Validation Loss: 0.0116, Save model\n",
      "Epoch [105/700], Validation Loss: 0.0116, Save model\n",
      "Epoch [106/700], Validation Loss: 0.0115, Save model\n",
      "Epoch [107/700], Validation Loss: 0.0113, Save model\n",
      "Epoch [108/700], Validation Loss: 0.0113, Save model\n",
      "Epoch [109/700], Validation Loss: 0.0112, Save model\n",
      "Epoch [110/700], Validation Loss: 0.0112, Save model\n",
      "Epoch [111/700], Validation Loss: 0.0111, Save model\n",
      "Epoch [112/700], Validation Loss: 0.0110, Save model\n",
      "Epoch [113/700], Validation Loss: 0.0110, Save model\n",
      "Epoch [114/700], Validation Loss: 0.0109, Save model\n",
      "Epoch [115/700], Validation Loss: 0.0109, Save model\n",
      "Epoch [116/700], Validation Loss: 0.0108, Save model\n",
      "Epoch [117/700], Validation Loss: 0.0108, Save model\n",
      "Epoch [118/700], Validation Loss: 0.0107, Save model\n",
      "Epoch [119/700], Validation Loss: 0.0106, Save model\n",
      "Epoch [120/700], Validation Loss: 0.0106, Save model\n",
      "Epoch [121/700], Validation Loss: 0.0105, Save model\n",
      "Epoch [122/700], Validation Loss: 0.0104, Save model\n",
      "Epoch [123/700], Validation Loss: 0.0103, Save model\n",
      "Epoch [124/700], Validation Loss: 0.0102, Save model\n",
      "Epoch [125/700], Validation Loss: 0.0102, Save model\n",
      "Epoch [126/700], Validation Loss: 0.0101, Save model\n",
      "Epoch [127/700], Validation Loss: 0.0099, Save model\n",
      "Epoch [128/700], Validation Loss: 0.0097, Save model\n",
      "Epoch [129/700], Validation Loss: 0.0098\n",
      "Epoch [130/700], Validation Loss: 0.0097, Save model\n",
      "Epoch [131/700], Validation Loss: 0.0095, Save model\n",
      "Epoch [132/700], Validation Loss: 0.0093, Save model\n",
      "Epoch [133/700], Validation Loss: 0.0096\n",
      "Epoch [134/700], Validation Loss: 0.0096\n",
      "Epoch [135/700], Validation Loss: 0.0093, Save model\n",
      "Epoch [136/700], Validation Loss: 0.0091, Save model\n",
      "Epoch [137/700], Validation Loss: 0.0091, Save model\n",
      "Epoch [138/700], Validation Loss: 0.0089, Save model\n",
      "Epoch [139/700], Validation Loss: 0.0089, Save model\n",
      "Epoch [140/700], Validation Loss: 0.0088, Save model\n",
      "Epoch [141/700], Validation Loss: 0.0088, Save model\n",
      "Epoch [142/700], Validation Loss: 0.0087, Save model\n",
      "Epoch [143/700], Validation Loss: 0.0086, Save model\n",
      "Epoch [144/700], Validation Loss: 0.0086\n",
      "Epoch [145/700], Validation Loss: 0.0085, Save model\n",
      "Epoch [146/700], Validation Loss: 0.0085, Save model\n",
      "Epoch [147/700], Validation Loss: 0.0084, Save model\n",
      "Epoch [148/700], Validation Loss: 0.0084, Save model\n",
      "Epoch [149/700], Validation Loss: 0.0084\n",
      "Epoch [150/700], Validation Loss: 0.0084\n",
      "Epoch [151/700], Validation Loss: 0.0082, Save model\n",
      "Epoch [152/700], Validation Loss: 0.0083\n",
      "Epoch [153/700], Validation Loss: 0.0082, Save model\n",
      "Epoch [154/700], Validation Loss: 0.0082, Save model\n",
      "Epoch [155/700], Validation Loss: 0.0081, Save model\n",
      "Epoch [156/700], Validation Loss: 0.0081\n",
      "Epoch [157/700], Validation Loss: 0.0081, Save model\n",
      "Epoch [158/700], Validation Loss: 0.0080, Save model\n",
      "Epoch [159/700], Validation Loss: 0.0080, Save model\n",
      "Epoch [160/700], Validation Loss: 0.0080, Save model\n",
      "Epoch [161/700], Validation Loss: 0.0083\n",
      "Epoch [162/700], Validation Loss: 0.0083\n",
      "Epoch [163/700], Validation Loss: 0.0083\n",
      "Epoch [164/700], Validation Loss: 0.0082\n",
      "Epoch [165/700], Validation Loss: 0.0080\n",
      "Epoch [166/700], Validation Loss: 0.0082\n",
      "Epoch [167/700], Validation Loss: 0.0080, Save model\n",
      "Epoch [168/700], Validation Loss: 0.0079, Save model\n",
      "Epoch [169/700], Validation Loss: 0.0079, Save model\n",
      "Epoch [170/700], Validation Loss: 0.0079, Save model\n",
      "Epoch [171/700], Validation Loss: 0.0078, Save model\n",
      "Epoch [172/700], Validation Loss: 0.0079\n",
      "Epoch [173/700], Validation Loss: 0.0079\n",
      "Epoch [174/700], Validation Loss: 0.0078\n",
      "Epoch [175/700], Validation Loss: 0.0079\n",
      "Epoch [176/700], Validation Loss: 0.0084\n",
      "Epoch [177/700], Validation Loss: 0.0086\n",
      "Epoch [178/700], Validation Loss: 0.0084\n",
      "Epoch [179/700], Validation Loss: 0.0082\n",
      "Epoch [180/700], Validation Loss: 0.0080\n",
      "Epoch [181/700], Validation Loss: 0.0079\n",
      "Epoch [182/700], Validation Loss: 0.0079\n",
      "Epoch [183/700], Validation Loss: 0.0078, Save model\n",
      "Epoch [184/700], Validation Loss: 0.0079\n",
      "Epoch [185/700], Validation Loss: 0.0077, Save model\n",
      "Epoch [186/700], Validation Loss: 0.0077, Save model\n",
      "Epoch [187/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [188/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [189/700], Validation Loss: 0.0076\n",
      "Epoch [190/700], Validation Loss: 0.0078\n",
      "Epoch [191/700], Validation Loss: 0.0079\n",
      "Epoch [192/700], Validation Loss: 0.0078\n",
      "Epoch [193/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [194/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [195/700], Validation Loss: 0.0076\n",
      "Epoch [196/700], Validation Loss: 0.0076, Save model\n",
      "Epoch [197/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [198/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [199/700], Validation Loss: 0.0076\n",
      "Epoch [200/700], Validation Loss: 0.0076\n",
      "Epoch [201/700], Validation Loss: 0.0076\n",
      "Epoch [202/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [203/700], Validation Loss: 0.0075, Save model\n",
      "Epoch [204/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [205/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [206/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [207/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [208/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [209/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [210/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [211/700], Validation Loss: 0.0074\n",
      "Epoch [212/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [213/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [214/700], Validation Loss: 0.0074\n",
      "Epoch [215/700], Validation Loss: 0.0074\n",
      "Epoch [216/700], Validation Loss: 0.0074, Save model\n",
      "Epoch [217/700], Validation Loss: 0.0074\n",
      "Epoch [218/700], Validation Loss: 0.0074\n",
      "Epoch [219/700], Validation Loss: 0.0074\n",
      "Epoch [220/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [221/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [222/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [223/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [224/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [225/700], Validation Loss: 0.0073\n",
      "Epoch [226/700], Validation Loss: 0.0074\n",
      "Epoch [227/700], Validation Loss: 0.0074\n",
      "Epoch [228/700], Validation Loss: 0.0075\n",
      "Epoch [229/700], Validation Loss: 0.0075\n",
      "Epoch [230/700], Validation Loss: 0.0073\n",
      "Epoch [231/700], Validation Loss: 0.0074\n",
      "Epoch [232/700], Validation Loss: 0.0073\n",
      "Epoch [233/700], Validation Loss: 0.0074\n",
      "Epoch [234/700], Validation Loss: 0.0074\n",
      "Epoch [235/700], Validation Loss: 0.0073\n",
      "Epoch [236/700], Validation Loss: 0.0074\n",
      "Epoch [237/700], Validation Loss: 0.0073\n",
      "Epoch [238/700], Validation Loss: 0.0073, Save model\n",
      "Epoch [239/700], Validation Loss: 0.0073\n",
      "Epoch [240/700], Validation Loss: 0.0072, Save model\n",
      "Epoch [241/700], Validation Loss: 0.0072\n",
      "Epoch [242/700], Validation Loss: 0.0072\n",
      "Epoch [243/700], Validation Loss: 0.0072\n",
      "Epoch [244/700], Validation Loss: 0.0072\n",
      "Epoch [245/700], Validation Loss: 0.0072, Save model\n",
      "Epoch [246/700], Validation Loss: 0.0072, Save model\n",
      "Epoch [247/700], Validation Loss: 0.0072\n",
      "Epoch [248/700], Validation Loss: 0.0072\n",
      "Epoch [249/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [250/700], Validation Loss: 0.0071\n",
      "Epoch [251/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [252/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [253/700], Validation Loss: 0.0071\n",
      "Epoch [254/700], Validation Loss: 0.0071\n",
      "Epoch [255/700], Validation Loss: 0.0071\n",
      "Epoch [256/700], Validation Loss: 0.0072\n",
      "Epoch [257/700], Validation Loss: 0.0071\n",
      "Epoch [258/700], Validation Loss: 0.0071\n",
      "Epoch [259/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [260/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [261/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [262/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [263/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [264/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [265/700], Validation Loss: 0.0071\n",
      "Epoch [266/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [267/700], Validation Loss: 0.0071, Save model\n",
      "Epoch [268/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [269/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [270/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [271/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [272/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [273/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [274/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [275/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [276/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [277/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [278/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [279/700], Validation Loss: 0.0070\n",
      "Epoch [280/700], Validation Loss: 0.0070\n",
      "Epoch [281/700], Validation Loss: 0.0070\n",
      "Epoch [282/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [283/700], Validation Loss: 0.0071\n",
      "Epoch [284/700], Validation Loss: 0.0071\n",
      "Epoch [285/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [286/700], Validation Loss: 0.0070\n",
      "Epoch [287/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [288/700], Validation Loss: 0.0070\n",
      "Epoch [289/700], Validation Loss: 0.0070\n",
      "Epoch [290/700], Validation Loss: 0.0070\n",
      "Epoch [291/700], Validation Loss: 0.0070\n",
      "Epoch [292/700], Validation Loss: 0.0070\n",
      "Epoch [293/700], Validation Loss: 0.0070\n",
      "Epoch [294/700], Validation Loss: 0.0070\n",
      "Epoch [295/700], Validation Loss: 0.0070\n",
      "Epoch [296/700], Validation Loss: 0.0070\n",
      "Epoch [297/700], Validation Loss: 0.0070, Save model\n",
      "Epoch [298/700], Validation Loss: 0.0070\n",
      "Epoch [299/700], Validation Loss: 0.0113\n",
      "Epoch [300/700], Validation Loss: 0.0116\n",
      "Epoch [301/700], Validation Loss: 0.0097\n",
      "Epoch [302/700], Validation Loss: 0.0092\n",
      "Epoch [303/700], Validation Loss: 0.0093\n",
      "Epoch [304/700], Validation Loss: 0.0087\n",
      "Epoch [305/700], Validation Loss: 0.0082\n",
      "Epoch [306/700], Validation Loss: 0.0080\n",
      "Epoch [307/700], Validation Loss: 0.0082\n",
      "Epoch [308/700], Validation Loss: 0.0078\n",
      "Epoch [309/700], Validation Loss: 0.0076\n",
      "Epoch [310/700], Validation Loss: 0.0076\n",
      "Epoch [311/700], Validation Loss: 0.0075\n",
      "Epoch [312/700], Validation Loss: 0.0075\n",
      "Epoch [313/700], Validation Loss: 0.0076\n",
      "Epoch 00313: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [314/700], Validation Loss: 0.0075\n",
      "Epoch [315/700], Validation Loss: 0.0075\n",
      "Epoch [316/700], Validation Loss: 0.0075\n",
      "Epoch [317/700], Validation Loss: 0.0075\n",
      "Epoch [318/700], Validation Loss: 0.0075\n",
      "Epoch [319/700], Validation Loss: 0.0074\n",
      "Epoch [320/700], Validation Loss: 0.0074\n",
      "Epoch [321/700], Validation Loss: 0.0073\n",
      "Epoch [322/700], Validation Loss: 0.0073\n",
      "Epoch [323/700], Validation Loss: 0.0073\n",
      "Early stopping...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::mkldnn_rnn_layer' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::mkldnn_rnn_layer' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/build/aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradHIP: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradVE: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMTIA: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/TraceType_2.cpp:16726 [kernel]\nAutocastCPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/autocast_mode.cpp:492 [kernel]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m all_outputs \u001b[39m=\u001b[39m []  \u001b[39m# 모델의 예측 값을 저장할 리스트를 초기화합니다.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39mfor\u001b[39;00m data, targets \u001b[39min\u001b[39;00m test_loader:  \u001b[39m# 테스트 데이터로더에서 배치를 반복적으로 가져옵니다.\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     outputs \u001b[39m=\u001b[39m model(data)  \u001b[39m# 모델을 사용하여 입력 데이터에 대한 예측값을 생성합니다.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     all_targets\u001b[39m.\u001b[39mappend(targets\u001b[39m.\u001b[39mnumpy())  \u001b[39m# 목표 값을 리스트에 추가합니다.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(outputs\u001b[39m.\u001b[39mnumpy())  \u001b[39m# 예측 값을 리스트에 추가합니다.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/EMS/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[47], line 75\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     74\u001b[0m \u001b[39m# LSTM 층을 통해 데이터를 전달하고 출력을 얻습니다.\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[1;32m     76\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])  \u001b[39m# 완전 연결 층을 통해 출력을 얻습니다.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/EMS/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/EMS/lib/python3.11/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::mkldnn_rnn_layer' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::mkldnn_rnn_layer' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/build/aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradHIP: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradVE: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradMTIA: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/VariableType_2.cpp:17488 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/autograd/generated/TraceType_2.cpp:16726 [kernel]\nAutocastCPU: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/autocast_mode.cpp:492 [kernel]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    data loader, model 초기화하는 코드\n",
    "    epoch만큼 train 실행하는 코드\n",
    "    모델 저장하는 코드\n",
    "\"\"\"\n",
    "########################################## Hyperparameters ##########################################\n",
    "hidden_dim = 128\n",
    "n_layers = 9\n",
    "learning_rate = 0.001\n",
    "num_epochs = 700\n",
    "batch_size = 256\n",
    "use_thread = True # GPU 사용시 num_workers 사용 유무\n",
    "scheduler_factor = 0.5 # scheduler를 위한 파라미터\n",
    "scheduler_patience = 15 # scheduler_patience동안 val loss 감소하지 않으면 learning rate를 scheduler_factor배 수행함\n",
    "patience = 25 # number of epochs to wait before stopping(early stopping을 위한 파라미터로, scheduler_patience보다 높게 설정하기)\n",
    "pretrained_model_path = \"\"\n",
    "########################################################################################################\n",
    "input_dim = len(train_set[0][0][0]) # 7\n",
    "output_dim = 24*56\n",
    "stop_epoch = -1\n",
    "\n",
    "if pretrained_model_path != \"\":\n",
    "    assert os.path.exists(pretrained_model_path)\n",
    "    pretrained_params_path = pretrained_model_path.replace(\"model_\", \"\")\n",
    "    pretrained_params_path = pretrained_params_path.replace(\"pt\", \"pkl\")\n",
    "    assert os.path.exists(pretrained_params_path)\n",
    "\n",
    "\n",
    "# 결과를 저장할 폴더의 경로를 지정합니다.\n",
    "results_folder = \"/home/kimyirum/EMS/ict-2023-ems/load/results/\"\n",
    "# 현재의 시간 정보를 가져옵니다.\n",
    "now = datetime.now()\n",
    "# 현재 시간 정보를 문자열 포맷으로 변환합니다. (예: 20230807_143000)\n",
    "now_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "# 결과 메트릭을 저장할 파일의 이름을 지정합니다.\n",
    "filename_metrics = f'{now_str}.pkl'\n",
    "# 학습된 모델을 저장할 파일의 이름을 지정합니다.\n",
    "filename_model = f'model_{now_str}.pt'\n",
    "\n",
    "# DataLoader\n",
    "# use_thread 변수의 값에 따라 DataLoader의 num_workers 값을 설정합니다.\n",
    "if use_thread:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "else:\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# LSTM 모델, 손실 함수, 최적화 도구를 초기화합니다.\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
    "\n",
    "# 만약 pretrained 모델이 존재한다면, 해당 모델을 로드합니다.\n",
    "if os.path.exists(pretrained_model_path):\n",
    "    model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    print(\"Loaded pretrained model.\")\n",
    "\n",
    "    with open(pretrained_params_path, 'rb') as f:\n",
    "        loaded_results = pickle.load(f)\n",
    "        learning_rate = loaded_results['Hyperparameters']['final_learning_rate']\n",
    "        print(\"Use lr:\", learning_rate)\n",
    "\n",
    "criterion = nn.MSELoss()  # MSE 손실 함수를 사용합니다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam 최적화 도구를 사용합니다.\n",
    "# ReduceLROnPlateau 스케줄러는 검증 손실이 개선되지 않을 때 학습률을 동적으로 감소시킵니다. 학습률 감소의 타이밍을 검증 성능에 기반하여 자동으로 조절할 수 있습니다.\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience, verbose=True)\n",
    "\n",
    "# 초기 검증 손실을 무한대로 설정합니다.\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve_epoch = 0\n",
    "\n",
    "########################################## Training loop ##########################################\n",
    "# 주어진 에포크만큼 모델을 학습합니다.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # 순전파를 수행합니다.\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # 역전파 및 최적화를 수행합니다.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 모델을 검증합니다.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data, targets in val_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}', end='')\n",
    "        \n",
    "        # 검증 손실이 감소했을 경우, 모델을 저장합니다.\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\", Save model\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), results_folder+filename_model)\n",
    "            no_improve_epoch = 0\n",
    "        else:\n",
    "            print(\"\")\n",
    "            no_improve_epoch += 1\n",
    "            \n",
    "        # 조기 종료 조건: 검증 손실이 연속으로 patience동안 개선되지 않을 때 학습을 중단합니다.\n",
    "        if no_improve_epoch > patience:\n",
    "            print('Early stopping...')\n",
    "            stop_epoch = epoch\n",
    "            break\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "final_learning_rate = current_lr\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "# 모델을 평가 모드로 설정합니다. 이는 dropout, batch normalization 등의 레이어가 \n",
    "# 학습 모드와 다르게 작동해야 할 때 필요합니다.\n",
    "model.eval()\n",
    "\n",
    "# torch.no_grad()를 사용하여 autograd의 gradient 계산을 비활성화합니다. \n",
    "# 이렇게 하면 메모리 사용량을 줄이고 속도를 높일 수 있습니다.\n",
    "with torch.no_grad():\n",
    "    all_targets = []  # 실제 목표 값들을 저장할 리스트를 초기화합니다.\n",
    "    all_outputs = []  # 모델의 예측 값을 저장할 리스트를 초기화합니다.\n",
    "    for data, targets in test_loader:  # 테스트 데이터로더에서 배치를 반복적으로 가져옵니다.\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(data)  # 모델을 사용하여 입력 데이터에 대한 예측값을 생성합니다.\n",
    "        all_targets.append(targets.cpu().numpy())  # 목표 값을 리스트에 추가합니다.\n",
    "        all_outputs.append(outputs.cpu().numpy())  # 예측 값을 리스트에 추가합니다.\n",
    "\n",
    "# 목표 값과 예측 값을 모두 단일 넘파이 배열로 연결(flatten)합니다.\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# 평균 절대 오차(MAE), 평균 제곱 오차(MSE) 및 제곱근 평균 제곱 오차(RMSE)를 계산합니다.\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# 계산된 지표들을 출력합니다. 이 값들은 모델의 성능을 평가하는 데 사용됩니다.\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "########################################## Evaluate the model ##########################################\n",
    "# 모델을 평가 모드로 설정합니다. 이는 dropout, batch normalization 등의 레이어가 \n",
    "# 학습 모드와 다르게 작동해야 할 때 필요합니다.\n",
    "model.eval()\n",
    "\n",
    "# torch.no_grad()를 사용하여 autograd의 gradient 계산을 비활성화합니다. \n",
    "# 이렇게 하면 메모리 사용량을 줄이고 속도를 높일 수 있습니다.\n",
    "with torch.no_grad():\n",
    "    all_targets = []  # 실제 목표 값들을 저장할 리스트를 초기화합니다.\n",
    "    all_outputs = []  # 모델의 예측 값을 저장할 리스트를 초기화합니다.\n",
    "    for data, targets in test_loader:  # 테스트 데이터로더에서 배치를 반복적으로 가져옵니다.\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(data)  # 모델을 사용하여 입력 데이터에 대한 예측값을 생성합니다.\n",
    "        all_targets.append(targets.cpu().numpy())  # 목표 값을 리스트에 추가합니다.\n",
    "        all_outputs.append(outputs.cpu().numpy())  # 예측 값을 리스트에 추가합니다.\n",
    "\n",
    "# 목표 값과 예측 값을 모두 단일 넘파이 배열로 연결(flatten)합니다.\n",
    "all_targets = np.concatenate(all_targets).flatten()\n",
    "all_outputs = np.concatenate(all_outputs).flatten()\n",
    "\n",
    "# 평균 절대 오차(MAE), 평균 제곱 오차(MSE) 및 제곱근 평균 제곱 오차(RMSE)를 계산합니다.\n",
    "mae = mean_absolute_error(all_targets, all_outputs)\n",
    "mse = mean_squared_error(all_targets, all_outputs)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# 계산된 지표들을 출력합니다. 이 값들은 모델의 성능을 평가하는 데 사용됩니다.\n",
    "print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'final_learning_rate': 0.0005, 'batch_size': 256, 'max_epochs': 700, 'stop_epoch': 322, 'hidden_dim': 128, 'n_layers': 9}\n",
      "MinMaxScaler()\n",
      "{'MAE': 0.060754657, 'MSE': 0.008112408, 'RMSE': 0.09006890818017045}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    위에서 학습시킨 모델의 성능이 괜찮다면, 파라미터 정보를 pkl 파일로 저장하는 코드\n",
    "\"\"\"\n",
    "\n",
    "# 사용한 하이퍼파라미터들을 저장합니다.\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'final_learning_rate': final_learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'max_epochs': num_epochs,\n",
    "    'stop_epoch': stop_epoch,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'n_layers': n_layers\n",
    "}\n",
    "\n",
    "# 성능 지표를 저장합니다.\n",
    "metrics = {\n",
    "    'MAE': mae,\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "}\n",
    "\n",
    "# 데이터 정규화에 사용된 scaler를 저장합니다.\n",
    "scalers = dataset.scaler\n",
    "\n",
    "# 위에서 정의한 모든 결과를 하나의 사전에 합칩니다.\n",
    "results = {\n",
    "    'Hyperparameters': hyperparams,\n",
    "    'Scalers': scalers,\n",
    "    'Metrics': metrics,\n",
    "    'pretrained_model': pretrained_model_path\n",
    "}\n",
    "\n",
    "# 결합된 결과를 pickle 파일로 저장합니다.\n",
    "with open(results_folder + filename_metrics, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# 테스트를 위해\n",
    "# pickle 파일로부터 결과를 불러옵니다.\n",
    "with open(results_folder + filename_metrics, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "\n",
    "# 불러온 결과에서 데이터에 접근할 수 있습니다.\n",
    "print(loaded_results['Hyperparameters'])\n",
    "print(loaded_results['Scalers'])\n",
    "print(loaded_results['Metrics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1344]) torch.Size([1, 1344])\n",
      "Error for next 24 hours:\n",
      "        0_SV-2      1_SV-5    2_SV-6      3_SV-7  4_HV-NM1    5_HV-NM2  \\\n",
      "0    20.391929   17.543260  0.000073    9.676495  0.004194   21.822572   \n",
      "1    -9.761591  -12.528391  0.001520   12.183724  0.002105   -2.834793   \n",
      "2    45.795999   36.270949  0.002023   28.808264 -0.004255    2.041764   \n",
      "3    41.413799   17.822542  0.000523   17.941186  0.006419   14.736699   \n",
      "4   -32.503847  -15.699409  0.005083  -18.360154 -0.004477  -51.364214   \n",
      "5    -9.256451  -17.102705 -0.001751   -2.808558 -0.002834  -38.702027   \n",
      "6    11.818612    0.483312 -0.014157   -5.658206  0.004317   -9.956563   \n",
      "7   -46.365596  -36.736003  0.005815  -50.618493 -0.006418  -53.290366   \n",
      "8    -5.014524  -22.094463  0.004860  -14.632381 -0.003872  -37.637172   \n",
      "9   -39.138962  -51.955936 -0.000002  -49.364316  0.005382  -43.769431   \n",
      "10   -9.872230  -32.008643  0.002276  -36.619340 -0.000400   -9.253279   \n",
      "11   33.973881   18.530084  0.001789   20.593989 -0.001586   33.421767   \n",
      "12 -150.249608 -152.403502  0.001142 -167.891217  0.008949 -161.079476   \n",
      "13 -169.898246 -188.625963 -0.001543 -167.028411  0.002394 -179.375884   \n",
      "14 -180.758741 -154.302129 -0.006069 -151.074427 -0.002900 -182.998514   \n",
      "15 -182.521661 -159.999070  0.003185 -176.616192 -0.001944 -170.842353   \n",
      "16  -54.905727  -71.042226  0.001481  -45.822080 -0.006053  -42.878650   \n",
      "17 -242.995944 -232.078457 -0.006786 -177.914862 -0.005294 -203.081677   \n",
      "18 -126.523388 -127.726261 -0.007587 -121.569430 -0.002320 -105.494888   \n",
      "19  -31.332353  -41.862871  0.002031  -59.857300  0.000271  -10.905917   \n",
      "20  -47.062425  -49.104137 -0.001428  -46.508540  0.000409  -40.469657   \n",
      "21 -115.348974 -124.049004 -0.004436  -84.219251  0.003769  -46.312651   \n",
      "22  -38.743852    4.730411  0.002485  -52.085647  0.000065  -66.517484   \n",
      "23   47.610148   36.522589  0.002520   -1.942328 -0.003288   20.983484   \n",
      "\n",
      "       6_고압콘덴샤  7_신재생에너지동     8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동  47_기숙사9동  \\\n",
      "0   103.028882  13.977179  46.920077 -10.280885  ...    22.484638 -2.452624   \n",
      "1   148.776492 -14.844563  25.395675 -17.834994  ...    38.726315 -5.440988   \n",
      "2   162.588991  23.164343  38.810713 -13.583222  ...     3.173638  0.255360   \n",
      "3   105.497876   8.470605  19.448133  -5.443566  ...    -5.743974 -2.599712   \n",
      "4    10.789590 -33.841115  14.024276  -0.719576  ...   -14.579016  4.634383   \n",
      "5    31.367150 -21.912436   4.640655 -11.104226  ...   -17.707771  5.122781   \n",
      "6    45.451988 -27.136682   0.035600   4.791381  ...    -2.489874 -3.030446   \n",
      "7    44.001436 -29.296111  15.977213 -12.277952  ...    -1.473887 -1.564803   \n",
      "8    46.062295 -35.776946  -1.323385  -5.922815  ...    17.979502 -1.935812   \n",
      "9    43.627424 -21.672536  -2.936608  -8.023107  ...    24.236034 -2.002647   \n",
      "10   26.312271 -40.188038  -1.264282   0.105883  ...    17.998935  1.312129   \n",
      "11   18.790909 -30.693352  -5.994530 -10.322376  ...    -8.459867  1.672761   \n",
      "12   49.476037 -76.177229 -13.325462 -11.577495  ...    -2.175865 -4.356987   \n",
      "13   55.318830 -83.394776 -10.664971 -20.431755  ...   -37.563700 -1.568772   \n",
      "14    9.834535 -45.963906  14.534304 -18.850574  ...   -14.662459 -1.280079   \n",
      "15   34.827457 -44.197373  -3.778537 -20.379547  ...   -30.462772  4.363941   \n",
      "16  -20.222932  -8.468325 -13.927395  -4.746394  ...   -24.348392 -2.185040   \n",
      "17  -23.326315 -32.597288 -17.745079 -16.359277  ...   -48.351624  3.739176   \n",
      "18  -40.796754 -35.029372  12.125974 -15.821163  ...   -16.882048  3.918938   \n",
      "19  -23.980409 -11.468423  21.214564  -4.289159  ...    -4.332478 -5.553410   \n",
      "20  -31.513038 -15.626873  -3.998034  -6.827146  ...    37.964595  0.730811   \n",
      "21  -48.608611 -23.770264   8.229309  -3.864002  ...    24.839883  4.134224   \n",
      "22  -46.509251  25.332443   8.420012  14.286744  ...   -23.165962  0.115444   \n",
      "23  -17.410898   4.850725  -8.012683   9.832821  ...     1.507445  3.010012   \n",
      "\n",
      "    48_고등광/극초단  49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  \\\n",
      "0   -68.369135      1.272022         -1.582414   -12.670536     2.103458   \n",
      "1   -49.934580     10.111807          5.851576    -5.226330     1.873145   \n",
      "2   -26.051131      7.353592         -0.108193    -3.886167    -3.348361   \n",
      "3   -51.956610     -0.776376          4.363958    -4.510525    -3.925342   \n",
      "4   -22.241901      0.143370          2.571560    -2.882055    -0.292941   \n",
      "5   -30.470610      1.229221          2.774743    -5.742736     3.087859   \n",
      "6     7.318565     -2.663936          1.315513    -4.303623     4.758409   \n",
      "7    46.604466      4.593551          2.140070    -4.081281    -4.087360   \n",
      "8     5.616254      5.965057          1.968176    -6.335431     0.674073   \n",
      "9    22.662121      2.766823         10.230620    -6.017841     1.643275   \n",
      "10   28.193596     -1.912368          3.296658     0.673000     0.053281   \n",
      "11  -24.126684     -3.401228          4.008903     1.233839     7.971609   \n",
      "12    8.316888     -1.627009          2.858070   -10.001270    -5.806820   \n",
      "13  -30.567250     -5.582522         -4.584305    -1.135442    -9.131711   \n",
      "14   56.809185      4.183344          9.921416   -25.480991     1.573308   \n",
      "15    0.077851      0.499456         -0.297010    -7.658202     1.029882   \n",
      "16    6.321221      3.632264          4.745531    10.311224     1.536005   \n",
      "17   29.500017      5.698849          2.538342    -4.772043     1.321863   \n",
      "18   15.052731      3.215154         -0.915605     1.546093     1.472594   \n",
      "19   56.919367      2.145509         -2.263653     4.148959    -2.658494   \n",
      "20   68.516007      4.512231          7.557472    -5.270371     1.015152   \n",
      "21   30.380510     -2.297829          3.119409   -12.364232     1.154983   \n",
      "22   76.997151      0.732237          8.523598    -9.402060     1.826548   \n",
      "23  122.333734      2.994759         10.919148     1.789123     1.431087   \n",
      "\n",
      "    53_LG도서관(E)  54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0     -9.441000    -6.893680     -6.996831  \n",
      "1     -4.711999     3.992288     -7.544811  \n",
      "2     -7.419745    -4.390941     -4.522670  \n",
      "3     -8.020739   -10.784310     -4.581336  \n",
      "4     -1.721177   -13.954712     -1.301777  \n",
      "5     -2.435496    -4.523766     -0.919226  \n",
      "6     -1.308849    -1.688305      8.108624  \n",
      "7     -1.711728     0.449919     -1.226462  \n",
      "8      1.993109    -5.253731      1.375686  \n",
      "9      4.258442     2.390916      1.885813  \n",
      "10     3.436189    -4.098840      2.522670  \n",
      "11     0.472882     1.045703      2.680855  \n",
      "12    -4.472733    -0.151262    -12.425692  \n",
      "13    -9.487215    -0.824141     -6.991895  \n",
      "14    -7.875325     3.312438    -10.127407  \n",
      "15    -6.317183   -23.532738     -1.458099  \n",
      "16    -7.461882     2.059665      3.772263  \n",
      "17    -5.140240   -15.300358     -0.069746  \n",
      "18    -4.685145   -26.566528     -0.090636  \n",
      "19    10.008458    -9.520054     -3.262834  \n",
      "20    -0.565182    -6.658867     -5.770565  \n",
      "21     1.214694   -11.939048     -1.033315  \n",
      "22    12.186359   -24.147964      9.894441  \n",
      "23    -4.651996    -1.706039      9.482858  \n",
      "\n",
      "[24 rows x 56 columns]\n",
      "MAE: 22.2884, MSE: 1909.8616, RMSE: 43.7020 (no normalization)\n",
      "err_total:  -7646.429910937795\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    테스트 세트의 첫 번째 시퀀스에 대한 예측을 수행한 후, 예측된 값과 실제 목표값 사이의 차이를 계산하는 코드\n",
    "    이러한 차이를 기반으로 여러 성능 지표를 계산하며, 전체 에러를 출력하는 코드\n",
    "    *df는 원래의 데이터셋이라 가정하며 'date' 컬럼을 포함한다고 가정합니다.\n",
    "\"\"\"\n",
    "# 건물 이름을 가져옵니다.\n",
    "building_names = df.columns[-56:]  # 필요에 따라 이 값을 조절하세요.\n",
    "# 테스트 세트를 위한 DataLoader를 생성합니다.\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "# 테스트 세트에서 첫 번째 시퀀스와 그 목표값을 가져옵니다.\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# 모델을 평가 모드로 전환합니다.\n",
    "model = model.to('cpu')  \n",
    "model.eval()\n",
    "\n",
    "# 예측을 수행합니다.\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "print(prediction.shape, real_target.shape)\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "# 패딩을 추가합니다.\n",
    "padding = np.zeros((prediction.shape[0], 7))\n",
    "prediction_pad = np.hstack((padding, prediction))\n",
    "real_target_pad = np.hstack((padding, real_target))\n",
    "# print(prediction_pad.shape, real_target_pad.shape)\n",
    "\n",
    "# 역변환을 적용하여 정규화를 해제합니다.\n",
    "prediction_inv = dataset.scaler.inverse_transform(prediction_pad)\n",
    "real_target_inv = dataset.scaler.inverse_transform(real_target_pad)\n",
    "\n",
    "# 처음 7개의 컬럼을 삭제합니다.\n",
    "prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "real_target_inv = np.delete(real_target_inv, np.s_[:7], axis=1)\n",
    "\n",
    "# 원래의 형태로 다시 변형합니다.\n",
    "prediction = prediction_inv.reshape(prediction.shape)\n",
    "real_target = real_target_inv.reshape(real_target.shape)\n",
    "\n",
    "# 에러(실제 목표값과 예측값의 차이)를 계산합니다.\n",
    "error = real_target - prediction\n",
    "\n",
    "# 예측값을 위한 DataFrame을 생성합니다.\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "err_total = total = error_df.values.flatten().sum()\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n",
    "\n",
    "# 성능 지표를 계산합니다.\n",
    "mae_n = mean_absolute_error(real_target, prediction)\n",
    "mse_n = mean_squared_error(real_target, prediction)\n",
    "rmse_n = sqrt(mse_n)\n",
    "print(f'MAE: {mae_n:.4f}, MSE: {mse_n:.4f}, RMSE: {rmse_n:.4f} (no normalization)')\n",
    "print(\"err_total: \", err_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    최적의 하이퍼파라미터를 찾기 위해 optuna 라이브러리를 이용해서 실험하는 코드\n",
    "    *실험 log는 따로 기록되지 않으므로 결과 복붙해서 log_optuna.txt로 따로 저장함\n",
    "\"\"\"\n",
    "# Optuna를 사용할 것인지 여부를 결정하는 플래그\n",
    "do_optuna = False\n",
    "\n",
    "# optuna 관련 라이브러리를 가져옵니다.\n",
    "import optuna\n",
    "import optuna.logging\n",
    "\n",
    "# Optuna의 기본 로깅 핸들러를 활성화합니다.\n",
    "optuna.logging.enable_default_handler()\n",
    "\n",
    "# 로깅의 상세 수준을 설정합니다.\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# Optuna를 사용하여 최적화할 목적 함수를 정의합니다.\n",
    "def objective(trial):\n",
    "    # Optuna를 사용하여 하이퍼파라미터를 추정합니다.\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 50, 300)\n",
    "    n_layers = trial.suggest_int('n_layers', 5, 9)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    \n",
    "    # 데이터 로더를 설정합니다.\n",
    "    if use_thread:\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "        test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    else:\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 모델, 손실 함수, 최적화 알고리즘을 초기화합니다.\n",
    "    model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\") # 초기에는 무한대로 설정합니다.\n",
    "    no_improve_epoch = 0\n",
    "\n",
    "    # 훈련 루프입니다.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # 순전파\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 모델을 검증합니다.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for data, targets in val_loader:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_losses.append(loss.item())\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            # 검증 손실이 줄어들면 모델을 저장합니다.\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                no_improve_epoch = 0\n",
    "            else:\n",
    "                no_improve_epoch += 1\n",
    "                \n",
    "            # 일찍 중단하기 위한 조건\n",
    "            if no_improve_epoch > patience:\n",
    "                print('Early stopping...')\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# do_optuna 플래그가 True로 설정되어 있으면 Optuna를 사용하여 하이퍼파라미터를 최적화합니다.\n",
    "if do_optuna:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=100, n_jobs=4)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\" Value: \", trial.value)\n",
    "\n",
    "    print(\" Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
