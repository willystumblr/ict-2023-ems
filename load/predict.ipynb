{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class TimeSeriesDataset_forPredict(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24):\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # If there are any missing values, fill them with the previous value in time-series\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Normalize numerical columns to range [0, 1]\n",
    "        scaler = MinMaxScaler()\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # One-hot encode categorical variables\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # Drop original categorical columns and merge with encoded ones\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.dataframe) - self.seq_len + 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len]\n",
    "        return torch.Tensor(x.values)  # return only x values\n",
    "\n",
    "    \n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize hidden state\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) # Initialize cell state\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "learning_rate: 0.001\n",
      "batch_size: 128\n",
      "num_epochs: 200\n",
      "hidden_dim: 128\n",
      "n_layers: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(63, 128, num_layers=7, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1344, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_day = '0901'\n",
    "\n",
    "hyperparameters_filepath = '/home/kimyirum/EMS/ict-2023-ems/load/results/20230802_204633.pkl'\n",
    "model_filepath = '/home/kimyirum/EMS/ict-2023-ems/load/results/model_20230802_204633.pt'\n",
    "test_data = '/home/kimyirum/EMS/ict-2023-ems/load/data/test_for_'+predict_day+'.xlsx'\n",
    "\n",
    "df = pd.read_excel(test_data)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "hyperparameters = {}\n",
    "\n",
    "# # Read hyperparameters from txt file\n",
    "# with open(hyperparameters_filepath, 'r') as file:\n",
    "#     for line in file:\n",
    "#         line = line.strip()\n",
    "#         if line == '' or ':' not in line:\n",
    "#             continue\n",
    "#         key, value = line.split(':')\n",
    "#         value = value.strip()\n",
    "#         if value != '':\n",
    "#             hyperparameters[key.strip()] = float(value)\n",
    "\n",
    "# Load results from a pickle file\n",
    "with open(hyperparameters_filepath, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "    hyperparameters = loaded_results['Hyperparameters']\n",
    "    scalers = loaded_results['Scalers']\n",
    "\n",
    "# Print hyperparameters\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "# Initialize our dataset class\n",
    "dataset = TimeSeriesDataset_forPredict(df)\n",
    "test_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Recreate the model architecture\n",
    "model = LSTMModel(\n",
    "    input_dim=63,\n",
    "    hidden_dim=int(hyperparameters['hidden_dim']),\n",
    "    output_dim=24*56,\n",
    "    n_layers=int(hyperparameters['n_layers'])\n",
    ").to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(model_filepath))\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is the original dataset and it includes a 'date' column\n",
    "building_names = df.columns[-56:]  # adjust this as necessary\n",
    "\n",
    "# Prepare storage for predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterate over test set\n",
    "for sequence in test_loader:\n",
    "    # Move sequence to correct device\n",
    "    sequence = sequence.to(device)\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(sequence).cpu().numpy()\n",
    "\n",
    "    prediction_res = prediction.squeeze(0).reshape(24, 56)\n",
    "    padding = np.zeros((prediction_res.shape[0], 7))\n",
    "    prediction_pad = np.hstack((padding, prediction_res))\n",
    "    prediction_inv = scalers.inverse_transform(prediction_pad)\n",
    "    prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "    prediction = prediction_inv.reshape(prediction.shape)\n",
    "\n",
    "    # Store the prediction\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Combine all predictions\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Create a DataFrame for predictions\n",
    "# Reshape the predictions to align with the number of building_names\n",
    "predictions = predictions.reshape(-1, len(building_names))\n",
    "predictions_df = pd.DataFrame(predictions, columns=building_names)\n",
    "\n",
    "predictions_df['total(KW)'] = predictions_df.sum(axis=1)\n",
    "\n",
    "# Save to Excel file\n",
    "output_filepath = '/home/kimyirum/EMS/ict-2023-ems/load/predict_for_'+predict_day+'.xlsx'  # adjust this as necessary\n",
    "predictions_df.to_excel(output_filepath, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
