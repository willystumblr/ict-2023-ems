{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediciton Application\n",
    "- input: ----\n",
    "         merged_data 엑셀 파일과 같은 형태이어야 함(월|일|요일|공휴일 유무|온도|습도|건물이름 유효전력량*56개 건물).\n",
    "- output: 원하는 예측 날짜의 1시간 단위로 예측한 결과\n",
    "\n",
    "### 주의할 점\n",
    "- input 데이터 형태를 꼭 맞춰줘야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    모델 및 데이터셋 클래스 정의하는 코드\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 시계열 데이터를 처리하는 클래스를 정의합니다.\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len  # 입력 시퀀스의 길이를 정의합니다.\n",
    "        self.pred_len = pred_len  # 예측할 시퀀스의 길이를 정의합니다.\n",
    "        self.scaler = MinMaxScaler()  # 데이터 정규화를 위한 MinMaxScaler 객체를 생성합니다.\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)  # 데이터 전처리 함수를 호출하여 dataframe을 전처리합니다.\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # 누락된 값을 시계열의 이전 값으로 채웁니다.\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # 숫자형 열을 [0, 1] 범위로 정규화합니다.\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # 범주형 변수를 원-핫 인코딩합니다.\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # 원래의 범주형 열을 삭제하고 인코딩된 열과 병합합니다.\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1  # 데이터셋의 전체 길이를 반환합니다.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len, :7]  # 입력 시퀀스의 앞 7열만 선택합니다.\n",
    "        # 마지막 56열이 전력 값이라고 가정하고 예측할 시퀀스를 선택합니다.\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] \n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # y 값을 평탄화하여 반환합니다.\n",
    "    \n",
    "# LSTM 모델을 정의합니다.\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # LSTM의 은닉층의 차원을 정의합니다.\n",
    "        self.n_layers = n_layers  # LSTM 층의 개수를 정의합니다.\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)  # LSTM 층을 정의합니다.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 완전 연결 층을 정의합니다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 은닉 상태와 셀 상태를 초기화합니다.\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM 층을 통해 데이터를 전달하고 출력을 얻습니다.\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 완전 연결 층을 통해 출력을 얻습니다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.003\n",
      "final_learning_rate: 9.375e-05\n",
      "batch_size: 128\n",
      "max_epochs: 700\n",
      "stop_epoch: 685\n",
      "hidden_dim: 128\n",
      "n_layers: 5\n",
      "MinMaxScaler()\n",
      "{'MAE': 0.04864518, 'MSE': 0.00555969, 'RMSE': 0.0745633276550118}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(7, 128, num_layers=5, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1344, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    pre-trained 모델 불러오는 코드\n",
    "\"\"\"\n",
    "path = '/home/kimyirum/EMS/ict-2023-ems/load/best_results/'\n",
    "name = '20230807_165319'\n",
    "hyperparameters_filepath = path + name + '.pkl'\n",
    "model_filepath = path + 'model_' + name + '.pt'\n",
    "hyperparameters = {}\n",
    "\n",
    "# Load results from a pickle file\n",
    "with open(hyperparameters_filepath, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "    hyperparameters = loaded_results['Hyperparameters']\n",
    "    scalers = loaded_results['Scalers']\n",
    "\n",
    "# Print hyperparameters\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f'{key}: {value}')\n",
    "print(loaded_results['Scalers'])\n",
    "print(loaded_results['Metrics'])    \n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "# Recreate the model architecture\n",
    "model = LSTMModel(\n",
    "    input_dim=7,\n",
    "    hidden_dim=int(hyperparameters['hidden_dim']),\n",
    "    output_dim=24*56,\n",
    "    n_layers=int(hyperparameters['n_layers'])\n",
    ").to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(model_filepath))\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    데이터셋 읽고 set으로 분할하는 코드\n",
    "\"\"\"\n",
    "\n",
    "# pandas 라이브러리를 사용하여 엑셀 파일을 불러옵니다.\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data_KW.xlsx')\n",
    "\n",
    "# TimeSeriesDataset 클래스의 인스턴스를 생성합니다. 위에서 정의한 클래스를 사용하여 데이터를 전처리합니다.\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# 학습, 검증 및 테스트 데이터 세트의 크기를 정의합니다.\n",
    "train_size = int(0.7 * len(dataset))  # 전체 데이터의 70%를 학습 데이터로 사용\n",
    "val_size = int(0.2 * len(dataset))    # 전체 데이터의 20%를 검증 데이터로 사용\n",
    "test_size = len(dataset) - train_size - val_size  # 나머지 데이터를 테스트 데이터로 사용\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "# 전체 데이터셋을 학습, 검증 및 테스트 데이터 세트로 분할합니다.\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1344]) torch.Size([1, 1344])\n",
      "Error for next 24 hours:\n",
      "        0_SV-2      1_SV-5    2_SV-6      3_SV-7  4_HV-NM1    5_HV-NM2  \\\n",
      "0    73.952796   62.423318 -0.000701   62.222245  0.008185   59.593039   \n",
      "1    -3.412818  -19.543162  0.003742    6.848563 -0.002815   22.246242   \n",
      "2    46.290809   28.060933 -0.000427   14.219180 -0.002912   25.329566   \n",
      "3    69.812954   46.388743 -0.003796   57.512630  0.006085   50.485635   \n",
      "4   -13.947147   42.922173  0.000036   19.017509 -0.002098  -24.012230   \n",
      "5   -11.259251   34.817916  0.002646    9.930516 -0.003195   -4.697957   \n",
      "6    11.109770   11.409646  0.002949  -13.451486  0.000483   16.136036   \n",
      "7   -34.139768  -34.799935 -0.000946  -36.171019 -0.002175  -18.633064   \n",
      "8    51.383837   32.490845 -0.006250   -9.796589 -0.001256   -4.125328   \n",
      "9    -9.722539   -8.084103 -0.000852  -27.154559 -0.012406  -14.368761   \n",
      "10   19.569574  -17.203056  0.008084   11.519160  0.003367   11.806749   \n",
      "11  119.171321   92.621760  0.013518  109.176786  0.003794  126.214022   \n",
      "12  -32.566154  -28.607340 -0.000620  -66.874279  0.008210  -16.165588   \n",
      "13  -78.513877  -74.925038  0.009452  -59.973903  0.002015  -88.255348   \n",
      "14  -53.099804    2.038637  0.005903  -11.283252 -0.002593  -45.513588   \n",
      "15  -10.677266  -45.136413 -0.006606  -39.926253  0.001218  -25.686367   \n",
      "16   93.570958   71.626397  0.004576   81.046107 -0.001766   79.983882   \n",
      "17 -158.353823 -145.129187  0.003756 -115.043642 -0.008091 -124.910952   \n",
      "18  -40.728235  -76.724653 -0.001681  -77.248768  0.001966  -34.284904   \n",
      "19   21.636387   11.849184 -0.003426   29.378752  0.014050   36.109332   \n",
      "20   46.574626   20.959262 -0.007569    8.989674  0.010845   40.775030   \n",
      "21 -112.632985  -87.513195 -0.008758  -42.882051 -0.013295  -15.073044   \n",
      "22   -6.575098   23.826407  0.014441  -25.895178 -0.005983  -68.553569   \n",
      "23   84.551162   91.837395  0.000853   43.376847  0.003663   51.055824   \n",
      "\n",
      "       6_고압콘덴샤  7_신재생에너지동     8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동  47_기숙사9동  \\\n",
      "0   118.016284  18.309438  45.742801  -9.618397  ...    13.287843 -4.717107   \n",
      "1    93.431550 -25.219024  15.273761 -12.636309  ...    37.139998 -7.282924   \n",
      "2     4.125615  20.385322  32.507470  -7.497960  ...     9.019004  0.439553   \n",
      "3   -28.905419  28.374088  12.137035   5.472819  ...   -16.038476  0.901713   \n",
      "4   -86.906468 -15.376711   6.390083  11.908869  ...    -9.136831  8.803039   \n",
      "5   -15.637331 -12.302461   0.981121   3.169687  ...   -13.687502  5.328345   \n",
      "6   -16.978841 -17.199894  -0.148530  16.344846  ...    -0.148781 -1.528893   \n",
      "7   -76.166219  -8.839927   6.787714  -1.601484  ...     5.370135  3.010162   \n",
      "8   -79.292137 -10.659248  -2.298099   5.303432  ...    33.835755  0.706268   \n",
      "9   -43.626570  -4.717293   1.217759   1.381115  ...    24.278820 -3.998132   \n",
      "10    5.786883 -11.210470  -2.996520  12.134791  ...    23.269870  0.660696   \n",
      "11    6.760415  20.443772  -5.307736   4.309592  ...    15.974798  7.076824   \n",
      "12   40.499958 -20.837427   2.802725   5.124519  ...    24.426469 -0.977639   \n",
      "13   80.896854 -48.193076  -1.400553  -7.304962  ...   -48.884145 -1.553008   \n",
      "14    0.151228   1.858668  35.346936  -3.610671  ...   -20.552001  2.431329   \n",
      "15   15.130727  -0.868904  21.503776  -3.458285  ...   -22.044014  8.258818   \n",
      "16  -14.274514  37.269143  12.879252   9.010962  ...   -21.902419 -2.640444   \n",
      "17   -1.862585   0.274806 -10.290859  -6.930430  ...   -68.796129  0.870021   \n",
      "18  -46.013007 -48.516970  12.823391 -10.789920  ...   -37.873812  4.185626   \n",
      "19    0.812531  -0.360700  35.922901  -2.678021  ...    -9.271383 -5.699448   \n",
      "20   24.484076  15.079302   9.163669  -3.300670  ...    20.233499 -2.663220   \n",
      "21   -6.852636  -9.546144   4.995149  -2.236723  ...     5.839724  2.845243   \n",
      "22   27.718982  30.233296   6.056126  11.645790  ...   -29.405048  0.012209   \n",
      "23   24.250665  21.131906  -3.114658   6.205423  ...     7.030058  1.835237   \n",
      "\n",
      "    48_고등광/극초단  49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  \\\n",
      "0   -72.626833     -0.288938         -7.163793    -1.839491     2.273610   \n",
      "1   -52.330791      8.558370          0.689746     1.193437     0.293442   \n",
      "2    -9.197565      7.757677         -3.238366     2.723119    -4.106893   \n",
      "3   -20.278472     -0.382739         -0.774270     5.569703    -5.939497   \n",
      "4     7.925377      0.381436          0.839297     4.653239    -2.625365   \n",
      "5    -3.437968      1.362446          0.635283     0.769960     0.350302   \n",
      "6    20.676837     -2.116900         -1.423948     2.510053     4.694682   \n",
      "7    58.114406      5.916247         -0.675662     4.658941    -5.191143   \n",
      "8    24.466374      7.814180         -0.665447    -1.088262    -1.037939   \n",
      "9     6.256154      4.604398         10.074699    -1.347132     1.944627   \n",
      "10   10.033719     -0.678868         -0.552151     8.538280     0.644167   \n",
      "11  -41.859615     -2.655635          2.977883     8.938320     8.807482   \n",
      "12    7.377868      1.815106          3.781039    -0.567527    -3.519704   \n",
      "13  -44.688258     -3.159225         -5.448640     5.781045    -6.159588   \n",
      "14   30.796444      5.065442         10.529824   -13.658580     3.391998   \n",
      "15   -6.480360      2.779872          1.175245    -2.285519     2.739122   \n",
      "16  -16.639170      4.217150          5.368300     8.628755     2.419538   \n",
      "17  -18.872695      5.996179          2.147305     1.370771     0.797116   \n",
      "18  -41.079587      1.631018         -3.089510    -7.883540    -0.201678   \n",
      "19    4.895215      1.383085         -5.382826     1.001042    -4.872513   \n",
      "20   37.333373      5.088779          6.869861     1.612332    -1.536119   \n",
      "21   -9.494431     -3.899652          1.541694    -6.199364     0.067254   \n",
      "22   25.159591     -0.861519          4.253174    -9.552023    -0.400898   \n",
      "23  109.859113      3.285074         10.394406     1.498880     1.386030   \n",
      "\n",
      "    53_LG도서관(E)  54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0    -10.108862    11.077238     -1.652060  \n",
      "1     -5.245667     4.977122     -6.408796  \n",
      "2     -3.747330    -1.580914     -3.155389  \n",
      "3     -9.622886     0.966376     -1.332222  \n",
      "4     -4.721733    -1.312336      1.140296  \n",
      "5     -1.320786    -0.705529     -0.627291  \n",
      "6      1.543721    -2.511092      9.179762  \n",
      "7      0.626398     7.090785      1.082840  \n",
      "8     -1.121282     5.728982      3.051057  \n",
      "9      0.347516    -4.505984      2.272326  \n",
      "10     3.311506     3.859509      3.658916  \n",
      "11     0.730319    14.146735      3.869495  \n",
      "12    -4.743023     1.575103     -6.774142  \n",
      "13    -5.475163     2.903327      2.620330  \n",
      "14    -1.594699    24.484480     -5.911762  \n",
      "15    -5.166534    -0.522978      3.054172  \n",
      "16    -5.563317     4.856940      7.573925  \n",
      "17    -4.148908   -15.597645      3.777631  \n",
      "18    -3.176407   -20.078075     -0.814569  \n",
      "19    13.893636   -10.685935     -3.702281  \n",
      "20     5.507186   -13.326238     -3.045427  \n",
      "21     1.885981    -5.762068     -0.750660  \n",
      "22    15.982358   -21.832722      6.538400  \n",
      "23    -1.836178    -3.542485      7.800658  \n",
      "\n",
      "[24 rows x 56 columns]\n",
      "MAE: 14.9379, MSE: 797.8314, RMSE: 28.2459 (no normalization)\n",
      "err_total:  1692.4654831623907\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    테스트 세트의 첫 번째 시퀀스에 대한 예측을 수행한 후, 예측된 값과 실제 목표값 사이의 차이를 계산하는 코드\n",
    "    이러한 차이를 기반으로 여러 성능 지표를 계산하며, 전체 에러를 출력하는 코드\n",
    "    *df는 원래의 데이터셋이라 가정하며 'date' 컬럼을 포함한다고 가정합니다.\n",
    "\"\"\"\n",
    "# 건물 이름을 가져옵니다.\n",
    "building_names = df.columns[-56:]  # 필요에 따라 이 값을 조절하세요.\n",
    "# 테스트 세트를 위한 DataLoader를 생성합니다.\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "# 테스트 세트에서 첫 번째 시퀀스와 그 목표값을 가져옵니다.\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# 모델을 평가 모드로 전환합니다.\n",
    "model.eval()\n",
    "\n",
    "# 예측을 수행합니다.\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "print(prediction.shape, real_target.shape)\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "# 패딩을 추가합니다.\n",
    "padding = np.zeros((prediction.shape[0], 7))\n",
    "prediction_pad = np.hstack((padding, prediction))\n",
    "real_target_pad = np.hstack((padding, real_target))\n",
    "# print(prediction_pad.shape, real_target_pad.shape)\n",
    "\n",
    "# 역변환을 적용하여 정규화를 해제합니다.\n",
    "prediction_inv = dataset.scaler.inverse_transform(prediction_pad)\n",
    "real_target_inv = dataset.scaler.inverse_transform(real_target_pad)\n",
    "\n",
    "# 처음 7개의 컬럼을 삭제합니다.\n",
    "prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "real_target_inv = np.delete(real_target_inv, np.s_[:7], axis=1)\n",
    "\n",
    "# 원래의 형태로 다시 변형합니다.\n",
    "prediction = prediction_inv.reshape(prediction.shape)\n",
    "real_target = real_target_inv.reshape(real_target.shape)\n",
    "\n",
    "# 에러(실제 목표값과 예측값의 차이)를 계산합니다.\n",
    "error = real_target - prediction\n",
    "\n",
    "# 예측값을 위한 DataFrame을 생성합니다.\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "err_total = total = error_df.values.flatten().sum()\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n",
    "\n",
    "# 성능 지표를 계산합니다.\n",
    "mae_n = mean_absolute_error(real_target, prediction)\n",
    "mse_n = mean_squared_error(real_target, prediction)\n",
    "rmse_n = sqrt(mse_n)\n",
    "print(f'MAE: {mae_n:.4f}, MSE: {mse_n:.4f}, RMSE: {rmse_n:.4f} (no normalization)')\n",
    "print(\"err_total: \", err_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
