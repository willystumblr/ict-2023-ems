{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediciton Application\n",
    "- input: ----\n",
    "         merged_data 엑셀 파일과 같은 형태이어야 함(월|일|요일|공휴일 유무|온도|습도|건물이름 유효전력량*56개 건물).\n",
    "- output: 원하는 예측 날짜의 1시간 단위로 예측한 결과\n",
    "\n",
    "### 주의할 점\n",
    "- input 데이터 형태를 꼭 맞춰줘야함. 해당 날짜의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    모델 및 데이터셋 클래스 정의하는 코드\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 시계열 데이터를 처리하는 클래스를 정의합니다.\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len  # 입력 시퀀스의 길이를 정의합니다.\n",
    "        self.pred_len = pred_len  # 예측할 시퀀스의 길이를 정의합니다.\n",
    "        self.scaler = MinMaxScaler()  # 데이터 정규화를 위한 MinMaxScaler 객체를 생성합니다.\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)  # 데이터 전처리 함수를 호출하여 dataframe을 전처리합니다.\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # 누락된 값을 시계열의 이전 값으로 채웁니다.\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # 숫자형 열을 [0, 1] 범위로 정규화합니다.\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # 범주형 변수를 원-핫 인코딩합니다.\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # 원래의 범주형 열을 삭제하고 인코딩된 열과 병합합니다.\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1  # 데이터셋의 전체 길이를 반환합니다.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len, :7]  # 입력 시퀀스의 앞 7열만 선택합니다.\n",
    "        # 마지막 56열이 전력 값이라고 가정하고 예측할 시퀀스를 선택합니다.\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] \n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # y 값을 평탄화하여 반환합니다.\n",
    "    \n",
    "# LSTM 모델을 정의합니다.\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # LSTM의 은닉층의 차원을 정의합니다.\n",
    "        self.n_layers = n_layers  # LSTM 층의 개수를 정의합니다.\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)  # LSTM 층을 정의합니다.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 완전 연결 층을 정의합니다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 은닉 상태와 셀 상태를 초기화합니다.\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM 층을 통해 데이터를 전달하고 출력을 얻습니다.\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 완전 연결 층을 통해 출력을 얻습니다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.001\n",
      "final_learning_rate: 0.00025\n",
      "batch_size: 256\n",
      "max_epochs: 700\n",
      "stop_epoch: -1\n",
      "hidden_dim: 128\n",
      "n_layers: 7\n",
      "MinMaxScaler()\n",
      "{'MAE': 0.050929334, 'MSE': 0.0059092115, 'RMSE': 0.07687139564201717}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(7, 128, num_layers=7, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1344, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    pre-trained 모델 불러오는 코드\n",
    "\"\"\"\n",
    "path = '/home/kimyirum/EMS/ict-2023-ems/load/results/'\n",
    "name = '20230807_180431'\n",
    "hyperparameters_filepath = path + name + '.pkl'\n",
    "model_filepath = path + 'model_' + name + '.pt'\n",
    "hyperparameters = {}\n",
    "\n",
    "# Load results from a pickle file\n",
    "with open(hyperparameters_filepath, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "    hyperparameters = loaded_results['Hyperparameters']\n",
    "    scalers = loaded_results['Scalers']\n",
    "\n",
    "# Print hyperparameters\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f'{key}: {value}')\n",
    "print(loaded_results['Scalers'])\n",
    "print(loaded_results['Metrics'])    \n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "# Recreate the model architecture\n",
    "model = LSTMModel(\n",
    "    input_dim=7,\n",
    "    hidden_dim=int(hyperparameters['hidden_dim']),\n",
    "    output_dim=24*56,\n",
    "    n_layers=int(hyperparameters['n_layers'])\n",
    ").to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(model_filepath))\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    데이터셋 읽고 set으로 분할하는 코드\n",
    "\"\"\"\n",
    "\n",
    "# pandas 라이브러리를 사용하여 엑셀 파일을 불러옵니다.\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data_KW.xlsx')\n",
    "\n",
    "# TimeSeriesDataset 클래스의 인스턴스를 생성합니다. 위에서 정의한 클래스를 사용하여 데이터를 전처리합니다.\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# 학습, 검증 및 테스트 데이터 세트의 크기를 정의합니다.\n",
    "train_size = int(0.7 * len(dataset))  # 전체 데이터의 70%를 학습 데이터로 사용\n",
    "val_size = int(0.2 * len(dataset))    # 전체 데이터의 20%를 검증 데이터로 사용\n",
    "test_size = len(dataset) - train_size - val_size  # 나머지 데이터를 테스트 데이터로 사용\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "# 전체 데이터셋을 학습, 검증 및 테스트 데이터 세트로 분할합니다.\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1344]) torch.Size([1, 1344])\n",
      "Error for next 24 hours:\n",
      "        0_SV-2      1_SV-5    2_SV-6      3_SV-7  4_HV-NM1    5_HV-NM2  \\\n",
      "0    23.338168   50.325544 -0.004405   11.368135  0.013050   48.949098   \n",
      "1   -16.622283  -16.869600 -0.002139   -0.776390 -0.005698    4.593105   \n",
      "2    20.585534   24.451163 -0.003496   19.881847 -0.006384   24.247361   \n",
      "3    59.875706   34.698752  0.007004   47.859519 -0.001406   48.698881   \n",
      "4    16.420311   17.805486 -0.000428   28.776995 -0.005714  -57.248253   \n",
      "5     9.816373  -21.714876 -0.002289    8.431441  0.006672   -9.830856   \n",
      "6    15.940458    1.993662 -0.007823   -9.981788 -0.009973    9.921731   \n",
      "7   -36.617046   -6.425423 -0.006662  -28.595400 -0.006599  -47.928327   \n",
      "8    34.775693   -8.880116 -0.007594   17.284823 -0.003296   10.206027   \n",
      "9   -14.095343  -17.397425 -0.000225    1.518854 -0.001212   -4.637233   \n",
      "10    7.171269   -5.730384 -0.011504  -20.087276 -0.006186    6.029848   \n",
      "11   80.213928   63.787278 -0.004630   53.173413  0.009033   85.939250   \n",
      "12  -82.603424  -81.790738 -0.001115  -83.187938  0.002628  -76.267759   \n",
      "13 -113.671965  -97.789721  0.013545  -81.760006 -0.008771  -80.532639   \n",
      "14  -88.566390  -54.819013 -0.001242  -66.914919 -0.000924  -60.109537   \n",
      "15  -54.819161  -63.370250  0.010928  -76.515901 -0.002698  -60.720105   \n",
      "16   38.418528   53.777045  0.009341   53.904253 -0.007581   51.115639   \n",
      "17 -164.659126 -167.913996  0.002954 -134.567481 -0.008823 -123.886830   \n",
      "18  -30.717883  -50.473089  0.004549  -40.647135  0.004837  -49.300441   \n",
      "19   52.220588   64.130256 -0.009319   25.374610  0.000419   55.548114   \n",
      "20   72.188380   33.373019 -0.007419   31.988797  0.019686   68.383444   \n",
      "21  -62.027986  -15.073914  0.000411    2.982381  0.003966    5.905737   \n",
      "22   21.393142   72.279072  0.003006  -23.592048 -0.005831  -27.352206   \n",
      "23   87.781592   80.928672 -0.000688   59.672123 -0.008794   43.124563   \n",
      "\n",
      "      6_고압콘덴샤  7_신재생에너지동     8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동  47_기숙사9동  \\\n",
      "0   52.833176  -5.270034  50.410743  -2.208510  ...    27.212801 -3.359520   \n",
      "1   66.491777 -41.876328  29.116810 -10.929974  ...    38.710243 -6.897847   \n",
      "2   84.696570  -0.541930  45.325596  -3.292743  ...     9.917218  1.383838   \n",
      "3    9.029479   8.126128  23.000046   5.853487  ...    14.540403 -0.243694   \n",
      "4  -68.067338 -26.197006  27.282294  10.780739  ...    -1.257792  7.308967   \n",
      "5  -14.566344 -22.225327  15.509074  -0.272802  ...    -1.927537  4.840683   \n",
      "6   19.940981 -27.221356   5.398926  11.445477  ...     8.735594 -2.761350   \n",
      "7   -1.845978 -18.670728  25.200816  -5.728041  ...    16.868305  0.729401   \n",
      "8  -10.755574 -13.953605  11.272513  -0.517919  ...    34.961716 -2.707179   \n",
      "9   -9.983680  -5.873144  18.068899   0.952775  ...    22.615083 -5.277811   \n",
      "10 -18.724534 -31.418833  11.245563   9.780262  ...    17.507588 -1.624622   \n",
      "11 -49.505781 -19.071664  11.282975  -1.945152  ...    -1.790545  2.523948   \n",
      "12   2.419865 -56.530689  13.017291  -1.797604  ...    -3.411303 -3.773370   \n",
      "13  19.650059 -62.450801  13.840999 -11.626311  ...   -59.809248 -1.834013   \n",
      "14 -21.648173 -28.589503  45.338852  -6.826139  ...   -41.166393 -1.072396   \n",
      "15  26.102944 -15.103506  25.669215 -12.807374  ...   -65.965046  5.273170   \n",
      "16  -5.138930  10.307470  17.973044   1.157683  ...   -62.063820 -2.361467   \n",
      "17 -14.258815 -20.925632  -1.775109 -11.596223  ...   -90.316341 -0.599998   \n",
      "18 -22.331103 -40.792728  22.685209 -10.305266  ...   -58.428749  1.380484   \n",
      "19 -31.646591   0.200753  34.475176  -3.176314  ...   -39.780629 -5.695537   \n",
      "20 -18.592956 -17.633646  17.838402  -2.983708  ...     0.181500 -0.671105   \n",
      "21 -15.242226 -20.746602  16.825194  -3.723484  ...   -17.275216  2.030122   \n",
      "22 -46.431234  17.978383   7.911147  13.792660  ...   -48.838368 -1.126408   \n",
      "23 -43.536484 -15.519735  -4.366216  12.838460  ...    -7.710974  3.303270   \n",
      "\n",
      "    48_고등광/극초단  49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  \\\n",
      "0  -102.342724      1.379939         -4.511988    -0.796959     0.874592   \n",
      "1   -62.361758      9.713184          3.089414    -3.656417     0.340196   \n",
      "2   -52.354106      7.564150         -2.784113     5.619712    -5.435899   \n",
      "3   -77.614604     -1.544467          0.599872     7.205486    -5.966097   \n",
      "4   -29.516958      0.844961          1.275726     8.754625    -2.204239   \n",
      "5   -52.562313      2.153491         -0.009398     6.164956     0.613514   \n",
      "6   -24.996572     -3.170412         -1.144791     5.959513     3.618290   \n",
      "7    29.398814      4.968783         -1.357524     7.086044    -5.821941   \n",
      "8   -14.011104      6.472984         -0.797088     6.370546    -1.516444   \n",
      "9     6.823523      4.228510          8.033616     3.340421    -0.519865   \n",
      "10    4.779971     -1.576698         -0.799994     8.089143    -1.128022   \n",
      "11  -47.984823     -2.795949          0.774930    11.574110     6.500409   \n",
      "12   -2.148395     -0.209855          2.453960    -2.309987    -7.453810   \n",
      "13  -34.656387     -3.720058         -4.849090     3.813057   -10.743242   \n",
      "14   62.381562      5.805065         10.636848   -19.187350     0.888213   \n",
      "15   19.672418      1.790566         -1.004420    -4.648983     0.681052   \n",
      "16    4.538436      4.033663          3.838805    12.151487     0.036267   \n",
      "17   30.471330      6.198935          0.827879    -5.819980     0.282422   \n",
      "18    5.870001      2.381688         -3.745838    -3.983033     1.239063   \n",
      "19   51.398681      1.350014         -3.991077     2.192895    -4.013906   \n",
      "20   78.812895      5.065670          6.807497    -4.870099    -0.857643   \n",
      "21   24.377271     -2.038688          2.388139   -11.734917     0.541483   \n",
      "22   70.647775     -1.057699          5.344640   -12.252697     0.728507   \n",
      "23  102.443982      2.482822          9.079507     1.959785     1.783322   \n",
      "\n",
      "    53_LG도서관(E)  54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0     -7.938335    -6.160231     -4.002801  \n",
      "1     -3.588844     0.078993     -5.030586  \n",
      "2     -6.174723    -0.909543     -1.940540  \n",
      "3     -5.625622    -3.789431     -0.035898  \n",
      "4      0.645275    -8.077363      2.025060  \n",
      "5     -1.229908    -6.833836      3.374501  \n",
      "6      3.325433    -7.216839     11.215439  \n",
      "7      1.902142     3.840473      2.998615  \n",
      "8      1.440149    -3.179172      7.085318  \n",
      "9      1.482963    -3.518021      7.548203  \n",
      "10     3.557269    -3.959690      5.973883  \n",
      "11     3.312027     5.901066      8.817594  \n",
      "12    -2.652558     2.828953     -4.707405  \n",
      "13    -5.197995     2.346524      4.111805  \n",
      "14    -2.144888     6.112735     -1.101260  \n",
      "15    -5.691024   -11.639970      5.862344  \n",
      "16    -9.287461    14.315896      9.725854  \n",
      "17    -6.940868   -14.007328      4.498193  \n",
      "18    -6.685901   -30.546363      6.975080  \n",
      "19    10.672500    -9.589843      0.622224  \n",
      "20    -0.464109   -10.767214      0.321936  \n",
      "21     1.144903    -5.937270      1.763606  \n",
      "22    14.447301   -17.861612     11.384646  \n",
      "23    -2.111565     5.107856     12.139266  \n",
      "\n",
      "[24 rows x 56 columns]\n",
      "MAE: 16.4235, MSE: 901.1540, RMSE: 30.0192 (no normalization)\n",
      "err_total:  -213.96897530053332\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    테스트 세트의 첫 번째 시퀀스에 대한 예측을 수행한 후, 예측된 값과 실제 목표값 사이의 차이를 계산하는 코드\n",
    "    이러한 차이를 기반으로 여러 성능 지표를 계산하며, 전체 에러를 출력하는 코드\n",
    "    *df는 원래의 데이터셋이라 가정하며 'date' 컬럼을 포함한다고 가정합니다.\n",
    "\"\"\"\n",
    "# 건물 이름을 가져옵니다.\n",
    "building_names = df.columns[-56:]  # 필요에 따라 이 값을 조절하세요.\n",
    "# 테스트 세트를 위한 DataLoader를 생성합니다.\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "# 테스트 세트에서 첫 번째 시퀀스와 그 목표값을 가져옵니다.\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# 모델을 평가 모드로 전환합니다.\n",
    "model.eval()\n",
    "\n",
    "# 예측을 수행합니다.\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "print(prediction.shape, real_target.shape)\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "# 패딩을 추가합니다.\n",
    "padding = np.zeros((prediction.shape[0], 7))\n",
    "prediction_pad = np.hstack((padding, prediction))\n",
    "real_target_pad = np.hstack((padding, real_target))\n",
    "# print(prediction_pad.shape, real_target_pad.shape)\n",
    "\n",
    "# 역변환을 적용하여 정규화를 해제합니다.\n",
    "prediction_inv = dataset.scaler.inverse_transform(prediction_pad)\n",
    "real_target_inv = dataset.scaler.inverse_transform(real_target_pad)\n",
    "\n",
    "# 처음 7개의 컬럼을 삭제합니다.\n",
    "prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "real_target_inv = np.delete(real_target_inv, np.s_[:7], axis=1)\n",
    "\n",
    "# 원래의 형태로 다시 변형합니다.\n",
    "prediction = prediction_inv.reshape(prediction.shape)\n",
    "real_target = real_target_inv.reshape(real_target.shape)\n",
    "\n",
    "# 에러(실제 목표값과 예측값의 차이)를 계산합니다.\n",
    "error = real_target - prediction\n",
    "\n",
    "# 예측값을 위한 DataFrame을 생성합니다.\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "err_total = total = error_df.values.flatten().sum()\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n",
    "\n",
    "# 성능 지표를 계산합니다.\n",
    "mae_n = mean_absolute_error(real_target, prediction)\n",
    "mse_n = mean_squared_error(real_target, prediction)\n",
    "rmse_n = sqrt(mse_n)\n",
    "print(f'MAE: {mae_n:.4f}, MSE: {mse_n:.4f}, RMSE: {rmse_n:.4f} (no normalization)')\n",
    "print(\"err_total: \", err_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
