{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediciton Application\n",
    "- input: ----\n",
    "         merged_data 엑셀 파일과 같은 형태이어야 함(월|일|요일|공휴일 유무|온도|습도|건물이름 유효전력량*56개 건물).\n",
    "- output: 원하는 예측 날짜의 1시간 단위로 예측한 결과\n",
    "\n",
    "### 주의할 점\n",
    "- input 데이터 형태를 꼭 맞춰줘야함. 해당 날짜의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    모델 및 데이터셋 클래스 정의하는 코드\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 시계열 데이터를 처리하는 클래스를 정의합니다.\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24, pred_len=24):\n",
    "        self.seq_len = seq_len  # 입력 시퀀스의 길이를 정의합니다.\n",
    "        self.pred_len = pred_len  # 예측할 시퀀스의 길이를 정의합니다.\n",
    "        self.scaler = MinMaxScaler()  # 데이터 정규화를 위한 MinMaxScaler 객체를 생성합니다.\n",
    "\n",
    "        self.dataframe = self._preprocess(dataframe)  # 데이터 전처리 함수를 호출하여 dataframe을 전처리합니다.\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # 누락된 값을 시계열의 이전 값으로 채웁니다.\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # 숫자형 열을 [0, 1] 범위로 정규화합니다.\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # 범주형 변수를 원-핫 인코딩합니다.\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # 원래의 범주형 열을 삭제하고 인코딩된 열과 병합합니다.\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.seq_len - self.pred_len + 1  # 데이터셋의 전체 길이를 반환합니다.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len, :7]  # 입력 시퀀스의 앞 7열만 선택합니다.\n",
    "        # 마지막 56열이 전력 값이라고 가정하고 예측할 시퀀스를 선택합니다.\n",
    "        y = self.dataframe.iloc[idx+self.seq_len:idx+self.seq_len+self.pred_len, -56:] \n",
    "        return torch.Tensor(x.values), torch.Tensor(y.values).reshape(-1)  # y 값을 평탄화하여 반환합니다.\n",
    "    \n",
    "# LSTM 모델을 정의합니다.\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # LSTM의 은닉층의 차원을 정의합니다.\n",
    "        self.n_layers = n_layers  # LSTM 층의 개수를 정의합니다.\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)  # LSTM 층을 정의합니다.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 완전 연결 층을 정의합니다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 은닉 상태와 셀 상태를 초기화합니다.\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM 층을 통해 데이터를 전달하고 출력을 얻습니다.\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 완전 연결 층을 통해 출력을 얻습니다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.001\n",
      "final_learning_rate: 0.0005\n",
      "batch_size: 256\n",
      "max_epochs: 700\n",
      "stop_epoch: 322\n",
      "hidden_dim: 128\n",
      "n_layers: 9\n",
      "MinMaxScaler()\n",
      "{'MAE': 0.060754657, 'MSE': 0.008112408, 'RMSE': 0.09006890818017045}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(7, 128, num_layers=9, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1344, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    pre-trained 모델 불러오는 코드\n",
    "\"\"\"\n",
    "path = '/home/kimyirum/EMS/ict-2023-ems/load/results/'\n",
    "name = '20230808_215632'\n",
    "hyperparameters_filepath = path + name + '.pkl'\n",
    "model_filepath = path + 'model_' + name + '.pt'\n",
    "hyperparameters = {}\n",
    "\n",
    "# Load results from a pickle file\n",
    "with open(hyperparameters_filepath, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "    hyperparameters = loaded_results['Hyperparameters']\n",
    "    scalers = loaded_results['Scalers']\n",
    "\n",
    "# Print hyperparameters\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f'{key}: {value}')\n",
    "print(loaded_results['Scalers'])\n",
    "print(loaded_results['Metrics'])    \n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "# Recreate the model architecture\n",
    "model = LSTMModel(\n",
    "    input_dim=7,\n",
    "    hidden_dim=int(hyperparameters['hidden_dim']),\n",
    "    output_dim=24*56,\n",
    "    n_layers=int(hyperparameters['n_layers'])\n",
    ").to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(model_filepath))\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    데이터셋 읽고 set으로 분할하는 코드\n",
    "\"\"\"\n",
    "\n",
    "# pandas 라이브러리를 사용하여 엑셀 파일을 불러옵니다.\n",
    "df = pd.read_excel('/home/kimyirum/EMS/ict-2023-ems/load/data/merged_data_KW.xlsx')\n",
    "\n",
    "# TimeSeriesDataset 클래스의 인스턴스를 생성합니다. 위에서 정의한 클래스를 사용하여 데이터를 전처리합니다.\n",
    "dataset = TimeSeriesDataset(df)\n",
    "\n",
    "# 학습, 검증 및 테스트 데이터 세트의 크기를 정의합니다.\n",
    "train_size = int(0.7 * len(dataset))  # 전체 데이터의 70%를 학습 데이터로 사용\n",
    "val_size = int(0.2 * len(dataset))    # 전체 데이터의 20%를 검증 데이터로 사용\n",
    "test_size = len(dataset) - train_size - val_size  # 나머지 데이터를 테스트 데이터로 사용\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "# 전체 데이터셋을 학습, 검증 및 테스트 데이터 세트로 분할합니다.\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1344]) torch.Size([1, 1344])\n",
      "Error for next 24 hours:\n",
      "        0_SV-2      1_SV-5    2_SV-6      3_SV-7  4_HV-NM1    5_HV-NM2  \\\n",
      "0     1.082076  -10.952450 -0.003277  -16.359706  0.003285   -1.934116   \n",
      "1   -31.070785  -47.686927 -0.003334  -12.552275  0.001724  -20.189036   \n",
      "2    23.545235    3.105843  0.003501    2.871590 -0.006436  -23.609016   \n",
      "3    15.263307    0.036147  0.001715   -9.988367  0.001965    2.854894   \n",
      "4   -49.103834  -33.824344  0.004298  -36.656871 -0.003685  -83.417129   \n",
      "5   -36.715905  -42.150427 -0.004314  -32.447053 -0.003832  -54.090871   \n",
      "6    -5.332697  -29.739259 -0.010827  -21.849856  0.004766  -30.574461   \n",
      "7   -60.274415  -59.780448  0.004076  -73.822932 -0.006717  -61.959190   \n",
      "8   -33.492719  -50.572100  0.006783  -29.073571 -0.004120  -62.643683   \n",
      "9   -63.509894  -79.231903  0.004183  -75.947824  0.010150  -61.676247   \n",
      "10  -24.496855  -53.202720  0.004255  -49.052714 -0.002904  -25.462596   \n",
      "11   20.909409   -2.161036  0.002661    2.915339 -0.004705   20.609600   \n",
      "12 -168.965557 -162.903102 -0.001262 -177.078548  0.008759 -175.091525   \n",
      "13 -175.270393 -189.950726 -0.002408 -168.144247 -0.001811 -187.389501   \n",
      "14 -201.811040 -164.260683 -0.007960 -164.219447 -0.002177 -201.874208   \n",
      "15 -212.593196 -189.334753  0.005134 -198.484881  0.001158 -192.206821   \n",
      "16  -88.118322 -108.835311 -0.003158  -78.801099 -0.002627  -76.143338   \n",
      "17 -268.850700 -261.477003 -0.005018 -211.128077 -0.007550 -231.751130   \n",
      "18 -145.313096 -156.004721 -0.009331 -143.488851 -0.001262 -126.056388   \n",
      "19  -54.328837  -71.853446 -0.000991  -79.425383  0.002839  -45.148756   \n",
      "20  -74.565770  -76.853987 -0.002396  -73.851971  0.003075  -63.844385   \n",
      "21 -146.944016 -156.529265 -0.004582 -110.487035  0.003680  -84.723150   \n",
      "22  -70.695816  -45.076692  0.008209  -87.145583  0.000543  -98.501399   \n",
      "23    4.778618   -1.650268  0.002299  -46.883489 -0.001787  -18.659687   \n",
      "\n",
      "       6_고압콘덴샤  7_신재생에너지동     8_대학B동  9_대학기숙사A동  ...  46_기혼자아파트E동  47_기숙사9동  \\\n",
      "0   106.317349   5.535158  38.901720 -11.714090  ...    21.742015 -2.880438   \n",
      "1   147.398083 -23.227254  18.737106 -19.226196  ...    34.367167 -5.642343   \n",
      "2   165.838957  12.906821  32.647410 -15.399902  ...     0.136351 -0.054005   \n",
      "3   114.630790   4.348401  13.409863  -7.131993  ...    -6.494843 -3.441728   \n",
      "4    27.847195 -39.347008   8.856191  -3.632071  ...   -16.882733  4.017566   \n",
      "5    27.631300 -29.358171  -2.053703 -14.100897  ...   -21.683222  4.423329   \n",
      "6    44.107168 -33.482775  -6.746018   2.440206  ...   -11.501967 -4.219389   \n",
      "7    42.534971 -35.917629  10.209108 -15.884381  ...    -8.600667 -2.516271   \n",
      "8    43.514911 -45.521463  -5.662363  -8.559869  ...    13.874847 -2.851954   \n",
      "9    43.748642 -31.157799  -9.211899 -10.191441  ...    23.411950 -2.746296   \n",
      "10   20.901003 -47.416347  -5.851819  -1.799382  ...    14.674480  1.011619   \n",
      "11    5.855502 -36.191935  -9.583567 -12.007702  ...    -5.726064  1.920625   \n",
      "12   48.672958 -76.629324 -14.476205 -12.854149  ...     2.013496 -3.574124   \n",
      "13   46.521052 -80.518899 -13.236218 -21.544357  ...   -33.865576 -0.688295   \n",
      "14   10.113442 -53.006514  15.525345 -20.043992  ...   -11.170252 -0.846174   \n",
      "15   36.975411 -51.608803  -9.534700 -21.661689  ...   -28.574512  3.803103   \n",
      "16  -19.683007 -14.211518 -20.937809  -6.917036  ...   -21.752711 -2.975589   \n",
      "17  -27.119409 -40.439789 -24.644636 -17.215816  ...   -43.634135  3.435336   \n",
      "18  -39.859961 -38.961984   6.384486 -16.871278  ...   -16.676572  3.388942   \n",
      "19  -28.738705 -14.719642  16.724689  -4.529168  ...    -4.011802 -6.015772   \n",
      "20  -32.571743 -19.516438 -11.181151  -8.319214  ...    36.785311  0.161744   \n",
      "21  -50.893108 -35.883674  -1.169596  -5.541083  ...    26.217500  3.623186   \n",
      "22  -42.638139  12.164818  -1.606795  12.841109  ...   -20.829836 -0.531326   \n",
      "23  -17.339343 -11.858647 -18.776629   8.974508  ...     4.011835  2.659929   \n",
      "\n",
      "    48_고등광/극초단  49_신소재공학동(E)  50_전기전자컴퓨터공학동(E)  51_생명과학동(E)  52_기계공학동(E)  \\\n",
      "0   -72.922443      1.241488         -2.802491   -11.379369     2.057767   \n",
      "1   -54.140106      9.862011          4.996408    -4.048632     1.714296   \n",
      "2   -32.728115      7.261081         -1.221859    -2.909029    -3.458479   \n",
      "3   -59.487003     -0.802745          3.402139    -2.444187    -4.100266   \n",
      "4   -26.822366      0.196393          1.565451    -2.334890    -0.651621   \n",
      "5   -27.518613      1.192958          1.182375    -4.314221     2.819013   \n",
      "6     2.217101     -2.599171          0.219014    -2.660032     4.484280   \n",
      "7    44.460450      4.274628          0.694036    -3.456405    -4.384149   \n",
      "8     0.195626      5.799282          0.838201    -4.630070     0.341070   \n",
      "9    17.018746      2.705791          9.350791    -4.542741     1.549650   \n",
      "10   17.735389     -1.506532          2.527098     2.746767     0.036360   \n",
      "11  -33.200341     -3.351128          3.305945     3.426596     8.067806   \n",
      "12    5.656085     -1.430580          2.819246    -6.778906    -5.563822   \n",
      "13  -28.478675     -5.160397         -4.598145     1.735337    -9.036354   \n",
      "14   55.207413      4.185874          9.573310   -23.867179     1.507633   \n",
      "15   -9.294759      0.547796         -0.537819    -7.345358     0.825585   \n",
      "16   -3.654100      3.232191          3.988040    10.369902     1.415477   \n",
      "17   24.285268      5.563844          1.992920    -2.038331     1.190819   \n",
      "18   14.785590      3.167287         -1.423887     3.248166     1.358122   \n",
      "19   54.868221      1.900578         -2.769107     7.379412    -2.894334   \n",
      "20   66.690545      4.229914          6.947208    -4.600565     0.682088   \n",
      "21   25.107936     -2.715439          1.942186   -10.946480     0.630782   \n",
      "22   65.782858      0.262518          7.292299    -8.440526     1.240704   \n",
      "23  112.264045      2.699547          9.640083     1.720546     1.061623   \n",
      "\n",
      "    53_LG도서관(E)  54_중앙P/P(E)  55_고등광연구소(E)  \n",
      "0     -9.649895   -10.446669     -6.772137  \n",
      "1     -6.064049     2.657632     -7.405636  \n",
      "2     -9.062348    -4.786696     -4.365979  \n",
      "3     -8.903250    -8.979983     -4.335073  \n",
      "4     -2.046256   -15.927106     -1.451733  \n",
      "5     -2.596286    -7.505815     -0.937916  \n",
      "6     -2.291858    -1.797415      8.657904  \n",
      "7     -1.861528     1.593403     -0.660387  \n",
      "8      1.597797    -5.250365      1.487761  \n",
      "9      4.093132     2.780536      2.323544  \n",
      "10     3.371698    -5.359595      2.411397  \n",
      "11     0.746583     1.557788      4.233761  \n",
      "12    -3.189539     0.835895    -10.379131  \n",
      "13    -8.787003     2.100860     -4.848711  \n",
      "14    -7.656069     2.850350     -7.907018  \n",
      "15    -7.318566   -23.854144     -0.919958  \n",
      "16    -7.644888     2.416139      4.067385  \n",
      "17    -4.607344   -17.056762      0.840674  \n",
      "18    -4.468987   -30.981532      0.922079  \n",
      "19    10.004638   -14.082083     -1.968695  \n",
      "20    -0.624527    -6.708364     -4.780085  \n",
      "21     0.351107   -10.431803     -1.178974  \n",
      "22    11.351858   -23.854860      8.558480  \n",
      "23    -5.677896    -4.459472      8.335898  \n",
      "\n",
      "[24 rows x 56 columns]\n",
      "MAE: 24.0564, MSE: 2177.5649, RMSE: 46.6644 (no normalization)\n",
      "err_total:  -13444.779179658626\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    테스트 세트의 첫 번째 시퀀스에 대한 예측을 수행한 후, 예측된 값과 실제 목표값 사이의 차이를 계산하는 코드\n",
    "    이러한 차이를 기반으로 여러 성능 지표를 계산하며, 전체 에러를 출력하는 코드\n",
    "    *df는 원래의 데이터셋이라 가정하며 'date' 컬럼을 포함한다고 가정합니다.\n",
    "\"\"\"\n",
    "# 건물 이름을 가져옵니다.\n",
    "building_names = df.columns[-56:]  # 필요에 따라 이 값을 조절하세요.\n",
    "# 테스트 세트를 위한 DataLoader를 생성합니다.\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "# 테스트 세트에서 첫 번째 시퀀스와 그 목표값을 가져옵니다.\n",
    "real_sequence, real_target = next(iter(test_loader))\n",
    "\n",
    "# 모델을 평가 모드로 전환합니다.\n",
    "model.eval()\n",
    "\n",
    "# 예측을 수행합니다.\n",
    "with torch.no_grad():\n",
    "    prediction = model(real_sequence)\n",
    "\n",
    "print(prediction.shape, real_target.shape)\n",
    "prediction = prediction.squeeze(0).reshape(24, 56).numpy()\n",
    "real_target = real_target.view(24, 56).numpy()\n",
    "\n",
    "# 패딩을 추가합니다.\n",
    "padding = np.zeros((prediction.shape[0], 7))\n",
    "prediction_pad = np.hstack((padding, prediction))\n",
    "real_target_pad = np.hstack((padding, real_target))\n",
    "# print(prediction_pad.shape, real_target_pad.shape)\n",
    "\n",
    "# 역변환을 적용하여 정규화를 해제합니다.\n",
    "prediction_inv = dataset.scaler.inverse_transform(prediction_pad)\n",
    "real_target_inv = dataset.scaler.inverse_transform(real_target_pad)\n",
    "\n",
    "# 처음 7개의 컬럼을 삭제합니다.\n",
    "prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "real_target_inv = np.delete(real_target_inv, np.s_[:7], axis=1)\n",
    "\n",
    "# 원래의 형태로 다시 변형합니다.\n",
    "prediction = prediction_inv.reshape(prediction.shape)\n",
    "real_target = real_target_inv.reshape(real_target.shape)\n",
    "\n",
    "# 에러(실제 목표값과 예측값의 차이)를 계산합니다.\n",
    "error = real_target - prediction\n",
    "\n",
    "# 예측값을 위한 DataFrame을 생성합니다.\n",
    "predicted_df = pd.DataFrame(prediction, columns=building_names)\n",
    "real_target_df = pd.DataFrame(real_target, columns=building_names)\n",
    "error_df = pd.DataFrame(error, columns=building_names)\n",
    "err_total = total = error_df.values.flatten().sum()\n",
    "\n",
    "# print(\"Predicted Values for next 24 hours:\")\n",
    "# print(predicted_df)\n",
    "\n",
    "# print(\"Real Values for next 24 hours:\")\n",
    "# print(real_target_df)\n",
    "\n",
    "print(\"Error for next 24 hours:\")\n",
    "print(error_df)\n",
    "\n",
    "# 성능 지표를 계산합니다.\n",
    "mae_n = mean_absolute_error(real_target, prediction)\n",
    "mse_n = mean_squared_error(real_target, prediction)\n",
    "rmse_n = sqrt(mse_n)\n",
    "print(f'MAE: {mae_n:.4f}, MSE: {mse_n:.4f}, RMSE: {rmse_n:.4f} (no normalization)')\n",
    "print(\"err_total: \", err_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
