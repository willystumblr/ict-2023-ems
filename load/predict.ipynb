{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediciton Application\n",
    "- input: 원하는 예측 날짜의 7일 전부터 전날까지 측정한 데이터. \n",
    "         merged_data 엑셀 파일과 같은 형태이어야 함(월|일|요일|공휴일 유무|온도|습도|건물이름 유효전력량*56개 건물).\n",
    "- output: 원하는 예측 날짜의 1시간 단위로 예측한 결과(ex: test_for_0901.xlsx)\n",
    "\n",
    "### 주의할 점\n",
    "- input 데이터 형태를 꼭 맞춰줘야함. 해당 날짜의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# 시계열 예측을 위한 데이터셋 클래스 정의\n",
    "class TimeSeriesDataset_forPredict(Dataset):\n",
    "    def __init__(self, dataframe, seq_len=7*24):\n",
    "        self.seq_len = seq_len  # 시퀀스 길이 지정 (기본값은 7일 * 24시간 = 168시간)\n",
    "\n",
    "        # 데이터 전처리 실행\n",
    "        self.dataframe = self._preprocess(dataframe)\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "        # 결측치가 있다면, 바로 앞의 값으로 채우기\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # 수치형 컬럼 정규화하여 [0, 1] 범위로 변환\n",
    "        scaler = MinMaxScaler()\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # 범주형 변수를 원-핫 인코딩하기\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if not categorical_cols.empty:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoded = encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names(categorical_cols))\n",
    "            \n",
    "            # 원래의 범주형 컬럼 제거하고 인코딩된 컬럼을 추가\n",
    "            df.drop(columns=categorical_cols, inplace=True)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 길이 반환\n",
    "        return max(0, len(self.dataframe) - self.seq_len + 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx 위치에서 시작하는 시퀀스 반환\n",
    "        x = self.dataframe.iloc[idx:idx+self.seq_len,:7]\n",
    "        return torch.Tensor(x.values)  # x값만 반환\n",
    "\n",
    "    \n",
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # LSTM 및 Fully Connected Layer 정의\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 hidden state 및 cell state 설정\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) \n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device) \n",
    "\n",
    "        # LSTM 계층을 통과한 후의 출력값 계산\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스만을 이용하여 최종 예측값 계산\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "learning_rate: 0.001\n",
      "final_learning_rate: 0.00025\n",
      "batch_size: 256\n",
      "max_epochs: 700\n",
      "stop_epoch: -1\n",
      "hidden_dim: 128\n",
      "n_layers: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimyirum/anaconda3/envs/EMS/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:479: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/home/kimyirum/anaconda3/envs/EMS/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:480: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(7, 128, num_layers=7, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1344, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_day = '0901'\n",
    "\n",
    "hyperparameters_filepath = '/home/kimyirum/EMS/ict-2023-ems/load/results/20230807_180431.pkl'\n",
    "model_filepath = '/home/kimyirum/EMS/ict-2023-ems/load/results/model_20230807_180431.pt'\n",
    "test_data = '/home/kimyirum/EMS/ict-2023-ems/load/data/test_for_'+predict_day+'.xlsx'\n",
    "\n",
    "df = pd.read_excel(test_data)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "hyperparameters = {}\n",
    "\n",
    "# Load results from a pickle file\n",
    "with open(hyperparameters_filepath, 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "    hyperparameters = loaded_results['Hyperparameters']\n",
    "    scalers = loaded_results['Scalers']\n",
    "\n",
    "# Print hyperparameters\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "# Initialize our dataset class\n",
    "dataset = TimeSeriesDataset_forPredict(df)\n",
    "test_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Recreate the model architecture\n",
    "model = LSTMModel(\n",
    "    input_dim=7,\n",
    "    hidden_dim=int(hyperparameters['hidden_dim']),\n",
    "    output_dim=24*56,\n",
    "    n_layers=int(hyperparameters['n_layers'])\n",
    ").to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(model_filepath))\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     predictions\u001b[39m.\u001b[39mappend(prediction)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Combine all predictions\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(predictions, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m \u001b[39m# Create a DataFrame for predictions\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# Reshape the predictions to align with the number of building_names\u001b[39;00m\n\u001b[1;32m     30\u001b[0m predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(building_names))\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# Assuming df is the original dataset and it includes a 'date' column\n",
    "building_names = df.columns[-56:]  # adjust this as necessary\n",
    "\n",
    "# Prepare storage for predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterate over test set\n",
    "for sequence in test_loader:\n",
    "    # Move sequence to correct device\n",
    "    sequence = sequence.to(device)\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(sequence).cpu().numpy()\n",
    "\n",
    "    prediction_res = prediction.squeeze(0).reshape(24, 56)\n",
    "    padding = np.zeros((prediction_res.shape[0], 7))\n",
    "    prediction_pad = np.hstack((padding, prediction_res))\n",
    "    prediction_inv = scalers.inverse_transform(prediction_pad)\n",
    "    prediction_inv = np.delete(prediction_inv, np.s_[:7], axis=1)\n",
    "    prediction = prediction_inv.reshape(prediction.shape)\n",
    "\n",
    "    # Store the prediction\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Combine all predictions\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Create a DataFrame for predictions\n",
    "# Reshape the predictions to align with the number of building_names\n",
    "predictions = predictions.reshape(-1, len(building_names))\n",
    "predictions_df = pd.DataFrame(predictions, columns=building_names)\n",
    "\n",
    "predictions_df['total(KW)'] = predictions_df.sum(axis=1)\n",
    "\n",
    "# Save to Excel file\n",
    "output_filepath = '/home/kimyirum/EMS/ict-2023-ems/load/predict_for_'+predict_day+'.xlsx'  # adjust this as necessary\n",
    "predictions_df.to_excel(output_filepath, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
